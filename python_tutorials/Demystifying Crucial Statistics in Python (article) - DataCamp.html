<!DOCTYPE html><html><head><meta charSet="utf-8" class="next-head"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" class="next-head"/><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" class="next-head"/><meta name="google-site-verification" content="NXbTi7gyLQESV4NIskeE9Ka0Am8KjAtzg5gm8g38HbU" class="next-head"/><meta name="keywords" content="statistics machine learning data science python" class="next-head"/><meta name="description" content="Learn about the basic statistics required for Data Science and Machine Learning in Python." class="next-head"/><title class="next-head">Demystifying Crucial Statistics in Python (article) - DataCamp</title><link rel="canonical" href="https://www.datacamp.com/community/tutorials/demystifying-crucial-statistics-python" class="next-head"/><link rel="author" href="https://plus.google.com/u/0/+Datacamp/" class="next-head"/><link rel="shortcut icon" type="image/x-icon" href="https://cdn.datacamp.com/main-app/assets/favicon-335cd0394b32102a39221d79e5fd7e51078e6d32a0c8aea59676a6869f84e9d8.ico" class="next-head"/><link rel="chrome-webstore-item" href="https://chrome.google.com/webstore/detail/lbbhbkehmgbndgfdbncbmikooblghdbi" class="next-head"/><meta property="og:title" content="Demystifying Crucial Statistics in Python" class="next-head"/><meta property="og:image" content="https://s3.amazonaws.com/datacamp-community-prod/social-share-tutorials.jpg" class="next-head"/><meta property="og:url" content="https://www.datacamp.com/community/tutorials/demystifying-crucial-statistics-python" class="next-head"/><meta property="og:type" content="article" class="next-head"/><meta property="og:published_time" content="2018-09-27T16:00:00.000Z" class="next-head"/><meta property="og:author" content="Sayak  Paul" class="next-head"/><meta property="og:description" content="Learn about the basic statistics required for Data Science and Machine Learning in Python." class="next-head"/><meta property="og:site_name" content="DataCamp Community" class="next-head"/><meta name="twitter:title" content="Demystifying Crucial Statistics in Python" class="next-head"/><meta name="twitter:description" content="Learn about the basic statistics required for Data Science and Machine Learning in Python." class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:site" content="@DataCamp" class="next-head"/><meta name="twitter:creator" content="@DataCamp" class="next-head"/><meta name="twitter:domain" content="www.datacamp.com" class="next-head"/><meta name="twitter:image" content="https://s3.amazonaws.com/datacamp-community-prod/social-share-tutorials.jpg" class="next-head"/><meta name="twitter:image:width" content="1200" class="next-head"/><meta name="twitter:image:height" content="628" class="next-head"/><meta name="twitter:image:alt" content="Demystifying Crucial Statistics in Python" class="next-head"/><meta name="article:publisher" content="https://www.facebook.com/DataCamp-726282547396228" class="next-head"/><meta name="fb:app_id" content="726282547396228" class="next-head"/><script class="next-head">
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TGGWB2P');
    </script><link rel="preload" href="/community/_next/f80eab32-3c58-4b52-a08c-d754687b7a03/page/community/tutorial.js" as="script"/><link rel="preload" href="/community/_next/f80eab32-3c58-4b52-a08c-d754687b7a03/page/_error.js" as="script"/><link rel="preload" href="/community/_next/b782294ddb8d954b4c94ee4c23476b23/app.js" as="script"/><style id="__jsx-396080995">.Logo.jsx-396080995{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;}
.Logo__image.jsx-396080995{display:block;width:122px;height:28px;margin-left:8px;}
.Logo__image.jsx-396080995 svg{fill:#FFFFFF;}
@media (min-width:800px){.Logo.jsx-396080995{height:59px;}.Logo__image.jsx-396080995{margin:17px auto 0;}}</style><style id="__jsx-499055511">.SidebarMenu.jsx-499055511{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:fixed;z-index:300;width:100vw;height:50px;background-image: linear-gradient(207deg, #2388B0, #33AACC);}
.Layout--banner .SidebarMenu.jsx-499055511{top:55px;}
.icon.jsx-499055511{text-align:right;}
.icon.jsx-499055511 svg{margin-right:9px;width:20px;height:20px;fill:#FFFFFF;}
@media (min-width:800px){.SidebarMenu.jsx-499055511{z-index:200;width:220px;top:0;left:0;background-image:none;}.Layout--banner .SidebarMenu.jsx-499055511{top:80px;}}</style><style id="__jsx-2769082546">.Menu.jsx-2769082546{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;margin-top:50px;}
.Layout--banner .Menu.jsx-2769082546{margin-top:105px;}
.Layout--openMenu .Menu.jsx-2769082546{min-height:calc(100vh - 50px - 134px);}
.Layout--openMenu.Layout--banner .Menu.jsx-2769082546{min-height:calc(100vh - 105px - 134px);}
.section.jsx-2769082546{margin-bottom:20px;}
.section.jsx-2769082546 h5.jsx-2769082546{margin:0;padding-left:17px;font-size:13px;-webkit-letter-spacing:3.3px;-moz-letter-spacing:3.3px;-ms-letter-spacing:3.3px;letter-spacing:3.3px;line-height:36px;text-align:left;text-transform:uppercase;background-color:#195B73;color:#7ECCE2;}
.item.jsx-2769082546{margin-bottom:1px;padding-left:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;font-size:15px;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:40px;text-decoration:none;color:#FFFFFF;}
.statusIcon.jsx-2769082546{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;padding:10px;}
.active.jsx-2769082546{background-color:#55AECB;}
a.jsx-2769082546:hover{background-color:#55AECB;}
.image.jsx-2769082546{margin-top:2px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;width:30px;height:30px;text-align:center;}
.image.jsx-2769082546 svg{fill:#195B73;}
.active.jsx-2769082546 svg,a.jsx-2769082546:hover svg{fill:#FFFFFF;}
.text.jsx-2769082546{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;}
.subMenu.jsx-2769082546{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}
@media (min-width:800px){.Menu.jsx-2769082546{position:fixed;width:220px;margin-top:50px;}.Layout--banner .Menu.jsx-2769082546{margin-top:130px;}.section.jsx-2769082546 h5.jsx-2769082546{padding-left:0;text-align:center;}}</style><style id="__jsx-1844558338">.Button.jsx-1844558338{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;height:33px;margin:auto 5px;padding:0 15px;font-size:13px;font-weight:bold;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;white-space:nowrap;color:#3A3A3A;border:1px solid transparent;border-radius:4px;background-color:transparent;cursor:pointer;outline:none;}
.Button.jsx-1844558338::before,.Button.jsx-1844558338::after{content:'';-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;}
.icon.jsx-1844558338{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:20px;}
.icon.jsx-1844558338 svg{-webkit-flex:1 1 0;-ms-flex:1 1 0;flex:1 1 0;height:20px;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;fill:#33AACC;}
.greyIcon.jsx-1844558338 .icon.jsx-1844558338 svg{min-width:16px;min-height:16px;fill:#3A3A3A;}
.same.jsx-1844558338 .icon.jsx-1844558338{min-width:13px;height:13px;margin-right:5px;}
.same.jsx-1844558338 .icon.jsx-1844558338 svg{height:13px;}
.Button.jsx-1844558338:disabled,.Button.jsx-1844558338:hover.jsx-1844558338:disabled{color:#D1D3D8;background-color:#E6EAEB;}
.primary.jsx-1844558338{background-color:#FFC844;}
.primary.jsx-1844558338:hover{background-color:#FBE28D;}
.secondary.jsx-1844558338{color:#FFFFFF;background-color:#33AACC;}
.secondary.jsx-1844558338:hover{background-color:#7ECCE2;}
.red.jsx-1844558338{color:#FFFFFF;background-color:#FE5C5C;}
.green.jsx-1844558338{height:35px;color:#FFFFFF;background-color:#FFFFFF;}
.green.jsx-1844558338 .icon.jsx-1844558338 svg{fill:#36D57D;width:35px;height:35px;}
.grey.jsx-1844558338{color:#3D4251;background-color:#D1D3D8;}
.grey.jsx-1844558338:hover{color:#3D4251;background-color:#E6EAEB;}
.big.jsx-1844558338{font-size:15px;height:42px;}
.extra.jsx-1844558338{font-size:17px;height:45px;}
.border.jsx-1844558338{border:1px solid #E3E7E8;}
.border.jsx-1844558338:hover{border:1px solid #33AACC;}
.seeAll.jsx-1844558338{border:1px solid #33AACC;}
.seeAll.jsx-1844558338:hover{border:1px solid #FFC844;}
.iconButton.jsx-1844558338:hover{color:#33AACC;}
.minWidth.jsx-1844558338{min-width:85px;}
.noPadding.jsx-1844558338{padding:0;}
@media (min-width:800px){.icon.jsx-1844558338{min-width:13px;height:13px;margin-right:5px;}.icon.jsx-1844558338 svg{height:13px;}.big.jsx-1844558338 .icon.jsx-1844558338{min-width:15px;height:15px;}.big.jsx-1844558338 svg{height:15px;}.extra.jsx-1844558338 .icon.jsx-1844558338,.extraIcon.jsx-1844558338{min-width:17px;height:17px;}.extra.jsx-1844558338 svg,.extraIcon.jsx-1844558338 svg{height:17px;}.green.jsx-1844558338{padding:0 15px;color:#FFFFFF;background-color:#36D57D;}.green.jsx-1844558338 .icon.jsx-1844558338 svg{width:13px;height:13px;fill:#FFFFFF;}.forcePadding.jsx-1844558338{padding:0 15px;}}</style><style id="__jsx-3863678361">.ActionBarSearch.jsx-3863678361{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;}</style><style id="__jsx-728636942">.SubmitAnArticleButton.jsx-728636942{margin-left:5px;}
.mobileButton.jsx-728636942{display:block !important;}
.mobileButton.jsx-728636942 svg{fill:#36D57D;width:35px;height:35px;}
.SubmitAnArticleButton.jsx-728636942 .desktopButton{display:none !important;}
@media (min-width:800px){.mobileButton.jsx-728636942{display:none !important;}.SubmitAnArticleButton.jsx-728636942 .desktopButton{display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;}}</style><style id="__jsx-3196442269">.ActionBarAuth.jsx-3196442269{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}
.wrapper.jsx-3196442269{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;cursor:pointer;}
.name.jsx-3196442269{margin-right:9px;font-size:13px;font-weight:bold;color:#3D4251;text-decoration:none;}
.name.jsx-3196442269:hover{color:#33AACC;}
.logout.jsx-3196442269{font-size:15px;padding:10px;color:#3D4251;display:inline-block;min-width:100px;}
.logout.jsx-3196442269:hover{background-color:#F0F4F5;border-bottom:solid 1px #E3E7E8;}
.menuList.jsx-3196442269 a.jsx-3196442269{display:block;}</style><style id="__jsx-2159026896">.ActionBar.jsx-2159026896{height:50px;margin-top:50px;padding:0 5px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#FFFFFF;border-bottom:1px solid #E3E7E8;}
.authBlock.jsx-2159026896{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-fles-direction:row;-ms-fles-direction:row;fles-direction:row;}
.Page.content .ActionBar{margin-bottom:10px;}
@media (min-width:800px){.ActionBar.jsx-2159026896{width:calc(100% - 220px);height:50px;margin-top:0;margin-bottom:0;padding:0 25px;position:fixed;z-index:300;}}</style><style id="__jsx-3889859319">.Title.jsx-3889859319{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
.Title.jsx-3889859319 .icon.jsx-3889859319{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-width:18px;height:18px;}
.icon.jsx-3889859319 svg{-webkit-flex:1 1 0;-ms-flex:1 1 0;flex:1 1 0;height:18px;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;fill:#33AACC;}
.Title.jsx-3889859319 .h1.jsx-3889859319,.Title.jsx-3889859319 h1.jsx-3889859319{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin:auto 0 auto 9px;font-size:22px;text-transform:capitalize;}
.Title.jsx-3889859319 .status.jsx-3889859319{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;padding:10px;}</style><style id="__jsx-1514242801">.TitleBar.jsx-1514242801{height:50px;padding:0 5px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;line-height:50px;background-color:#FFFFFF;border-bottom:1px solid #E3E7E8;margin-bottom:65px;}
.filter.jsx-1514242801{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:48px;}
.action.jsx-1514242801{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;line-height:normal;}
.title.jsx-1514242801{height:65px;line-height:65px;-webkit-flex:0 0 100%;-ms-flex:0 0 100%;flex:0 0 100%;-webkit-order:1;-ms-flex-order:1;order:1;text-align:center;}
h1.jsx-1514242801{margin:0 0;}
.Page.content .TitleBar{display:none;}
@media (min-width:800px){.TitleBar.jsx-1514242801{height:50px;padding:0 25px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-wrap:nowrap;-ms-flex-wrap:nowrap;flex-wrap:nowrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-bottom:29px;margin-top:50px;background-color:#FFFFFF;border-bottom:1px solid #E3E7E8;}.filter.jsx-1514242801{-webkit-flex:0 0 33%;-ms-flex:0 0 33%;flex:0 0 33%;line-height:normal;}.action.jsx-1514242801{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;line-height:normal;-webkit-flex:0 0 33%;-ms-flex:0 0 33%;flex:0 0 33%;}.action.jsx-1514242801 a{line-height:0;}.title.jsx-1514242801{-webkit-order:0;-ms-flex-order:0;order:0;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;text-align:center;}h1.jsx-1514242801{margin:0 0;}}</style><style id="__jsx-3293774837">.CommentCounter.jsx-3293774837{width:54px;height:54px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:4px;border:1px solid #E6EAEB;background-color:#F0F4F5;cursor:pointer;}
.CommentCounter.jsx-3293774837 .icon.jsx-3293774837{font-size:13px;line-height:0;color:#33AACC;}
.CommentCounter.jsx-3293774837 .icon.jsx-3293774837 svg{width:16px;height:16px;fill:#33AACC;}
.CommentCounter.jsx-3293774837 .count.jsx-3293774837{font-size:13px;font-weight:bold;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;color:#686F75;}
.CommentCounter.jsx-3293774837:hover{background-color:#FFFFFF;}</style><style id="__jsx-1972554161">.Upvote.jsx-1972554161{position:relative;width:54px;height:54px;overflow:hidden;border-radius:4px;border:1px solid #E6EAEB;background-color:#F0F4F5;cursor:pointer;}
.Upvote.news.jsx-1972554161{width:45px;height:35px;border:none;background-color:transparent;}
.Upvote.comment.jsx-1972554161{width:45px;height:25px;border:none;background-color:transparent;}
.Upvote.jsx-1972554161>div.jsx-1972554161{position:absolute;top:0;-webkit-transition:all 0.15s ease-in-out;transition:all 0.15s ease-in-out;}
.Upvote.upvoted.jsx-1972554161>div.jsx-1972554161{top:-54px;}
.Upvote.upvoted.news.jsx-1972554161>div.jsx-1972554161{top:-35px;}
.Upvote.upvoted.comment.jsx-1972554161>div.jsx-1972554161{top:-25px;}
.Upvote.jsx-1972554161>div.jsx-1972554161>div.jsx-1972554161{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;width:54px;height:54px;}
.Upvote.news.jsx-1972554161>div.jsx-1972554161>div.jsx-1972554161{width:45px;height:35px;}
.Upvote.comment.jsx-1972554161>div.jsx-1972554161>div.jsx-1972554161{width:45px;height:25px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}
.Upvote.jsx-1972554161 .icon.jsx-1972554161{font-size:13px;line-height:0;color:#33AACC;}
.Upvote.comment.jsx-1972554161 .icon.jsx-1972554161{padding-right:5px;}
.Upvote.comment.jsx-1972554161 .count.jsx-1972554161{padding-bottom:1px;}
.Upvote.jsx-1972554161 .icon.jsx-1972554161 svg{width:12px;height:12px;fill:#33AACC;}
.Upvote.jsx-1972554161 .count.jsx-1972554161{font-size:13px;font-weight:bold;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;color:#686F75;}
.Upvote.jsx-1972554161:hover{background-color:#FFFFFF;}
.Upvote.news.jsx-1972554161:hover,.Upvote.comment.jsx-1972554161:hover{background-color:#EBF4F7;}
.Upvote.news.jsx-1972554161:hover .count.jsx-1972554161,.Upvote.comment.jsx-1972554161:hover .count.jsx-1972554161{color:#33AACC;}
.Upvote.jsx-1972554161 .voted.jsx-1972554161{background-color:#33AACC;border-color:#33AACC;}
.Upvote.news.jsx-1972554161 .voted.jsx-1972554161,.Upvote.comment.jsx-1972554161 .voted.jsx-1972554161{background-color:#FFFFFF;border:none;}
.Upvote.jsx-1972554161 .voted.jsx-1972554161 .icon.jsx-1972554161 svg{fill:#195B73;}
.Upvote.news.jsx-1972554161 .voted.jsx-1972554161 .icon.jsx-1972554161 svg,.Upvote.comment.jsx-1972554161 .voted.jsx-1972554161 .icon.jsx-1972554161 svg{fill:#36D57D;}
.Upvote.jsx-1972554161 .voted.jsx-1972554161 .count.jsx-1972554161{color:#FFFFFF;}
.Upvote.news.jsx-1972554161 .voted.jsx-1972554161 .count.jsx-1972554161,.Upvote.comment.jsx-1972554161 .voted.jsx-1972554161 .count.jsx-1972554161{color:#36D57D;}
@media (min-width:800px){.Upvote.news.jsx-1972554161{height:45px;}.Upvote.comment.jsx-1972554161{height:25px;}.Upvote.upvoted.news.jsx-1972554161>div.jsx-1972554161{top:-45px;}.Upvote.upvoted.comment.jsx-1972554161>div.jsx-1972554161{top:-25px;}.Upvote.news.jsx-1972554161>div.jsx-1972554161>div.jsx-1972554161{height:45px;}.Upvote.comment.jsx-1972554161>div.jsx-1972554161>div.jsx-1972554161{height:25px;}}</style><style id="__jsx-494086174">.Social.jsx-494086174{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;}
.icons.jsx-494086174{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
.icon.jsx-494086174{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:28px;height:28px;border:1px solid #E3E7E8;background-color:#FFFFFF;border-radius:50%;}
.icon.jsx-494086174:hover{background-color:#F0F4F5;}
.centerIcon.jsx-494086174{margin:0 10px;}
.icon.jsx-494086174 svg{fill:#686F75;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;}
@media (min-width:800px){.Social.jsx-494086174{margin-top:18px;}.vertical.jsx-494086174{margin-top:10px;}.vertical.jsx-494086174 .icons.jsx-494086174{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.vertical.jsx-494086174 .centerIcon.jsx-494086174{margin:10px 0;}}</style><style id="__jsx-3208234818">.Avatar.jsx-3208234818{display:inline-block;background-size:cover;background-color:#E6EAEB;background-repeat:no-repeat;}</style><style id="__jsx-566588255">.Author.jsx-566588255 a.jsx-566588255{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;cursor:pointer;}
.info.jsx-566588255{margin-left:9px;white-space:nowrap;}
.mirrored.jsx-566588255 .info.jsx-566588255{margin:0 9px 0 0;-webkit-order:-1;-ms-flex-order:-1;order:-1;}
.name.jsx-566588255{font-size:13px;font-weight:bold;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;color:#3D4251;text-decoration:none;}
.name.jsx-566588255:hover{color:#33AACC;}
.date.jsx-566588255{font-size:11px;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;color:#686F75;}</style><style id="__jsx-1764811326">.Tag.jsx-1764811326{display:inline-block;border-radius:4px;background-color:#F0F4F5;border:solid 1px #E6EAEB;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;color:#686F75;cursor:default;}
.title.jsx-1764811326{line-height:20px;padding:0 8px;font-size:11px;text-transform:uppercase;}
.Tag.mustRead.jsx-1764811326{background-color:#AD86CE;border-color:#AD86CE;color:#FFFFFF;font-weight:bold;}
.Tag.jsx-1764811326:hover{background-color:#FFFFFF;}
.Tag.mustRead.jsx-1764811326:hover{background-color:#CEABEC;border-color:#CEABEC;}</style><style id="__jsx-1022557955">.more.jsx-1022557955{font-size:11px;cursor:default;color:#686F75;}
.more.jsx-1022557955:hover{text-decoration:underline;}</style><style id="__jsx-422934526">.tooltipInner .Tag{margin:4px;}
.rc-tooltip{position:absolute;z-index:200;display:block;visibility:visible;line-height:1.5;font-size:12px;border-radius:4px;}
.rc-tooltip-hidden{display:none;}
.rc-tooltip-inner{padding:12px;color:#333333;text-align:left;text-decoration:none;background-color:#ffffff;border-radius:4px;min-height:34px;border:1px solid #e3e7e8;}
.rc-tooltip-arrow,.rc-tooltip-arrow-inner{position:absolute;width:0;height:0;border-color:transparent;border-style:solid;}
.rc-tooltip-placement-top .rc-tooltip-arrow,.rc-tooltip-placement-topLeft .rc-tooltip-arrow,.rc-tooltip-placement-topRight .rc-tooltip-arrow{bottom:-5px;margin-left:-6px;border-width:6px 6px 0;border-top-color:#e3e7e8;}
.rc-tooltip-placement-top .rc-tooltip-arrow-inner,.rc-tooltip-placement-topLeft .rc-tooltip-arrow-inner,.rc-tooltip-placement-topRight .rc-tooltip-arrow-inner{bottom:1px;margin-left:-6px;border-width:6px 6px 0;border-top-color:#ffffff;}
.rc-tooltip-placement-top .rc-tooltip-arrow{left:50%;}
.rc-tooltip-placement-topLeft .rc-tooltip-arrow{left:15%;}
.rc-tooltip-placement-topRight .rc-tooltip-arrow{right:15%;}
.rc-tooltip-placement-right .rc-tooltip-arrow,.rc-tooltip-placement-rightTop .rc-tooltip-arrow,.rc-tooltip-placement-rightBottom .rc-tooltip-arrow{left:-5px;margin-top:-6px;border-width:6px 6px 6px 0;border-right-color:#e3e7e8;}
.rc-tooltip-placement-right .rc-tooltip-arrow-inner,.rc-tooltip-placement-rightTop .rc-tooltip-arrow-inner,.rc-tooltip-placement-rightBottom .rc-tooltip-arrow-inner{left:1px;margin-top:-6px;border-width:6px 6px 6px 0;border-right-color:#ffffff;}
.rc-tooltip-placement-right .rc-tooltip-arrow{top:50%;}
.rc-tooltip-placement-rightTop .rc-tooltip-arrow{top:15%;margin-top:0;}
.rc-tooltip-placement-rightBottom .rc-tooltip-arrow{bottom:15%;}
.rc-tooltip-placement-left .rc-tooltip-arrow,.rc-tooltip-placement-leftTop .rc-tooltip-arrow,.rc-tooltip-placement-leftBottom .rc-tooltip-arrow{right:-5px;margin-top:-6px;border-width:6px 0 6px 6px;border-left-color:#e3e7e8;}
.rc-tooltip-placement-left .rc-tooltip-arrow-inner,.rc-tooltip-placement-leftTop .rc-tooltip-arrow-inner,.rc-tooltip-placement-leftBottom .rc-tooltip-arrow-inner{right:1px;margin-top:-6px;border-width:6px 0 6px 6px;border-left-color:#ffffff;}
.rc-tooltip-placement-left .rc-tooltip-arrow{top:50%;}
.rc-tooltip-placement-leftTop .rc-tooltip-arrow{top:15%;margin-top:0;}
.rc-tooltip-placement-leftBottom .rc-tooltip-arrow{bottom:15%;}
.rc-tooltip-placement-bottom .rc-tooltip-arrow,.rc-tooltip-placement-bottomLeft .rc-tooltip-arrow,.rc-tooltip-placement-bottomRight .rc-tooltip-arrow{top:-5px;margin-left:-6px;border-width:0 6px 6px;border-bottom-color:#e3e7e8;}
.rc-tooltip-placement-bottom .rc-tooltip-arrow-inner,.rc-tooltip-placement-bottomLeft .rc-tooltip-arrow-inner,.rc-tooltip-placement-bottomRight .rc-tooltip-arrow-inner{top:1px;margin-left:-6px;border-width:0 6px 6px;border-bottom-color:#ffffff;}
.rc-tooltip-placement-bottom .rc-tooltip-arrow{left:50%;}
.rc-tooltip-placement-bottomLeft .rc-tooltip-arrow{left:15%;}
.rc-tooltip-placement-bottomRight .rc-tooltip-arrow{right:15%;}
.rc-tooltip.rc-tooltip-zoom-enter,.rc-tooltip.rc-tooltip-zoom-leave{display:block;}
.rc-tooltip-zoom-enter,.rc-tooltip-zoom-appear{opacity:0;-webkit-animation-duration:0.3s;-webkit-animation-duration:0.3s;animation-duration:0.3s;-webkit-animation-fill-mode:both;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-timing-function:cubic-bezier(0.18,0.89,0.32,1.28);-webkit-animation-timing-function:cubic-bezier(0.18,0.89,0.32,1.28);animation-timing-function:cubic-bezier(0.18,0.89,0.32,1.28);-webkit-animation-play-state:paused;-webkit-animation-play-state:paused;animation-play-state:paused;}
.rc-tooltip-zoom-leave{-webkit-animation-duration:0.3s;-webkit-animation-duration:0.3s;animation-duration:0.3s;-webkit-animation-fill-mode:both;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-timing-function:cubic-bezier(0.6,-0.3,0.74,0.05);-webkit-animation-timing-function:cubic-bezier(0.6,-0.3,0.74,0.05);animation-timing-function:cubic-bezier(0.6,-0.3,0.74,0.05);-webkit-animation-play-state:paused;-webkit-animation-play-state:paused;animation-play-state:paused;}
.rc-tooltip-zoom-enter.rc-tooltip-zoom-enter-active,.rc-tooltip-zoom-appear.rc-tooltip-zoom-appear-active{-webkit-animation-name:rcToolTipZoomIn;-webkit-animation-name:rcToolTipZoomIn;animation-name:rcToolTipZoomIn;-webkit-animation-play-state:running;-webkit-animation-play-state:running;animation-play-state:running;}
.rc-tooltip-zoom-leave.rc-tooltip-zoom-leave-active{-webkit-animation-name:rcToolTipZoomOut;-webkit-animation-name:rcToolTipZoomOut;animation-name:rcToolTipZoomOut;-webkit-animation-play-state:running;-webkit-animation-play-state:running;animation-play-state:running;}
@-webkit-keyframes rcToolTipZoomIn{0%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}100%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}}
@-webkit-keyframes rcToolTipZoomIn{0%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}100%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}}
@keyframes rcToolTipZoomIn{0%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}100%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}}
@-webkit-keyframes rcToolTipZoomOut{0%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}100%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}}
@-webkit-keyframes rcToolTipZoomOut{0%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}100%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}}
@keyframes rcToolTipZoomOut{0%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}100%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}}</style><style id="__jsx-2792531181">.TagLine.jsx-2792531181{display:inline-block;white-space:nowrap;}
.TagLine.jsx-2792531181>.Tag{margin-right:10px;}
.more.jsx-2792531181{font-size:11px;}</style><style id="__jsx-1091791248">.markdown{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;font-family:'Lora',serif;font-size:16px;padding:1.5em;}
.markdown [data-datacamp-exercise]{overflow:hidden;}
.markdown>div{width:100%;}
.markdown div{outline:none;}
.markdown hr{border:0;border-bottom:1px solid #E6EAEB;margin:3em 0;}
.markdown div[data-type="mathjax"]{margin:1.5em 0;}
.markdown p{font-family:'Lora',serif;font-size:1em;line-height:1.8em;color:#3D4251;}
.markdown .powered-by-datacamp+p{margin-top:1.5em !important;}
.markdown p+p,.markdown p+img,.markdown p+div,.markdown p+table,.markdown p+ol,.markdown p+ul,.markdown p+nav,.markdown div+p,.markdown p+iframe,.markdown iframe+p,.markdown pre+img,.markdown pre+p{margin-top:1.5em !important;}
.markdown h2{font-family:'Lato',sans-serif;font-size:1.5em;font-weight:700;color:#3D4251;line-height:1.3em;margin:1.5em 0 0.5em;}
.markdown h3{font-family:'Lato',sans-serif;font-size:1.1em;font-weight:700;color:#3D4251;line-height:1.2em;margin:1.5em 0 0.5em;}
.markdown h4{font-family:'Lato',sans-serif;font-size:1em;font-weight:700;color:#3D4251;line-height:1.2em;margin:1.5em 0 0.5em;}
.markdown .videoWrapper{position:relative;padding-bottom:47.25%;padding-top:25px;height:0;margin-bottom:1.5em;}
.markdown .videoWrapper iframe{position:absolute;top:0;left:0;width:100%;height:100%;}
.markdown p code,.markdown li code{display:inline-block;padding:0 5px;border-radius:4px;font-family:'Roboto Mono',monospace;font-size:0.9em;line-height:1.6em;color:#3D4251;background-color:#E6EAEB;}
.markdown a code{color:#33AACC;}
.markdown pre{padding:1em 1.5em;font-family:'Roboto Mono',monospace;font-size:0.9em;background-color:#002B36 !important;border-radius:4px;overflow-x:auto;-webkit-overflow-scrolling:touch;}
.markdown pre code{padding:0;font-size:0.9em;line-height:2em;background-color:#002B36;overflow-x:visible;-webkit-overflow-scrolling:touch;}
.markdown img{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;margin:auto;max-width:100%;height:auto !important;}
.markdown img+p{margin-top:1.5em !important;}
.markdown nav+p,.markdown nav+div{margin-top:1.5em !important;}
.markdown iframe{width:100%;}
.markdown ul,.markdown ol{font-family:'Lora',serif;color:#3D4251;padding:0 0 0 1em;}
.markdown ul+p,.markdown ol+p,.markdown ul+div,.markdown ol+div,.markdown ul+pre,.markdown ol+pre{margin-top:1.5em !important;}
.markdown div.datacamp-exercise ul,.markdown div.datacamp-exercise ol{background-color:initial;margin:initial;padding:initial;width:initial;list-style-position:initial;line-height:initial;}
.markdown ul.oneliner,.markdown ol.oneliner{padding:0;list-style-position:inside;line-height:1.5em;}
.markdown li{line-height:1.8em;}
.markdown li+li{margin-top:1em;}
.markdown div.datacamp-exercise li{padding-left:initial;background-color:initial;font-size:initial;font-weight:initial;line-height:initial;}
.markdown li p{margin:0;font-size:1em;line-height:1.8em;}
.markdown ul.oneliner li,.markdown ol.oneliner li{padding-left:0;white-space:nowrap;text-overflow:ellipsis;overflow:hidden;}
.markdown ul ul{margin:0;list-style:circle;}
.markdown ol ol{margin:0;}
.markdown ol ul,.markdown ul ol{margin:0;}
.markdown a{font-weight:400;text-decoration:none;color:#33AACC;}
.markdown a:hover{text-decoration:underline;}
.markdown div.datacamp-exercise a{font-weight:initial;text-decoration:initial;}
.markdown div.datacamp-exercise a:hover{text-decoration:initial;}
.markdown div.datacamp-exercise li+li{margin-top:unset;}
.markdown blockquote{margin:1.5em 0;font-family:'Lato',sans-serif;color:#3D4251;font-weight:300;font-size:1.5em;font-style:italic;line-height:2em;}
.markdown blockquote::before{display:block;margin-bottom:15px;width:35px;height:35px;font-family:'Lora',serif;font-size:36px;font-weight:bold;font-style:normal;line-height:60px;text-align:center;color:#33AACC;content:'â€œ';border:1px solid #E3E7E8;border-radius:50%;}
.markdown table{width:100% !important;border:1px solid #E3E7E8;border-radius:4px;overflow:hidden;border-collapse:separate;border-spacing:0;}
.markdown table th,.markdown table td{padding:0.75em;}
.markdown table tr th,.markdown table tr td{border:1px solid #E3E7E8;vertical-align:middle;}
.markdown table thead tr th{font-family:'Lato',sans-serif;background-color:#F0F4F5;}
.markdown table tr:nth-child(even){background-color:#F0F4F5;}
.markdown table+p{margin-top:1.5em !important;}
.markdown table+img{margin-top:1.5em !important;}
.markdown table+div{margin-top:1.5em !important;}
.markdown .dcl-content--tab-body{margin-top:0 !important;}
@media (min-width:800px){.markdown{font-size:20px;padding:0;}.markdown h2,.markdown h3,.markdown h4{margin:1.5em 0 0.5em;}.markdown p{margin:0;}.markdown li p{font-size:inherit;line-height:inherit;}.markdown ul+p,.markdown ol+p,.markdown ul+div,.markdown ol+div,.markdown ul+pre,.markdown ol+pre{margin-top:1.5em !important;}.markdown blockquote{position:relative;margin:50px 55px;}.markdown blockquote::before{position:absolute;left:-55px;}}
.output_wrapper{overflow-x:auto;-webkit-overflow-scrolling:touch;}</style><style id="__jsx-3956319705">.PostAComment.jsx-3956319705{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;padding:0 0 60px;}</style><style id="__jsx-2506565400">.SidebarSocial.jsx-2506565400{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;font-size:11px;font-weight:bold;}
.rss.jsx-2506565400{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;line-height:50px;}
.rss.jsx-2506565400 svg{padding-right:7px;fill:#FFC844;}
.rss.jsx-2506565400 a.jsx-2506565400{text-decoration:none;color:#FFFFFF;}
.rss.jsx-2506565400 a.jsx-2506565400:hover{text-decoration:none;color:#FFC844;}
.icons.jsx-2506565400{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
.icon.jsx-2506565400{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 7px;width:30px;height:30px;background-color:#195B73;border-radius:50%;}
.icon.jsx-2506565400 svg{fill:#7ECCE2;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;-webkit-flex:1 1 0;-ms-flex:1 1 0;flex:1 1 0;}
.icon.jsx-2506565400:hover svg{fill:#FFFFFF;}
.menu.jsx-2506565400{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;line-height:44px;}
.menuItem.jsx-2506565400{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;padding:5px;text-decoration:none;color:#195B73;}
.menuItem--active.jsx-2506565400,a.jsx-2506565400:hover{color:#F0F4F5;}
@media (min-width:800px) and (min-height:585px){.SidebarSocial.jsx-2506565400{width:220px;position:fixed;bottom:0;left:0;}}</style><style id="__jsx-879378290">.BottomBar.jsx-879378290{position:fixed;bottom:0;width:100vw;padding:15px;z-index:300;background-color:#FFFFFF;box-shadow:0 -2px 26px 0 rgba(168,168,168,0.5);}
.BottomBar.editor.jsx-879378290{padding-top:5px;}
.barView.jsx-879378290{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
.editor.jsx-879378290 .barView.jsx-879378290{display:none;}
.blueBar.jsx-879378290{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;height:50px;margin-left:15px;padding:0 15px;color:#33AACC;line-height:50px;border-radius:4px;background-color:#ebf4f7;}
.editorView.jsx-879378290{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;}
.editorView.jsx-879378290 .avatar{display:none;}
.bar.jsx-879378290 .editorView.jsx-879378290{display:none;}
.hideBar.jsx-879378290{display:none;}
@media (min-width:800px){.BottomBar.jsx-879378290{width:calc(100% - 220px);margin-left:220px;}.editorView.jsx-879378290 .avatar{display:block;}}</style><style id="__jsx-1028385822">.Layout.jsx-1028385822{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;}
.Layout--openMenu.jsx-1028385822{min-height:100vh;}
.Main.jsx-1028385822{min-height:100vh;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;background-color:#F0F4F5;}
.Layout--banner.jsx-1028385822 .Main.jsx-1028385822{margin-top:55px;min-height:calc(100vh - 55px);}
.Layout.bar.jsx-1028385822:not(.Layout--openMenu) .SidebarSocial{margin-bottom:90px;}
.Layout.editor.jsx-1028385822:not(.Layout--openMenu) .SidebarSocial{margin-bottom:300px;}
@media (min-width:800px){.Main.jsx-1028385822{margin-left:220px;}.Layout--banner.jsx-1028385822 .Main.jsx-1028385822{margin-top:80px;min-height:calc(100vh - 80px);}.Layout.bar.jsx-1028385822:not(.Layout--openMenu) .SidebarSocial,.Layout.editor.jsx-1028385822:not(.Layout--openMenu) .SidebarSocial{margin-bottom:0;}.Layout.bar.jsx-1028385822 .Main > div:last-child{margin-bottom:90px;}.Layout.editor.jsx-1028385822 .Main > div:last-child{margin-bottom:300px;}}</style><style id="__jsx-1464850800">.Tutorial.jsx-1464850800{margin:0 0px 30px;padding:20px 0 0;background-color:#FFFFFF;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}
.preface.jsx-1464850800{margin:0 20px;}
.author.jsx-1464850800{margin-bottom:10px;}
h1.jsx-1464850800{margin-top:20px;}
.illustration.jsx-1464850800{margin-bottom:30px;}
.illustration.jsx-1464850800 img.jsx-1464850800{max-width:100%;}
.Tutorial.jsx-1464850800 .social__top .voteAndSocial,.social__bottom.jsx-1464850800 .voteAndSocial.jsx-1464850800{padding:40px 20px 20px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:flex-end;-webkit-box-align:flex-end;-ms-flex-align:flex-end;align-items:flex-end;}
.Tutorial.jsx-1464850800 .voteAndSocial.jsx-1464850800>div.jsx-1464850800{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
.Tutorial.jsx-1464850800 .voteAndSocial.jsx-1464850800>div.jsx-1464850800 .CommentCounter{margin-left:10px;}
@media (min-width:800px){.Tutorial.jsx-1464850800{margin:0 auto 30px;padding:30px 100px 100px;max-width:1120px;border-radius:4px;border:1px solid #E3E7E8;}.preface.jsx-1464850800{margin:0;}.Tutorial.jsx-1464850800 .social__top{position:absolute;margin-top:220px;}.Tutorial.jsx-1464850800 .social__top .voteAndSocial{position:absolute;left:-100px;top:0;width:100px;height:auto;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.Tutorial.jsx-1464850800 .voteAndSocial.jsx-1464850800>div.jsx-1464850800{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.Tutorial.jsx-1464850800 .voteAndSocial > div .CommentCounter{margin-left:0;margin-bottom:10px;}}</style><style id="__jsx-2612661084">*{box-sizing:border-box;}
html,body{min-height:100vh;margin:0;padding:0;background-image: linear-gradient(207deg, #2388B0, #33AACC); background-size: 100vw 100vh; background-attachment: fixed; background-repeat: no-repeat;}
body.ReactModal__Body--open{overflow:hidden;}
.ReactModal__Content{width:100%;}
img{margin:auto;}
.mobileOnlyShow{display:block !important;}
.mobileOnlyHide{display:none !important;}
.mobileOnly{display:block !important;}
.desktopOnly{display:none !important;}
@media (min-width:800px){body{background-image: linear-gradient(207deg, #2388B0, #33AACC); background-size: 220px 100vh; background-attachment: fixed; background-repeat: no-repeat;}.ReactModal__Content{width:auto;}.mobileOnlyShow{display:block !important;}.mobileOnlyHide{display:block !important;}.mobileOnly{display:none !important;}.desktopOnly{display:block !important;}}</style><style id="__jsx-63629563">body,input,button,select,textarea{font-family:'Lato',sans-serif;color:#686F75;font-size:15px;}
h1,.h1,h2,h3,h4,h5{font-family:'Lato',sans-serif;}
.pageTitle{font-family:'Lato',sans-serif;font-size:32px;font-weight:bold;line-height:1.3em;margin-bottom:0.5em;}
.pageDescription{font-family:'Lora',serif;font-size:20.8px;line-height:1.5em;margin-bottom:1.4em;color:#3D4251;}
h1,.h1{font-size:29px;color:#3D4251;font-weight:bold;}
h2{font-size:20px;-webkit-letter-spacing:0.3px;-moz-letter-spacing:0.3px;-ms-letter-spacing:0.3px;letter-spacing:0.3px;line-height:1.33;font-weight:bold;margin:18px 0px;color:#3D4251;}
h2.blue{color:#33AACC;}
a{color:#33AACC;text-decoration:none;}
.blocText{font-size:15px;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:1.47;color:#686F75;}
label{display:block;width:100%;margin-bottom:8px;font-size:13px;}
label span{float:right;font-weight:300;}
input,textarea{padding:15px;font-weight:300;color:#3D4251;background-color:#F0F4F5;border:1px solid transparent;border-radius:4px;outline-style:none;}
input:disabled,textarea:disabled{color:#686F75;background-color:#E6EAEB;}
input.error,textarea.error{border:1px solid #FE5C5C;}
input:focus,textarea:focus{border:1px solid #33AACC;-webkit-transition:border 150ms ease-out;transition:border 150ms ease-out;}
input::-webkit-input-placeholder,textarea::-webkit-input-placeholder{color:#33AACC;-webkit-transition:color 150ms ease-out;transition:color 150ms ease-out;}
input::-moz-placeholder,textarea::-moz-placeholder{color:#33AACC;-webkit-transition:color 150ms ease-out;transition:color 150ms ease-out;}
input:-ms-input-placeholder,textarea:-ms-input-placeholder{color:#33AACC;-webkit-transition:color 150ms ease-out;transition:color 150ms ease-out;}
input::placeholder,textarea::placeholder{color:#33AACC;-webkit-transition:color 150ms ease-out;transition:color 150ms ease-out;}
input:focus::-webkit-input-placeholder,textarea:focus::-webkit-input-placeholder{color:transparent;}
input:focus::-moz-placeholder,textarea:focus::-moz-placeholder{color:transparent;}
input:focus:-ms-input-placeholder,textarea:focus:-ms-input-placeholder{color:transparent;}
input:focus::placeholder,textarea:focus::placeholder{color:transparent;}
input.small,textarea.small{margin-bottom:19px;padding:8px 10px;font-size:13px;}
textarea.small{min-height:55px;}
@media (min-width:800px){h1,.h1{font-size:36px;}h2{font-size:32px;}.pageTitle{font-size:40px;}.pageDescription{font-size:26px;}}</style></head><body><div id="__next"><div data-reactroot=""><div class="Page content"><div class="jsx-1028385822 Layout bar"><div class="jsx-499055511 SidebarMenu"><div class="jsx-396080995 Logo"><a href="https://www.datacamp.com" class="jsx-396080995 Logo__image"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1367.47 306.77"><path d="M201.49 130.16c2.64-10.87 2.31-21.09-3.62-35.38 0 0 9.55 3.71 8.56-3.65S194.91 76 187 71.11c-3.48-2.17-12.89-6.79-25.52-9.6l2.36-5.32-.66-.32c-19.59-9.54-42.24-.89-42.47-.8l-.7.27 2.24 5.92a78 78 0 0 0-19 7.09C50 96.68 77 152 93.5 165.17s8.89 40 0 64.31H165c1-4.55.13 2.24 3.46-15.91s16.18-11.41 26.3-12 8.52-7.66 8.4-15.56c4.94-2.22 4.94-4.57 1.64-7.41 4.28-4 3.62-4 1.32-7.83s-.66-5.68-.66-5.68 6.92-1.35 7.9-5c.32-7.89-14.5-19.09-11.87-29.93zm-14.78-2.28l-10.59-3.6c-.56 1.45-6 14.9-13 17.27l-6.68-8.84c-3.21 2.51-10.54 4.72-14.07 3.69v-.32h-.19l.2 24.36c-14.05 1.45-30.84-7.06-38.88-18.77l-8.78 6.67c-10-12.07-11.51-26.62-11.62-35h11.77c0-9.27 3.62-21.34 8.72-28l9.23 6.52c.75-.93 7.72-9.27 17.05-12.28h.14l-6.67-17.64c14.09-4.06 24.44-4 35.7.19a3.87 3.87 0 0 1 1 .48l.18.13-1.99 5.26c.53.23 8 3.59 11.09 6.74l2.49-3.44a42.47 42.47 0 0 1 19 25.34L187 98v-.1a55.32 55.32 0 0 1-.28 29.98z"></path><path d="M141.69 95.69a11.77 11.77 0 1 0 11.76 11.77 11.78 11.78 0 0 0-11.76-11.77zM486.07 147.93a85.48 85.48 0 0 1-5.79 31.92 71.67 71.67 0 0 1-41.54 41.32 89 89 0 0 1-32.79 5.83h-60.24V68.9h60.23a88.33 88.33 0 0 1 32.79 5.85A74 74 0 0 1 464 91a72.76 72.76 0 0 1 16.29 25 85.49 85.49 0 0 1 5.78 31.93zm-30.17 0a73.61 73.61 0 0 0-3.44-23.34 48.4 48.4 0 0 0-9.95-17.49 43.45 43.45 0 0 0-15.74-11 54 54 0 0 0-20.82-3.83h-30.72v111.3h30.72a54 54 0 0 0 20.82-3.83 43.39 43.39 0 0 0 15.74-11 48.37 48.37 0 0 0 9.95-17.49 73.6 73.6 0 0 0 3.44-23.32zM505.36 130.44q19.35-17.71 46.57-17.71a45.54 45.54 0 0 1 17.6 3.22 37.21 37.21 0 0 1 13.12 9 38.4 38.4 0 0 1 8.14 13.72 52.72 52.72 0 0 1 2.79 17.49V227h-12.25a12.38 12.38 0 0 1-5.9-1.15q-2.08-1.15-3.28-4.65l-2.4-8.09a97.29 97.29 0 0 1-8.31 6.72 48.91 48.91 0 0 1-8.42 4.86 45.32 45.32 0 0 1-9.35 3 53.93 53.93 0 0 1-11 1 43.08 43.08 0 0 1-13.12-1.91 28.78 28.78 0 0 1-10.38-5.74 25.73 25.73 0 0 1-6.78-9.51 33.72 33.72 0 0 1-2.4-13.23 26.32 26.32 0 0 1 1.42-8.47 24.76 24.76 0 0 1 4.65-8 38.63 38.63 0 0 1 8.36-7.21 54.43 54.43 0 0 1 12.63-5.9 109.08 109.08 0 0 1 17.44-4.1 174.35 174.35 0 0 1 22.74-1.91v-6.56q0-11.26-4.81-16.67t-13.88-5.41a33 33 0 0 0-10.88 1.53 44.91 44.91 0 0 0-7.6 3.44q-3.28 1.91-6 3.44a11.78 11.78 0 0 1-6 1.53 7.79 7.79 0 0 1-4.81-1.48 12.33 12.33 0 0 1-3.17-3.44zm61.87 48.64a149.08 149.08 0 0 0-19.68 2 52.42 52.42 0 0 0-12.79 3.77 16.8 16.8 0 0 0-6.89 5.36 11.63 11.63 0 0 0-2.08 6.67q0 7.11 4.21 10.17t11 3.06a32 32 0 0 0 14.37-3 42.69 42.69 0 0 0 11.86-9.11zM657.79 228.72q-14.65 0-22.46-8.25t-7.82-22.79V135H616a5.26 5.26 0 0 1-3.72-1.42 5.53 5.53 0 0 1-1.53-4.26v-10.68l18-3 5.68-30.61a6 6 0 0 1 2.08-3.39 6.18 6.18 0 0 1 3.94-1.2h14v35.36h30V135h-30v60.78q0 5.25 2.57 8.2a8.85 8.85 0 0 0 7 3 12.7 12.7 0 0 0 4.21-.6 23 23 0 0 0 3-1.26q1.26-.66 2.24-1.26a3.75 3.75 0 0 1 2-.6 3.1 3.1 0 0 1 2 .6 9.23 9.23 0 0 1 1.64 1.8l8.09 13.12a40.87 40.87 0 0 1-13.55 7.43 50.47 50.47 0 0 1-15.86 2.51zM702.11 130.44q19.35-17.71 46.57-17.71a45.54 45.54 0 0 1 17.6 3.22 37.19 37.19 0 0 1 13.12 9 38.37 38.37 0 0 1 8.14 13.72 52.72 52.72 0 0 1 2.79 17.49V227h-12.25a12.37 12.37 0 0 1-5.9-1.15q-2.08-1.15-3.28-4.65l-2.4-8.09a97.21 97.21 0 0 1-8.31 6.72 49 49 0 0 1-8.42 4.86 45.33 45.33 0 0 1-9.35 3 53.92 53.92 0 0 1-11 1 43.08 43.08 0 0 1-13.12-1.91 28.78 28.78 0 0 1-10.38-5.74 25.74 25.74 0 0 1-6.78-9.51 33.72 33.72 0 0 1-2.4-13.23 26.31 26.31 0 0 1 1.42-8.47 24.76 24.76 0 0 1 4.65-8 38.66 38.66 0 0 1 8.36-7.21 54.45 54.45 0 0 1 12.63-5.9 109.07 109.07 0 0 1 17.44-4.1 174.32 174.32 0 0 1 22.76-1.93v-6.56q0-11.26-4.81-16.67t-13.88-5.41a33 33 0 0 0-10.88 1.53 44.89 44.89 0 0 0-7.6 3.44q-3.28 1.91-6 3.44a11.79 11.79 0 0 1-6 1.53 7.79 7.79 0 0 1-4.81-1.48 12.33 12.33 0 0 1-3.17-3.44zM764 179.09a149.06 149.06 0 0 0-19.68 2 52.43 52.43 0 0 0-12.79 3.77 16.81 16.81 0 0 0-6.89 5.36 11.63 11.63 0 0 0-2.08 6.67q0 7.11 4.21 10.17t11 3.06a32 32 0 0 0 14.37-3A42.68 42.68 0 0 0 764 198zM929.42 189.69a5.87 5.87 0 0 1 4.26 1.86l11.59 12.57a63.64 63.64 0 0 1-23.67 18.26q-14 6.34-33.72 6.34-17.6 0-31.65-6a69.71 69.71 0 0 1-24-16.73A73.19 73.19 0 0 1 817 180.4a96 96 0 0 1-5.3-32.47 90.36 90.36 0 0 1 5.68-32.63 75 75 0 0 1 16-25.52A72.5 72.5 0 0 1 858 73.11a81.77 81.77 0 0 1 31.7-6q17.27 0 30.66 5.68a73.08 73.08 0 0 1 22.83 14.91l-9.84 13.66a9 9 0 0 1-2.24 2.29 6.35 6.35 0 0 1-3.77 1 7.2 7.2 0 0 1-3.39-.93q-1.75-.93-3.83-2.29t-4.81-3a39.67 39.67 0 0 0-6.34-3 51.92 51.92 0 0 0-8.36-2.29 57.87 57.87 0 0 0-11-.93A48.74 48.74 0 0 0 870.23 96a42.16 42.16 0 0 0-15.14 10.93 50.38 50.38 0 0 0-9.84 17.49 73.27 73.27 0 0 0-3.5 23.56 69.25 69.25 0 0 0 3.77 23.72 52.12 52.12 0 0 0 10.22 17.49 43.61 43.61 0 0 0 15.2 10.81 47 47 0 0 0 18.8 3.77 81.89 81.89 0 0 0 10.88-.66 46.17 46.17 0 0 0 9-2.08 39.7 39.7 0 0 0 7.76-3.66 48.48 48.48 0 0 0 7.27-5.52 10.31 10.31 0 0 1 2.29-1.58 5.56 5.56 0 0 1 2.48-.58zM961.6 130.44q19.35-17.71 46.57-17.71a45.54 45.54 0 0 1 17.6 3.22 37.19 37.19 0 0 1 13.12 9 38.37 38.37 0 0 1 8.14 13.72 52.72 52.72 0 0 1 2.79 17.49V227h-12.24a12.37 12.37 0 0 1-5.9-1.15q-2.08-1.15-3.28-4.65l-2.4-8.09a97.21 97.21 0 0 1-8.31 6.72 49 49 0 0 1-8.42 4.86 45.33 45.33 0 0 1-9.35 3 53.92 53.92 0 0 1-11 1 43.08 43.08 0 0 1-13.12-1.91 28.78 28.78 0 0 1-10.38-5.74 25.74 25.74 0 0 1-6.78-9.51 33.73 33.73 0 0 1-2.4-13.23 26.31 26.31 0 0 1 1.42-8.47 24.76 24.76 0 0 1 4.65-8 38.66 38.66 0 0 1 8.36-7.21 54.45 54.45 0 0 1 12.63-5.9 109.07 109.07 0 0 1 17.44-4.1 174.32 174.32 0 0 1 22.74-1.91v-6.56q0-11.26-4.81-16.67t-13.88-5.41a33 33 0 0 0-10.88 1.53 44.89 44.89 0 0 0-7.6 3.44q-3.28 1.91-6 3.44a11.79 11.79 0 0 1-6 1.53 7.79 7.79 0 0 1-4.81-1.48 12.33 12.33 0 0 1-3.17-3.44zm61.87 48.64a149.06 149.06 0 0 0-19.68 2 52.43 52.43 0 0 0-12.79 3.8 16.81 16.81 0 0 0-6.89 5.36 11.63 11.63 0 0 0-2.08 6.67q0 7.11 4.21 10.17t11 3.06a32 32 0 0 0 14.37-3 42.68 42.68 0 0 0 11.86-9.13zM1078.18 227V114.81h16.51a6.63 6.63 0 0 1 6.89 4.92l1.75 8.31a63.82 63.82 0 0 1 6.18-6 39.66 39.66 0 0 1 6.89-4.7 37.29 37.29 0 0 1 7.87-3.12 35.05 35.05 0 0 1 9.24-1.15q10.6 0 17.43 5.74a34.56 34.56 0 0 1 10.22 15.25 34.23 34.23 0 0 1 6.56-9.57 35.9 35.9 0 0 1 8.63-6.5 39.77 39.77 0 0 1 10-3.72 48.07 48.07 0 0 1 10.66-1.2 44.82 44.82 0 0 1 16.51 2.84 31.69 31.69 0 0 1 12.13 8.31 36.63 36.63 0 0 1 7.49 13.34 58.1 58.1 0 0 1 2.57 18V227h-27v-71.41q0-10.71-4.7-16.12t-13.77-5.41a20.57 20.57 0 0 0-7.71 1.42 18.37 18.37 0 0 0-6.23 4.1 18.73 18.73 0 0 0-4.21 6.72 26.13 26.13 0 0 0-1.53 9.29V227h-27.11v-71.41q0-11.26-4.54-16.4t-13.28-5.14a21.83 21.83 0 0 0-11 2.9 36.66 36.66 0 0 0-9.46 7.92V227zM1289.85 130.12a58.51 58.51 0 0 1 15.63-12.57q8.74-4.81 20.55-4.81a36.79 36.79 0 0 1 16.78 3.83 37.68 37.68 0 0 1 13.12 11.09 53.26 53.26 0 0 1 8.53 17.93 90.21 90.21 0 0 1 3 24.43 76.84 76.84 0 0 1-3.39 23.28 57 57 0 0 1-9.67 18.58 44.72 44.72 0 0 1-15.19 12.3 44.22 44.22 0 0 1-19.95 4.43 40.23 40.23 0 0 1-16.07-2.9 39.23 39.23 0 0 1-11.92-8v45.91h-27V114.81h16.51a6.63 6.63 0 0 1 6.89 4.92zm1.42 67.77a26.57 26.57 0 0 0 10 7.87 29.76 29.76 0 0 0 11.75 2.3 25.79 25.79 0 0 0 11-2.3 21.63 21.63 0 0 0 8.36-7 34.62 34.62 0 0 0 5.3-11.86 67.79 67.79 0 0 0 1.86-16.89 75 75 0 0 0-1.58-16.67 32.22 32.22 0 0 0-4.54-11.09 17.92 17.92 0 0 0-7.16-6.23 22.34 22.34 0 0 0-9.57-2 27.28 27.28 0 0 0-14.32 3.55 43.46 43.46 0 0 0-11.15 10z"></path><path d="M144.06 306.77a9.83 9.83 0 0 1-4.72-1.21L23.79 242a9.82 9.82 0 0 1-5-7.56L.06 57.52a9.77 9.77 0 0 1 6.5-10.29L139.37.55a9.88 9.88 0 0 1 6.43 0L281.5 47.2a9.86 9.86 0 0 1 6.5 10.47L266.47 234.6a9.85 9.85 0 0 1-4.91 7.35l-112.68 63.56a9.84 9.84 0 0 1-4.82 1.26zm-110-77l110 60.49 107.22-60.48L271.89 60.5 142.64 16 16.15 60.5z"></path></svg></a></div><div class="jsx-499055511 icon mobileOnly"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 70"><path d="M6 1a6 6 0 1 0 0 12h68a6 6 0 0 0 0-12H6zm0 28a6 6 0 1 0 0 12h68a6 6 0 0 0 0-12H6zm0 28a6 6 0 1 0 0 12h68a6 6 0 0 0 0-12H6z"></path></svg></div></div><div class="jsx-2769082546 Menu mobileOnlyHide"><div class="jsx-2769082546 section"><h5 class="jsx-2769082546">community</h5><nav class="jsx-2769082546"><div><a target="_self" class="jsx-2769082546 item" href="/community"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 23 23"><path id="a" d="M2 4.5h20a1.5 1.5 0 0 1 0 3H2a1.5 1.5 0 0 1 0-3zm0 6h14a1.5 1.5 0 0 1 0 3H2a1.5 1.5 0 0 1 0-3zm0 6h20a1.5 1.5 0 0 1 0 3H2a1.5 1.5 0 0 1 0-3z"></path></svg></div><div class="jsx-2769082546 text">News</div><div class="jsx-2769082546 statusIcon"><svg height="20" xmlns="http://www.w3.org/2000/svg" width="40" viewBox="0 0 40 20"><g fill="none" fill-rule="evenodd"><rect width="40" height="20" fill="#FFC844" rx="4"></rect><text fill="#3D4251" font-family="Lato-Bold, Lato" font-size="11" font-weight="bold" letter-spacing=".5"><tspan x="5" y="14">BETA</tspan></text></g></svg></div></a></div><div><a target="_self" class="jsx-2769082546 item active" href="/community/tutorials"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 24.22"><path d="M16.23 24.22a2 2 0 0 1-.73-.14L7 20.79a2 2 0 0 1-1.29-1.88v-4a1.5 1.5 0 0 1 3 0v3.36l7.54 2.92 7.54-2.92v-3.45a1.5 1.5 0 0 1 3 0v4.09a2 2 0 0 1-1.29 1.88L17 24.08a2 2 0 0 1-.77.14zm-.35-2.94zm.7 0z"></path><path d="M16.23 13.35a2 2 0 0 1-.62-.1C9.17 11.16 2.36 9 1.61 8.76a2 2 0 0 1-.25-3.87l14-4.78a2 2 0 0 1 1.3 0l14 4.78a2 2 0 0 1 0 3.81l-13.8 4.55a2 2 0 0 1-.63.1zm-.31-3zM5.21 6.74c3.49 1.11 9.07 2.92 11 3.56l10.68-3.53L16 3.05z"></path></svg></div><div class="jsx-2769082546 text">Tutorials</div></a></div><div><a target="_self" class="jsx-2769082546 item" href="/community/data-science-cheatsheets"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 26"><path d="M18.5 26h-13A5.51 5.51 0 0 1 0 20.5v-15A5.51 5.51 0 0 1 5.5 0h13A5.51 5.51 0 0 1 24 5.5v15a5.51 5.51 0 0 1-5.5 5.5zM5.5 3A2.5 2.5 0 0 0 3 5.5v15A2.5 2.5 0 0 0 5.5 23h13a2.5 2.5 0 0 0 2.5-2.5v-15A2.5 2.5 0 0 0 18.5 3z"></path><path d="M16 11H8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3zM16 18H8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3z"></path></svg></div><div class="jsx-2769082546 text">Cheat Sheets</div></a></div><div><a target="_self" class="jsx-2769082546 item" href="/community/open-courses"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 34 26"><path d="M28.5 26h-23A5.51 5.51 0 0 1 0 20.5v-15A5.51 5.51 0 0 1 5.5 0h23A5.51 5.51 0 0 1 34 5.5v15a5.51 5.51 0 0 1-5.5 5.5zM5.5 3A2.5 2.5 0 0 0 3 5.5v15A2.5 2.5 0 0 0 5.5 23h23a2.5 2.5 0 0 0 2.5-2.5v-15A2.5 2.5 0 0 0 28.5 3z"></path><path d="M13.5 26a1.5 1.5 0 0 1-1.5-1.5v-22a1.5 1.5 0 0 1 3 0v22a1.5 1.5 0 0 1-1.5 1.5zM27 11h-8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3zM27 18h-8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3z"></path></svg></div><div class="jsx-2769082546 text">Open Courses</div></a></div><div><a target="_self" class="jsx-2769082546 item" href="/community/podcast"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" width="18" viewBox="0 0 18 18"><path d="M9.415 11.077h-.369a2.777 2.777 0 0 1-2.769-2.77V2.77A2.777 2.777 0 0 1 9.047 0h.368a2.777 2.777 0 0 1 2.77 2.77v5.538a2.777 2.777 0 0 1-2.77 2.769zm5.008-7.615c.573 0 1.039.464 1.039 1.038v3.462c0 3.08-2.25 5.64-5.193 6.136v1.825h2.077a1.038 1.038 0 1 1 0 2.077h-6.23a1.038 1.038 0 1 1 0-2.077h2.076v-1.825C5.25 13.602 3 11.042 3 7.962V4.5a1.038 1.038 0 1 1 2.077 0v3.462a4.158 4.158 0 0 0 4.154 4.153 4.158 4.158 0 0 0 4.154-4.153V4.5c0-.574.465-1.038 1.038-1.038z"></path></svg></div><div class="jsx-2769082546 text">Podcast - DataFramed</div></a></div><div><a target="_self" class="jsx-2769082546 item" href="/community/chat"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18 16"><g transform="translate(0 -1)"><path id="path-1" d="M12.595 13.364c.01-.111.02-.197.028-.251.058-.405.372-.702.74-.702h1.257c1.035-.002 1.875-.934 1.88-2.082V4.764c-.001-1.155-.842-2.092-1.878-2.094H3.38c-1.034.002-1.873.931-1.88 2.076v5.565c.001 1.156.842 2.092 1.878 2.094h6.626c.292 0 .557.189.68.484.408.977 1.07 1.576 1.94 1.85a6.004 6.004 0 0 1-.03-1.375h.001zm1.51 1.119c.048.314.136.521.235.606.566.487.258 1.497-.458 1.497-1.87 0-3.423-.785-4.33-2.51H3.376C1.513 14.07.004 12.39 0 10.311V4.74C.014 2.673 1.52 1.004 3.378 1h11.245c1.864.004 3.373 1.686 3.377 3.763v5.57c-.01 2.07-1.518 3.744-3.378 3.748h-.551c.004.138.016.273.035.402h-.001zm-8.423-5.81a1.115 1.115 0 1 0 0-2.229 1.115 1.115 0 0 0 0 2.229zm3.268 0a1.115 1.115 0 1 0 0-2.229 1.115 1.115 0 0 0 0 2.229zm3.318 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229z"></path></g></svg></div><div class="jsx-2769082546 text">Chat</div><div class="jsx-2769082546 statusIcon"><svg height="20" xmlns="http://www.w3.org/2000/svg" width="40" viewBox="0 0 40 17"><g fill="none" fill-rule="evenodd"><rect width="40" height="17" fill="#36D57D" rx="4"></rect><text fill="#FFF" font-family="Lato-Bold, Lato" font-size="12" font-weight="bold" letter-spacing=".4"><tspan x="5" y="13">NEW</tspan></text></g></svg></div></a></div></nav></div><div class="jsx-2769082546 section"><h5 class="jsx-2769082546">datacamp</h5><nav class="jsx-2769082546"><div><a target="_self" class="jsx-2769082546 item" href="/community/blog"><div class="jsx-2769082546 image"><svg height="14" id="RÃ©teg_1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 17 18"><path id="path-1_1_" d="M12.3 9.8s-.1.1 0 .4c.1.2.2.2-.1.5.2.2.2.3-.1.5 0 .5.1.9-.5 1-.6 0-1.4-.4-1.7.8-.2 1.1-.2.7-.2 1H5.3c.6-1.5 1-3.2 0-4-1.1-1-2.7-4.4.6-6.2.4-.2.8-.3 1.2-.4L6.9 3s1.4-.5 2.7.1v.2c.8.2 1.4.5 1.6.6.5.3 1.2.8 1.2 1.3.1.5-.5.2-.5.2.4.9.4 1.5.2 2.2-.2.7.8 1.4.7 1.9-.1.3-.5.3-.5.3m4.2-7L8.6 0h-.4L.4 2.8c-.3.1-.4.3-.4.6l1.1 10.4c0 .2.1.4.3.4l6.8 3.7c.2.1.4.1.6 0l6.6-3.7c.2-.1.3-.3.3-.4L17 3.4c-.1-.3-.2-.5-.5-.6M8.3 7c-.4 0-.7-.4-.7-.8s.3-.7.7-.7c.4 0 .7.3.7.7 0 .4-.3.8-.7.8zm2.9-1.4l.2-.1c-.3-1-1.2-1.5-1.2-1.5l-.2.2c-.1-.2-.6-.4-.7-.5l.1-.3h-.1c-.6-.3-1.3-.3-2.1 0l.4 1.1c-.6.2-1.1.7-1.1.7l-.6-.4c-.3.5-.5 1.2-.5 1.8h-.8c0 .5.1 1.4.7 2.2l.6-.4c.5.7 1.6 1.3 2.4 1.2V8.1c.3 0 .7-.1.9-.3l.4.6c.4-.1.8-1 .8-1.1l.7.2c.2-.6.2-1.3.1-1.9z"></path></svg></div><div class="jsx-2769082546 text">Official Blog</div></a></div><div><a target="_self" class="jsx-2769082546 item" href="/community/tech"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 21.75 29.76"><path d="M15.56 22.57a1.5 1.5 0 0 1-1.5-1.5 7.66 7.66 0 0 1 2.47-5.29 7.38 7.38 0 0 0 2.21-5.31A7.48 7.48 0 0 0 11.28 3h-.82a7.47 7.47 0 0 0-5.2 12.83 7.63 7.63 0 0 1 2.42 5.23 1.5 1.5 0 0 1-3 0 4.65 4.65 0 0 0-1.45-3A10.47 10.47 0 0 1 10.47 0h.82a10.47 10.47 0 0 1 7.28 18 4.68 4.68 0 0 0-1.5 3.08 1.5 1.5 0 0 1-1.51 1.49zM16.5 24.26a5.5 5.5 0 0 1-11 0"></path><path d="M10.89 22.56a1.5 1.5 0 0 1-1.5-1.5v-8.84a1.5 1.5 0 0 1 3 0v8.84a1.5 1.5 0 0 1-1.5 1.5z"></path></svg></div><div class="jsx-2769082546 text">Tech Thoughts</div></a></div></nav></div></div><main class="jsx-1028385822 Main"><div class="jsx-2159026896 ActionBar"><div><div class="jsx-3863678361 ActionBarSearch"><button style="font-weight:normal" class="jsx-1844558338 Button extra noPadding"><div class="jsx-1844558338 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20.03 23"><path d="M10.39 19.29A9.65 9.65 0 1 1 20 9.65a9.66 9.66 0 0 1-9.61 9.64zm0-17.06a7.42 7.42 0 1 0 7.42 7.42 7.43 7.43 0 0 0-7.42-7.42z"></path><path d="M1.11 23a1.11 1.11 0 0 1-.89-1.78l4.1-5.47a1.11 1.11 0 1 1 1.78 1.34L2 22.56a1.11 1.11 0 0 1-.89.44z"></path></svg></div><div class="jsx-1844558338 desktopOnly">Search</div></button></div></div><div class="jsx-2159026896 authBlock"><div></div><div class="jsx-3196442269 ActionBarAuth"><div class="jsx-3196442269"><a href="https://www.datacamp.com/users/sign_in?redirect=https://www.datacamp.com/community" class="jsx-3196442269"><button class="jsx-1844558338 Button border minWidth"><div class="jsx-1844558338 ">Log in</div></button></a><button class="jsx-1844558338 Button primary"><div class="jsx-1844558338 ">Create Account</div></button></div><div class="jsx-728636942 SubmitAnArticleButton"><button class="jsx-1844558338 Button desktopButton green noPadding"><div class="jsx-1844558338 icon"><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" viewBox="-90 92 18 18"><path id="a" d="M-80.4 100.4V97c0-.3-.3-.6-.6-.6s-.6.3-.6.6v3.4H-85c-.3 0-.6.3-.6.6s.3.6.6.6h3.4v3.4c0 .3.3.6.6.6s.6-.3.6-.6v-3.4h3.4c.3 0 .6-.3.6-.6s-.3-.6-.6-.6h-3.4zM-81 92c5 0 9 4 9 9s-4 9-9 9-9-4-9-9 4-9 9-9z"></path></svg></div><div class="jsx-1844558338 desktopOnly">Share an Article</div></button><div class="jsx-728636942 mobileButton"><svg xmlns="http://www.w3.org/2000/svg" width="40" height="40" viewBox="0 0 40 40"><path d="M20.86 19.15v-4.822c0-.47-.385-.851-.86-.851s-.86.38-.86.85v4.822h-4.878c-.475 0-.86.381-.86.851s.385.85.86.85h4.877v4.822c0 .47.386.851.861.851s.86-.38.86-.85V20.85h4.878c.475 0 .86-.381.86-.851s-.385-.85-.86-.85h-4.877zM20 0c11.045 0 20 8.953 20 20 0 11.045-8.955 20-20 20C8.953 40 0 31.045 0 20 0 8.953 8.953 0 20 0z"></path></svg></div></div></div></div></div><div class="jsx-1514242801 TitleBar"><div class="jsx-1514242801 filter"><button class="jsx-1844558338 Button iconButton noPadding"><div class="jsx-1844558338 icon"><svg id="RÃ©teg_1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 7 12"><path id="path-1_1_" d="M5.9 0c.4 0 .9.3 1 .7s.1.9-.2 1.2L2.7 6l4 4.1c.3.5.3 1.1-.1 1.6s-1.1.4-1.5.1l-4.8-5c-.4-.4-.4-1.2 0-1.6L5.1.3c.2-.2.5-.3.8-.3z"></path></svg></div><div class="jsx-1844558338 desktopOnly">Back to Tutorials</div></button></div><div class="jsx-1514242801 title"><div class="jsx-3889859319 Title"><div class="jsx-3889859319 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 24.22"><path d="M16.23 24.22a2 2 0 0 1-.73-.14L7 20.79a2 2 0 0 1-1.29-1.88v-4a1.5 1.5 0 0 1 3 0v3.36l7.54 2.92 7.54-2.92v-3.45a1.5 1.5 0 0 1 3 0v4.09a2 2 0 0 1-1.29 1.88L17 24.08a2 2 0 0 1-.77.14zm-.35-2.94zm.7 0z"></path><path d="M16.23 13.35a2 2 0 0 1-.62-.1C9.17 11.16 2.36 9 1.61 8.76a2 2 0 0 1-.25-3.87l14-4.78a2 2 0 0 1 1.3 0l14 4.78a2 2 0 0 1 0 3.81l-13.8 4.55a2 2 0 0 1-.63.1zm-.31-3zM5.21 6.74c3.49 1.11 9.07 2.92 11 3.56l10.68-3.53L16 3.05z"></path></svg></div><div class="jsx-3889859319 h1">Tutorials</div></div></div><div class="jsx-1514242801 action"></div></div><div class="jsx-1464850800 Tutorial"><div><div><div></div><div class="jsx-undefined social__top desktopOnly"><div class="jsx-undefined voteAndSocial"><div class="jsx-undefined"><a href="#comments" class="jsx-3293774837 CommentCounter"><span class="jsx-3293774837 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18 18"><path d="M12.595 13.364c.01-.111.02-.197.028-.251.058-.405.372-.702.74-.702h1.257c1.035-.002 1.875-.934 1.88-2.082V4.764c-.001-1.155-.842-2.092-1.878-2.094H3.38c-1.034.002-1.873.931-1.88 2.076v5.565c.001 1.156.842 2.092 1.878 2.094h6.626c.292 0 .557.189.68.484.408.977 1.07 1.576 1.94 1.85a6.004 6.004 0 0 1-.03-1.375zm1.51 1.119c.048.314.136.521.235.606.566.487.258 1.497-.458 1.497-1.87 0-3.423-.785-4.33-2.51H3.376C1.513 14.07.004 12.39 0 10.311V4.74C.014 2.673 1.52 1.004 3.378 1h11.245c1.864.004 3.373 1.686 3.377 3.763v5.57c-.01 2.07-1.518 3.744-3.378 3.748h-.551c.004.138.016.273.035.402zm-8.423-5.81a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.268 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.318 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229z"></path></svg></span><span class="jsx-3293774837 count">0</span></a><div class="jsx-1972554161 Upvote"><div class="jsx-1972554161"><div class="jsx-1972554161 normal"><span class="jsx-1972554161 icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12"><path d="M1 10L6 0l5 10z"></path></svg></span><span class="jsx-1972554161 count">45</span></div><div class="jsx-1972554161 voted"><span class="jsx-1972554161 icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12"><path d="M1 10L6 0l5 10z"></path></svg></span><span class="jsx-1972554161 count">45</span></div></div></div></div><div class="jsx-494086174 Social vertical"><div class="jsx-494086174 icons"><a href="https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/demystifying-crucial-statistics-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon"><svg height="12" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11.73 22.58"><path d="M7.61 22.58v-10.3h3.46l.52-4h-4V5.7c0-1.16.32-2 2-2h2.13V.16A28.47 28.47 0 0 0 8.63 0C5.56 0 3.47 1.87 3.47 5.31v3H0v4h3.47v10.3h4.14z"></path></svg></a><a href="https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/demystifying-crucial-statistics-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon centerIcon"><svg height="10" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20.42 16.67"><path d="M10 5.18c0-.28-.06-.53-.07-.78a4 4 0 0 1 .73-2.57A4.08 4.08 0 0 1 13.93 0 4 4 0 0 1 17 1.15a.43.43 0 0 0 .46.12 8.68 8.68 0 0 0 2.2-.84l.2-.1a4.36 4.36 0 0 1-1.75 2.28A9 9 0 0 0 20.42 2l-.21.3a3.83 3.83 0 0 1-.23.3A8.45 8.45 0 0 1 18.5 4a.28.28 0 0 0-.13.27A12 12 0 0 1 17 10.18a11.8 11.8 0 0 1-3.37 4.11 11.17 11.17 0 0 1-4.39 2.06 12.53 12.53 0 0 1-4.44.22 11.87 11.87 0 0 1-4.74-1.73L0 14.79a8.6 8.6 0 0 0 6.16-1.74 4.28 4.28 0 0 1-3.91-2.91h.95a6.18 6.18 0 0 0 .89-.12A4.2 4.2 0 0 1 .8 5.88a4 4 0 0 0 1.81.49 4.23 4.23 0 0 1-1.78-3A4.07 4.07 0 0 1 1.38.79 12.06 12.06 0 0 0 10 5.18z" id="iOjKBC.tif"></path></svg></a><a href="https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/demystifying-crucial-statistics-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon"><svg height="10" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16.99 17"><path d="M3.85 17H.34V5.67h3.51zM2.07 4.18a2.09 2.09 0 1 1 2.08-2.09 2.08 2.08 0 0 1-2.08 2.09zM17 17h-3.5v-5.95c0-1.63-.62-2.54-1.91-2.54s-2.14.95-2.14 2.54V17H6.09V5.67h3.36v1.52a4 4 0 0 1 3.42-1.87c2.4 0 4.12 1.47 4.12 4.5V17z"></path></svg></a></div></div></div></div></div><div class="jsx-1464850800 preface"><div class="jsx-1464850800 author"><div class="jsx-566588255 Author"><a href="/profile/spsayakpaul" target="_blank" class="jsx-566588255"><div style="background-image:url(https://res.cloudinary.com/dyd911kmh/image/fetch/t_avatar_thumbnail/https://assets.datacamp.com/users/avatars/000/535/025/square/Capture.PNG?1556868932);border-radius:20px;min-width:40px;min-height:40px" class="jsx-3208234818 Avatar"></div><div class="jsx-566588255 info"><div class="jsx-566588255 name">Sayak  Paul</div><div class="jsx-566588255 date"><span>September 27th, 2018</span></div></div></a></div></div><div class="jsx-1464850800 tags"><div class="jsx-2792531181 TagLine"><div class="jsx-1764811326 Tag mustRead"><span class="jsx-1764811326 title">must read</span></div><div class="jsx-1764811326 Tag"><span class="jsx-1764811326 title">python</span></div><a class="jsx-1022557955 more">+<!-- -->2</a></div></div><h1 class="jsx-1464850800 pageTitle">Demystifying Crucial Statistics in Python</h1><div class="jsx-1464850800 description pageDescription">Learn about the basic statistics required for Data Science and Machine Learning in Python.</div></div><div class="markdown"><div><p>If you have little experience in applying machine learning algorithm, you would have discovered that it does not require any knowledge of Statistics as a prerequisite.</p>
<p>However, knowing some statistics can be beneficial to understand machine learning technically as well intuitively. Knowing some statistics will eventually be required when you want to start validating your results and interpreting them. After all, when there is data, there are statistics. Like Mathematics is the language of Science. Statistics is one of a kind language for Data Science and Machine Learning.</p>
<p>Statistics is a field of mathematics with lots of theories and findings. However, there are various concepts, tools, techniques, and notations are taken from this field to make machine learning what it is today. You can use descriptive statistical methods to help transform observations into useful information that you will be able to understand and share with others. You can use inferential statistical techniques to reason from small samples of data to whole domains. Later in this post, you will study descriptive and inferential statistics. So, don't worry.  </p>
<p>Before getting started, let's walk through ten examples where statistical methods are used in an applied machine learning project:</p>
<ul>
<li><strong>Problem Framing</strong>: Requires the use of exploratory data analysis and data mining.</li>
<li><strong>Data Understanding</strong>: Requires the use of summary statistics and data visualization.</li>
<li><strong>Data Cleaning</strong>: Requires the use of outlier detection, imputation and more.</li>
<li><strong>Data Selection</strong>: Requires the use of data sampling and feature selection methods.</li>
<li><strong>Data Preparation</strong>: Requires the use of data transforms, scaling, encoding and much more.</li>
<li><strong>Model Evaluation</strong>: Requires experimental design and resampling methods.</li>
<li><strong>Model Configuration</strong>: Requires the use of statistical hypothesis tests and estimation statistics.</li>
<li><strong>Model Selection</strong>: Requires the use of statistical hypothesis tests and estimation statistics.</li>
<li><strong>Model Presentation</strong>: Requires the use of estimation statistics such as confidence intervals.</li>
<li><strong>Model Predictions</strong>: Requires the use of estimation statistics such as prediction intervals.</li>
</ul>
<p>Source: <a href="https://machinelearningmastery.com/statistics_for_machine_learning/">Statistical Methods for Machine Learning</a></p>
<p>Isn't that fascinating?</p>
<p>This post will give you a solid background in the essential but necessary statistics required for becoming a good machine learning practitioner.</p>
<p>In this post, you will study:</p>
<ul>
<li>Introduction to Statistics and its types</li>
<li>Statistics for data preparation</li>
<li>Statistics for model evaluation</li>
<li>Gaussian and Descriptive stats</li>
<li>Variable correlation</li>
<li>Non-parametric Statistics</li>
</ul>
<p>You have a lot to cover, and all of the topics are equally important. Let's get started!</p>
<h2 id="introduction-to-statistics-and-its-types-">Introduction to Statistics and its types:</h2>
<p>Let's briefly study how to define statistics in simple terms.</p>
<p>Statistics is considered a subfield of mathematics. It refers to a multitude of methods for working with data and using that data to answer many types of questions.</p>
<p>When it comes to the statistical tools that are used in practice, it can be helpful to divide the field of statistics into two broad groups of methods: descriptive statistics for summarizing data, and inferential statistics for concluding samples of data (<a href="https://machinelearningmastery.com/statistics-for-machine-learning-mini-course/">Statistics for Machine Learning (7-Day Mini-Course)</a>).</p>
<ul>
<li><strong>Descriptive Statistics</strong>: Descriptive statistics are used to describe the essential features of the data in a study. They provide simple summaries about the sample and the measures. Together with simple graphics analysis, they form the basis of virtually every quantitative analysis of data. The below infographic provides a good summary of descriptive statistics:</li>
</ul>
<p><img src="https://i2.wp.com/intellspot.com/wp-content/uploads/2017/11/descriptive-statistic-spreadsheet-and-pie-chart.png?resize=720%2C437" /></p>
<p><em><strong>Source: IntellSpot</strong></em></p>
<ul>
<li><p><strong>Inferential Statistics</strong>: Inferential statistics are methods that help in quantifying properties of the domain or population from a tinier set of obtained observations called a sample. Below is an infographic which beautifully describes inferential statistics:</p>
<p>  <img src="https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/20150849/what-is-inferential-statistics.jpg" /></p>
</li>
</ul>
<p><em><strong>Source: Analytics Vidhya</strong></em></p>
<p>In the next section, you will study the use of statistics for data preparation.</p>
<h2 id="statistics-for-data-preparation-">Statistics for data preparation:</h2>
<p>Statistical methods are required in the development of train and test data for your machine learning model.</p>
<p>This includes techniques for:</p>
<ul>
<li>Outlier detection</li>
<li>Missing value imputation</li>
<li>Data sampling</li>
<li>Data scaling</li>
<li>Variable encoding</li>
</ul>
<p>A basic understanding of data distributions, descriptive statistics, and data visualization is required to help you identify the methods to choose when performing these tasks.</p>
<p>Let's analyze each of the above points briefly.</p>
<h3 id="outlier-detection-">Outlier detection:</h3>
<p>Let's first see what an outlier is.</p>
<p>An outlier is considered an observation that appears to deviate from other observations in the sample. The following figure makes the definition more prominent.</p>
<p><img src="https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/34795/versions/7/screenshot.png" /></p>
<p><em><strong>Source: MathWorks</strong></em></p>
<p>You can spot the outliers in the data as given the above figure.</p>
<p>Many machine learning algorithms are sensitive to the range and distribution of attribute values in the input data. Outliers in input data can skew and mislead the training process of machine learning algorithms resulting in longer training times, less accurate models and ultimately more mediocre results.</p>
<p><strong>Identification of potential outliers is vital for the following reasons:</strong></p>
<ul>
<li><p>An outlier could indicate the data is bad. In example, the data maybe coded incorrectly, or the experiment did not run correctly. If it can be determined that an outlying point is, in fact, erroneous, then the value that is outlying should be removed from the analysis. If it is possible to correct that is another option.</p>
</li>
<li><p>In a few cases, it may not be possible to determine whether an outlying point is a bad data point. Outliers could be due to random variation or could possibly indicate something scientifically interesting. In any event, you typically do not want to just delete the outlying observation. However, if the data contains significant outliers, you may need to consider the use of robust statistical techniques.</p>
</li>
</ul>
<p>So, outliers are often not good for your predictive models (Although, sometimes, these outliers can be used as an advantage. But that is out of the scope of this post). You need the statistical know-how to handle outliers efficiently.</p>
<h3 id="missing-value-imputation-">Missing value imputation:</h3>
<p>Well, most of the datasets now suffer from the problem of missing values. Your machine learning model may not get trained effectively if the data that you are feeding to the model contains missing values. Statistical tools and techniques come here for the rescue.</p>
<p>Many people tend to discard the data instances which contain a missing value. But that is not a good practice because during that course you may lose essential features/representations of the data. Although there are advanced methods for dealing with missing value problems, these are the quick  techniques that one would go for: <strong>Mean Imputation</strong> and <strong>Median Imputation</strong>.</p>
<p>It is imperative that you understand what mean and median are.</p>
<p>Say, you have a feature <strong>X1</strong> which has these values - 13, 18, 13, 14, 13, 16, 14, 21, 13</p>
<p>The <strong>mean</strong> is the usual average, so I'll add and then divide:</p>
<p>(13 + 18 + 13 + 14 + 13 + 16 + 14 + 21 + 13) / 9 = 15</p>
<p>Note that the mean, in this case, isn't a value from the original list. This is a common result. You should not assume that your mean will be one of your original numbers.</p>
<p>The <strong>median</strong> is the middle value, so first, you will have to rewrite the list in numerical order:</p>
<p>13, 13, 13, 13, 14, 14, 16, 18, 21</p>
<p>There are nine numbers in the list, so the middle one will be the (9 + 1) / 2 = 10 / 2 = 5th number:</p>
<p>13, 13, 13, 13, 14, 14, 16, 18, 21</p>
<p>So the median is 14.</p>
<h3 id="data-sampling-">Data sampling:</h3>
<p>Data is considered the currency of applied machine learning. Therefore, its collection and usage both are equally significant.</p>
<p>Data sampling refers to statistical methods for selecting observations from the domain with the objective of estimating a population parameter. In other words, sampling is an active process of gathering observations with the intent of estimating a population variable.</p>
<p>Each row of a dataset represents an observation that is indicative of a particular population. When working with data, you often do not have access to all possible observations. This could be for many reasons, for example:</p>
<ul>
<li>It may be difficult or expensive to make more observations.</li>
<li>It may be challenging to gather all the observations together.</li>
<li>More observations are expected to be made in the future.</li>
</ul>
<p>Many times, you will not have the right proportion of the data samples. So, you will have to under-sample or over-sample based on the type of problem.</p>
<p>You perform under-sampling when the data samples for a particular category are very high compared to other meaning you discard some of the data samples from the category where they are higher. You perform over-sampling when the data samples for a particular type are decidedly lower compared to the other. In this case, you generate data samples.</p>
<p>This applies to multi-class scenarios as well.</p>
<p>Statistical sampling is a large field of study, but in applied machine learning, there may be three types of sampling that you are likely to use: simple random sampling, systematic sampling, and stratified sampling.</p>
<ul>
<li><strong>Simple Random Sampling</strong>: Samples are drawn with a uniform probability from the domain.</li>
<li><strong>Systematic Sampling</strong>: Samples are drawn using a pre-specified pattern, such as at intervals.</li>
<li><strong>Stratified Sampling</strong>: Samples are drawn within pre-specified categories (i.e., strata).</li>
</ul>
<p>Although these are the more common types of sampling that you may encounter, there are other techniques (<a href="https://machinelearningmastery.com/statistical-sampling-and-resampling/">A Gentle Introduction to Statistical Sampling and Resampling</a>).</p>
<h3 id="data-scaling-">Data Scaling:</h3>
<p>Often, the features of your dataset may widely vary in ranges. Some features may have a scale of 0 to 100 while the other may have ranges of 0.01 - 0.001, 10000- 20000, etc.</p>
<p>This is very problematic for efficient modeling. Because a small change in the feature which has a lower value range than the other feature may not have a significant impact on those other features. It affects the process of good learning. Dealing with this problem is known as <strong>data scaling</strong>.</p>
<p>There are different data scaling techniques such as Min-Max scaling, Absolute scaling, Standard scaling, etc.</p>
<h3 id="variable-encoding-">Variable encoding:</h3>
<p>At times, your datasets contain a mixture of both numeric and non-numeric data. Many machine learning frameworks like <code>scikit-learn</code> expect all the data to be present in all numeric format. This is also helpful to speed up the computation process.</p>
<p>Again, statistics come for saving you.</p>
<p>Techniques like Label encoding, One-Hot encoding, etc. are used to convert non-numeric data to numeric.</p>
<h2 id="it-s-time-to-apply-the-techniques-">It's time to apply the techniques!</h2>
<p>You have covered a lot of theory for now. You will apply some of these to get the real feel.</p>
<p>You will start off by applying some statistical methods to detect <strong>Outliers</strong>.</p>
<p>You will use the <code>Z-Score</code> index to detect outliers, and for this, you will investigate the <a href="https://www.kaggle.com/c/boston-housing">Boston House Price dataset</a>. Let's start off by importing the dataset from sklearn's utilities, and as you go along, you will start the necessary concepts.</p>
<pre><code class="lang-python">import pandas as pd
import numpy as np
from sklearn.datasets import load_boston

# Load the Boston dataset into a variable called boston
boston = load_boston()
</code></pre>
<pre><code class="lang-python"># Separate the features from the target
x = boston.data
y = boston.target
</code></pre>
<p>To view the dataset in a standard tabular format with the all the feature names, you will convert this into a <code>pandas</code> dataframe.</p>
<pre><code class="lang-python"># Take the columns separately in a variable
columns = boston.feature_names

# Create the dataframe
boston_df = pd.DataFrame(boston.data)
boston_df.columns = columns
boston_df.head()
</code></pre>
<p><center><img src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537905729/Screen_Shot_2018-09-25_at_4.01.46_PM_jqafmv.png" /></center></p>
<p>It is a common practice to start with univariate outlier analysis where you consider just one feature at a time. Often, a simple box-plot of a particular feature can give you good starting point. You will make a box-plot using <code>seaborn</code> and you will use the <code>DIS</code> feature.  </p>
<pre><code class="lang-python">import seaborn as sns
sns.boxplot(x=boston_df['DIS'])

import matplotlib.pyplot as plt
plt.show()
</code></pre>
<pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x8abded0&gt;
</code></pre><p><center><img src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537905647/output_24_1_plge8y.png" /></center></p>
<p>To view the box-plot, you did the second import of <code>matplotlib</code> since <code>seaborn</code> plots are displayed like ordinary matplotlib plots.</p>
<p>The above plot shows three points between 10 to 12, these are outliers as they're are not included in the box of other observations. Here you analyzed univariate outlier, i.e., you used DIS feature only to check for the outliers.</p>
<p>Let's proceed with Z-Score now.</p>
<p>&quot;<em>The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.</em>&quot; - <a href="&quot;https://en.wikipedia.org/?title=Z-score&amp;redirect=no&quot;">Wikipedia</a></p>
<p>The idea behind Z-score is to describe any data point regarding their relationship with the Standard Deviation and Mean for the group of data points. Z-score is about finding the distribution of data where the mean is 0, and the standard deviation is 1, i.e., normal distribution.</p>
<p>Wait! How on earth does this help in identifying the outliers?</p>
<p>Well, while calculating the Z-score you re-scale and center the data (mean of 0 and standard deviation of 1) and look for the instances which are too far from zero. These data points that are way too far from zero are treated as the outliers. In most common cases the threshold of 3 or -3 is used. In example, say the Z-score value is greater than or less than 3 or -3 respectively. This data point will then be identified as an outlier.</p>
<p>You will use the <code>Z-score</code> function defined in <code>scipy</code> library to detect the outliers.</p>
<pre><code class="lang-python">from scipy import stats

z = np.abs(stats.zscore(boston_df))
print(z)
</code></pre>
<pre><code>[[0.41771335 0.28482986 1.2879095  ... 1.45900038 0.44105193 1.0755623 ]
 [0.41526932 0.48772236 0.59338101 ... 0.30309415 0.44105193 0.49243937]
 [0.41527165 0.48772236 0.59338101 ... 0.30309415 0.39642699 1.2087274 ]
 ...
 [0.41137448 0.48772236 0.11573841 ... 1.17646583 0.44105193 0.98304761]
 [0.40568883 0.48772236 0.11573841 ... 1.17646583 0.4032249  0.86530163]
 [0.41292893 0.48772236 0.11573841 ... 1.17646583 0.44105193 0.66905833]]
</code></pre><p>It is not possible to detect the outliers by just looking at the above output. You are more intelligent! You will define the threshold for yourself, and you will use a simple condition for detecting the outliers that cross your threshold.</p>
<pre><code class="lang-python">threshold = 3
print(np.where(z &gt; 3))
</code></pre>
<pre><code>(array([ 55,  56,  57, 102, 141, 142, 152, 154, 155, 160, 162, 163, 199,
       200, 201, 202, 203, 204, 208, 209, 210, 211, 212, 216, 218, 219,
       220, 221, 222, 225, 234, 236, 256, 257, 262, 269, 273, 274, 276,
       277, 282, 283, 283, 284, 347, 351, 352, 353, 353, 354, 355, 356,
       357, 358, 363, 364, 364, 365, 367, 369, 370, 372, 373, 374, 374,
       380, 398, 404, 405, 406, 410, 410, 411, 412, 412, 414, 414, 415,
       416, 418, 418, 419, 423, 424, 425, 426, 427, 427, 429, 431, 436,
       437, 438, 445, 450, 454, 455, 456, 457, 466], dtype=int32), array([ 1,  1,  1, 11, 12,  3,  3,  3,  3,  3,  3,  3,  1,  1,  1,  1,  1,
        1,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  5,  3,  3,  1,  5,
        5,  3,  3,  3,  3,  3,  3,  1,  3,  1,  1,  7,  7,  1,  7,  7,  7,
        3,  3,  3,  3,  3,  5,  5,  5,  3,  3,  3, 12,  5, 12,  0,  0,  0,
        0,  5,  0, 11, 11, 11, 12,  0, 12, 11, 11,  0, 11, 11, 11, 11, 11,
       11,  0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11],
      dtype=int32))
</code></pre><p>Again, a confusing output! The first array contains the list of row numbers and the second array contains their respective column numbers. For example, <code>z[55][1]</code> have a Z-score higher than 3.</p>
<pre><code class="lang-python">print(z[55][1])
</code></pre>
<pre><code>3.375038763517309
</code></pre><p>So, the 55th record on column <code>ZN</code> is an outlier. You can extend things from here.</p>
<p>You saw how you could use Z-Score and set its threshold to detect potential outliers in the data. Next, you will see how to do some <strong>missing value imputation</strong>.</p>
<p>You will use the famous <a href="https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"><strong>Pima Indian Diabetes</strong></a> dataset which is known to have missing values. But before proceeding any further, you will have to load the dataset into your workspace.</p>
<p>You will load the dataset into a DataFrame object <strong>data</strong>.</p>
<pre><code class="lang-python">data = pd.read_csv(&quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv&quot;,header=None)
print(data.describe())
</code></pre>
<pre><code>                0           1           2           3           4           5  \
count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   
mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   
std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   
min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   
25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   
50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   
75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   
max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   

                6           7           8  
count  768.000000  768.000000  768.000000  
mean     0.471876   33.240885    0.348958  
std      0.331329   11.760232    0.476951  
min      0.078000   21.000000    0.000000  
25%      0.243750   24.000000    0.000000  
50%      0.372500   29.000000    0.000000  
75%      0.626250   41.000000    1.000000  
max      2.420000   81.000000    1.000000  
</code></pre><p>You might have already noticed that the column names are numeric here. This is because you are using an already preprocessed dataset. But don't worry, you will discover the names soon.</p>
<p>Now, this dataset is known to have missing values, but for your first glance at the above statistics, it might appear that the dataset does not contain missing values at all. But if you take a closer look, you will find that there are some columns where a zero value is entirely invalid. These are the values that are missing.</p>
<p>Specifically, the below columns have an invalid zero value as the minimum:</p>
<ul>
<li>Plasma glucose concentration</li>
<li>Diastolic blood pressure</li>
<li>Triceps skinfold thickness</li>
<li>2-Hour serum insulin</li>
<li>Body mass index</li>
</ul>
<p>Let's confirm this by looking at the raw data, the example prints the first 20 rows of data.</p>
<pre><code class="lang-python">data.head(20)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align:right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89</td>
      <td>66</td>
      <td>23</td>
      <td>94</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137</td>
      <td>40</td>
      <td>35</td>
      <td>168</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>116</td>
      <td>74</td>
      <td>0</td>
      <td>0</td>
      <td>25.6</td>
      <td>0.201</td>
      <td>30</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3</td>
      <td>78</td>
      <td>50</td>
      <td>32</td>
      <td>88</td>
      <td>31.0</td>
      <td>0.248</td>
      <td>26</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10</td>
      <td>115</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>35.3</td>
      <td>0.134</td>
      <td>29</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2</td>
      <td>197</td>
      <td>70</td>
      <td>45</td>
      <td>543</td>
      <td>30.5</td>
      <td>0.158</td>
      <td>53</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>8</td>
      <td>125</td>
      <td>96</td>
      <td>0</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.232</td>
      <td>54</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>4</td>
      <td>110</td>
      <td>92</td>
      <td>0</td>
      <td>0</td>
      <td>37.6</td>
      <td>0.191</td>
      <td>30</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>10</td>
      <td>168</td>
      <td>74</td>
      <td>0</td>
      <td>0</td>
      <td>38.0</td>
      <td>0.537</td>
      <td>34</td>
      <td>1</td>
    </tr>
    <tr>
      <th>12</th>
      <td>10</td>
      <td>139</td>
      <td>80</td>
      <td>0</td>
      <td>0</td>
      <td>27.1</td>
      <td>1.441</td>
      <td>57</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1</td>
      <td>189</td>
      <td>60</td>
      <td>23</td>
      <td>846</td>
      <td>30.1</td>
      <td>0.398</td>
      <td>59</td>
      <td>1</td>
    </tr>
    <tr>
      <th>14</th>
      <td>5</td>
      <td>166</td>
      <td>72</td>
      <td>19</td>
      <td>175</td>
      <td>25.8</td>
      <td>0.587</td>
      <td>51</td>
      <td>1</td>
    </tr>
    <tr>
      <th>15</th>
      <td>7</td>
      <td>100</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>30.0</td>
      <td>0.484</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0</td>
      <td>118</td>
      <td>84</td>
      <td>47</td>
      <td>230</td>
      <td>45.8</td>
      <td>0.551</td>
      <td>31</td>
      <td>1</td>
    </tr>
    <tr>
      <th>17</th>
      <td>7</td>
      <td>107</td>
      <td>74</td>
      <td>0</td>
      <td>0</td>
      <td>29.6</td>
      <td>0.254</td>
      <td>31</td>
      <td>1</td>
    </tr>
    <tr>
      <th>18</th>
      <td>1</td>
      <td>103</td>
      <td>30</td>
      <td>38</td>
      <td>83</td>
      <td>43.3</td>
      <td>0.183</td>
      <td>33</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1</td>
      <td>115</td>
      <td>70</td>
      <td>30</td>
      <td>96</td>
      <td>34.6</td>
      <td>0.529</td>
      <td>32</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>Clearly there are 0 values in the columns 2, 3, 4, and 5.</p>
<p>As this dataset has missing values denoted as 0, so it might be tricky to handle it by just using the conventional means. Let's summarize the approach you will follow to combat this:</p>
<ul>
<li>Get the count of zeros in each of the columns you saw earlier.</li>
<li>Determine which columns have the most zero values from the previous step.</li>
<li>Replace the zero values in those columns with <code>NaN</code>.</li>
<li>Check if the NaNs are getting appropriately reflected.</li>
<li>Call the fillna() function with the imputation strategy.</li>
</ul>
<pre><code class="lang-python"># Step 1: Get the count of zeros in each of the columns
print((data[[1,2,3,4,5]] == 0).sum())
</code></pre>
<pre><code>1      5
2     35
3    227
4    374
5     11
dtype: int64
</code></pre><p>You can see that columns 1,2 and 5 have just a few zero values, whereas columns 3 and 4 show a lot more, nearly half of the rows.</p>
<pre><code class="lang-python"># Step -2: Mark zero values as missing or NaN
data[[1,2,3,4,5]] = data[[1,2,3,4,5]].replace(0, np.NaN)

# Count the number of NaN values in each column
print(data.isnull().sum())
</code></pre>
<pre><code>0      0
1      5
2     35
3    227
4    374
5     11
6      0
7      0
8      0
dtype: int64
</code></pre><p>Let's get sure at this point of time that your NaN replacement was a hit by taking a look at the dataset as a whole:</p>
<pre><code class="lang-python"># Step 4
data.head(20)
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align:right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148.0</td>
      <td>72.0</td>
      <td>35.0</td>
      <td>NaN</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85.0</td>
      <td>66.0</td>
      <td>29.0</td>
      <td>NaN</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183.0</td>
      <td>64.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89.0</td>
      <td>66.0</td>
      <td>23.0</td>
      <td>94.0</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137.0</td>
      <td>40.0</td>
      <td>35.0</td>
      <td>168.0</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>116.0</td>
      <td>74.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>25.6</td>
      <td>0.201</td>
      <td>30</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>3</td>
      <td>78.0</td>
      <td>50.0</td>
      <td>32.0</td>
      <td>88.0</td>
      <td>31.0</td>
      <td>0.248</td>
      <td>26</td>
      <td>1</td>
    </tr>
    <tr>
      <th>7</th>
      <td>10</td>
      <td>115.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>35.3</td>
      <td>0.134</td>
      <td>29</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>2</td>
      <td>197.0</td>
      <td>70.0</td>
      <td>45.0</td>
      <td>543.0</td>
      <td>30.5</td>
      <td>0.158</td>
      <td>53</td>
      <td>1</td>
    </tr>
    <tr>
      <th>9</th>
      <td>8</td>
      <td>125.0</td>
      <td>96.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>0.232</td>
      <td>54</td>
      <td>1</td>
    </tr>
    <tr>
      <th>10</th>
      <td>4</td>
      <td>110.0</td>
      <td>92.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>37.6</td>
      <td>0.191</td>
      <td>30</td>
      <td>0</td>
    </tr>
    <tr>
      <th>11</th>
      <td>10</td>
      <td>168.0</td>
      <td>74.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>38.0</td>
      <td>0.537</td>
      <td>34</td>
      <td>1</td>
    </tr>
    <tr>
      <th>12</th>
      <td>10</td>
      <td>139.0</td>
      <td>80.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>27.1</td>
      <td>1.441</td>
      <td>57</td>
      <td>0</td>
    </tr>
    <tr>
      <th>13</th>
      <td>1</td>
      <td>189.0</td>
      <td>60.0</td>
      <td>23.0</td>
      <td>846.0</td>
      <td>30.1</td>
      <td>0.398</td>
      <td>59</td>
      <td>1</td>
    </tr>
    <tr>
      <th>14</th>
      <td>5</td>
      <td>166.0</td>
      <td>72.0</td>
      <td>19.0</td>
      <td>175.0</td>
      <td>25.8</td>
      <td>0.587</td>
      <td>51</td>
      <td>1</td>
    </tr>
    <tr>
      <th>15</th>
      <td>7</td>
      <td>100.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>30.0</td>
      <td>0.484</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0</td>
      <td>118.0</td>
      <td>84.0</td>
      <td>47.0</td>
      <td>230.0</td>
      <td>45.8</td>
      <td>0.551</td>
      <td>31</td>
      <td>1</td>
    </tr>
    <tr>
      <th>17</th>
      <td>7</td>
      <td>107.0</td>
      <td>74.0</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>29.6</td>
      <td>0.254</td>
      <td>31</td>
      <td>1</td>
    </tr>
    <tr>
      <th>18</th>
      <td>1</td>
      <td>103.0</td>
      <td>30.0</td>
      <td>38.0</td>
      <td>83.0</td>
      <td>43.3</td>
      <td>0.183</td>
      <td>33</td>
      <td>0</td>
    </tr>
    <tr>
      <th>19</th>
      <td>1</td>
      <td>115.0</td>
      <td>70.0</td>
      <td>30.0</td>
      <td>96.0</td>
      <td>34.6</td>
      <td>0.529</td>
      <td>32</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>



<p>You can see that marking the missing values had the intended effect.</p>
<p>Up till now, you analyzed essential trends when data is missing and how you can make use of simple statistical measures to get a hold of it. Now, you will impute the missing values using <strong>Mean Imputation</strong> which is essentially imputing the mean of the respective column in place of missing values.</p>
<pre><code class="lang-python"># Step 5: Call the fillna() function with the imputation strategy
data.fillna(data.mean(), inplace=True)

# Count the number of NaN values in each column to verify
print(data.isnull().sum())
</code></pre>
<pre><code>0    0
1    0
2    0
3    0
4    0
5    0
6    0
7    0
8    0
dtype: int64
</code></pre><p>Excellent!</p>
<p>This <a href="https://www.datacamp.com/community/tutorials/preprocessing-in-data-science-part-1-centering-scaling-and-knn">DataCamp article</a> effectively guides you in implementing <strong>data scaling</strong> as a data preprocessing step. Be sure to check it out.</p>
<p>Next, you will do <strong>variable encoding</strong>.</p>
<p>Before that, you need a dataset which actually contains non-numeric data. You will use the famous <a href="http://archive.ics.uci.edu/ml/datasets/Iris">Iris dataset</a> for this.</p>
<pre><code class="lang-python"># Load the dataset to a DataFrame object iris
iris = pd.read_csv(&quot;http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot;,header=None)
</code></pre>
<pre><code class="lang-python"># See first 20 rows of the dataset
iris.head(20)
</code></pre>
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align:right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.9</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.7</td>
      <td>3.2</td>
      <td>1.3</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4.6</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5.0</td>
      <td>3.6</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5.4</td>
      <td>3.9</td>
      <td>1.7</td>
      <td>0.4</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>6</th>
      <td>4.6</td>
      <td>3.4</td>
      <td>1.4</td>
      <td>0.3</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>7</th>
      <td>5.0</td>
      <td>3.4</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>8</th>
      <td>4.4</td>
      <td>2.9</td>
      <td>1.4</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>9</th>
      <td>4.9</td>
      <td>3.1</td>
      <td>1.5</td>
      <td>0.1</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>10</th>
      <td>5.4</td>
      <td>3.7</td>
      <td>1.5</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>11</th>
      <td>4.8</td>
      <td>3.4</td>
      <td>1.6</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>12</th>
      <td>4.8</td>
      <td>3.0</td>
      <td>1.4</td>
      <td>0.1</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>13</th>
      <td>4.3</td>
      <td>3.0</td>
      <td>1.1</td>
      <td>0.1</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>14</th>
      <td>5.8</td>
      <td>4.0</td>
      <td>1.2</td>
      <td>0.2</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>15</th>
      <td>5.7</td>
      <td>4.4</td>
      <td>1.5</td>
      <td>0.4</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>16</th>
      <td>5.4</td>
      <td>3.9</td>
      <td>1.3</td>
      <td>0.4</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>17</th>
      <td>5.1</td>
      <td>3.5</td>
      <td>1.4</td>
      <td>0.3</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>18</th>
      <td>5.7</td>
      <td>3.8</td>
      <td>1.7</td>
      <td>0.3</td>
      <td>Iris-setosa</td>
    </tr>
    <tr>
      <th>19</th>
      <td>5.1</td>
      <td>3.8</td>
      <td>1.5</td>
      <td>0.3</td>
      <td>Iris-setosa</td>
    </tr>
  </tbody>
</table>
</div>



<p>You can easily convert the string values to integer values using the <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html">LabelEncoder</a>. The three class values (Iris-setosa, Iris-versicolor, Iris-virginica) are mapped to the integer values (0, 1, 2).</p>
<p>In this case, the fourth column/feature of the dataset contains non-numeric values. So you need to separate it out.</p>
<pre><code class="lang-python"># Convert the DataFrame to a NumPy array
iris = iris.values

# Separate
Y = iris[:,4]
</code></pre>
<pre><code class="lang-python"># Label Encode string class values as integers
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
label_encoder = label_encoder.fit(Y)
label_encoded_y = label_encoder.transform(Y)
</code></pre>
<p>Now, let's study another area where the need for elementary knowledge of statistics is very crucial.</p>
<h2 id="statistics-for-model-evaluation-">Statistics for model evaluation:</h2>
<p>You have designed and developed your machine learning model. Now, you want to evaluate the performance of your model on the test data. In this regards, you seek help of various statistical metrics like Precision, Recall, ROC, AUC, RMSE, etc. You also seek help from multiple data resampling techniques such as <strong>k-fold Cross-Validation</strong>.</p>
<p>Statistics can effectively be used to:</p>
<ul>
<li><a href="https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/">Estimate a hypothesis accuracy</a></li>
<li><a href="https://www.universalclass.com/articles/math/statistics/types-of-errors-in-hypothesis-testing.htm">Determine the error of two hypotheses</a></li>
<li><a href="https://machinelearningmastery.com/mcnemars-test-for-machine-learning/">Compare learning algorithms using McNemar's test</a></li>
</ul>
<p><em>It is important to note that the hypothesis refers to learned models; the results of running a learning algorithm on a dataset. Evaluating and comparing the hypothesis means comparing learned models, which is different from evaluating and comparing machine learning algorithms, which could be trained on different samples from the same problem or various problems.</em></p>
<p>Let's study Gaussian and Descriptive statistics now.</p>
<h2 id="introduction-to-gaussian-and-descriptive-stats-">Introduction to Gaussian and Descriptive stats:</h2>
<p>A sample of data is nothing but a snapshot from a broader population of all the potential observations that could be taken from a domain or generated by a process.</p>
<p>Interestingly, many observations fit a typical pattern or distribution called the normal distribution, or more formally, the Gaussian distribution. This is the bell-shaped distribution that you may be aware of. The following figure denotes a Gaussian distribution:</p>
<p><img src="http://hyperphysics.phy-astr.gsu.edu/hbase/Math/immath/gauds.gif" /></p>
<p><em><strong>Source: HyperPhysics</strong></em></p>
<p>Gaussian processes and Gaussian distributions are whole another sub-fields unto themselves. But, you will now study two of the most essential ingredients that build the entire world of Gaussian distributions in general.</p>
<p>Any sample data taken from a Gaussian distribution can be summarized with two parameters:</p>
<ul>
<li><strong>Mean</strong>: The central tendency or most likely value in the distribution (the top of the bell).</li>
<li><strong>Variance</strong>: The average difference that observations have from the mean value in the distribution (the spread).</li>
</ul>
<p>The term <em>variance</em> also gives rise to another critical term, i.e., <em>standard deviation</em>, which is merely the square root of the variance.</p>
<p>The mean, variance, and standard deviation can be directly calculated from data samples using <code>numpy</code>.</p>
<p>You will first generate a sample of 100 random numbers pulled from a Gaussian distribution with a mean of 50 and a standard deviation of 5. You will then calculate the summary statistics.</p>
<p>First, you will import all the dependencies.</p>
<pre><code class="lang-python">#  Dependencies
from numpy.random import seed
from numpy.random import randn
from numpy import mean
from numpy import var
from numpy import std
</code></pre>
<p>Next, you set the random number generator seed so that your results are reproducible.</p>
<pre><code class="lang-python">seed(1)
</code></pre>
<pre><code class="lang-python"># Generate univariate observations
data = 5 * randn(10000) + 50
</code></pre>
<pre><code class="lang-python"># Calculate statistics
print('Mean: %.3f' % mean(data))
print('Variance: %.3f' % var(data))
print('Standard Deviation: %.3f' % std(data))
</code></pre>
<pre><code>Mean: 50.049
Variance: 24.939
Standard Deviation: 4.994
</code></pre><p>Close enough, eh?</p>
<p>Let's study the next topic now.</p>
<h2 id="variable-correlation-">Variable correlation:</h2>
<p>Generally, the features that are contained in a dataset can often be related to each other which is very obvious to happen in practice. In statistical terms, this relationship between the features of your dataset (be it simple or complex) is often termed as <em>correlation</em>.</p>
<p>It is crucial to find out the degree of the correlation of the features in a dataset. This step essentially serves you as <em>feature selection</em> which concerns selecting the most important features from a dataset. This step is one of the most vital steps in a standard machine learning pipeline as it can give you a tremendous accuracy boost that too within a lesser amount of time.</p>
<p>For better understanding and to keep it more practical let's understand why features can be related to each other:</p>
<ul>
<li>One feature can be a determinant of another feature</li>
<li>One feature could be associated with another feature in some degree of composition</li>
<li>Multiple features can combine and give birth to another feature</li>
</ul>
<p>Correlation between the features can be of three types: - <strong>Positive correlation</strong> where both the feature change in the same direction, <strong>Neutral correlation</strong> when there is no relationship of the change in the two features, <strong>Negative correlation</strong> where both the features change in opposite directions.</p>
<p>Correlation measurements form the fundamental of filter-based feature selection techniques. Check <a href="https://www.datacamp.com/community/tutorials/feature-selection-python">this article</a> if you want to study more about feature selection.</p>
<p>You can mathematically the relationship between samples of two variables using a statistical method called <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">Pearsonâ€™s correlation coefficient</a>, named after the developer of the method, <strong>Karl Pearson</strong>.</p>
<p>You can calculate the Pearson's correlation score by using the <code>corr()</code> function of <code>pandas</code> with the <code>method</code> parameter as <code>pearson</code>. Let's study the correlation between the features of the Pima Indians Diabetes dataset that you used earlier. You already have the data in good shape.</p>
<pre><code class="lang-python"># Data
data.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align:right;">
      <th></th>
      <th>0</th>
      <th>1</th>
      <th>2</th>
      <th>3</th>
      <th>4</th>
      <th>5</th>
      <th>6</th>
      <th>7</th>
      <th>8</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148.0</td>
      <td>72.0</td>
      <td>35.00000</td>
      <td>155.548223</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85.0</td>
      <td>66.0</td>
      <td>29.00000</td>
      <td>155.548223</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183.0</td>
      <td>64.0</td>
      <td>29.15342</td>
      <td>155.548223</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89.0</td>
      <td>66.0</td>
      <td>23.00000</td>
      <td>94.000000</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137.0</td>
      <td>40.0</td>
      <td>35.00000</td>
      <td>168.000000</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>




<pre><code class="lang-python"># Create the matrix of correlation score between the features and the label
scoreTable = data.corr(method='pearson')
</code></pre>
<pre><code class="lang-python"># Visulaize the matrix
data.corr(method='pearson').style.format(&quot;{:.2}&quot;).background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)
</code></pre>
<p><center><img src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537966124/Pearson_table_c1t5rt.png" /></center></p>
<p>You can clearly see the Pearson's correlation between all the features and the label of the dataset.</p>
<p>In the next section, you will study non-parametric statistics.</p>
<h2 id="non-parametric-statistics-">Non-parametric statistics:</h2>
<p>A large portion of the field of statistics and statistical methods is dedicated to data where the distribution is known.</p>
<p>Non-parametric statistics comes in handy when there is no or few information available about the population parameters. Non-parametric tests make no assumptions about the distribution of data.</p>
<p>In the case where you are working with nonparametric data, specialized nonparametric statistical methods can be used that discard all information about the distribution. As such, these methods are often referred to as <em>distribution-free</em> methods.</p>
<p>Bu before a nonparametric statistical method can be applied, the data must be converted into a rank format. Statistical methods that expect data in a rank format are sometimes called <em>rank statistics</em>. Examples of rank statistics can be rank correlation and rank statistical hypothesis tests. Ranking data is exactly as its name suggests.</p>
<p>A widely used nonparametric statistical hypothesis test for checking for a difference between two independent samples is the <em><strong>Mann-Whitney U test</strong></em>, named for Henry Mann and Donald Whitney.</p>
<p>You will implement this test in Python via the <code>mannwhitneyu()</code> which is provided by <code>SciPy</code>.</p>
<pre><code class="lang-python"># The dependencies that you need
from scipy.stats import mannwhitneyu
from numpy.random import rand

# seed the random number generator
seed(1)
</code></pre>
<pre><code class="lang-python"># Generate two independent samples
data1 = 50 + (rand(100) * 10)
data2 = 51 + (rand(100) * 10)
# Compare samples
stat, p = mannwhitneyu(data1, data2)
print('Statistics = %.3f, p = %.3f' % (stat, p))
# Interpret
alpha = 0.05
if p &gt; alpha:
    print('Same distribution (fail to reject H0)')
else:
    print('Different distribution (reject H0)')
</code></pre>
<pre><code>Statistics = 4077.000, p = 0.012
Different distribution (reject H0)
</code></pre><p><code>alpha</code> is the threshold parameter which is decided by you. The <code>mannwhitneyu()</code> returns two things:</p>
<ul>
<li><p>statistic: The Mann-Whitney U statistic, equal to min(U for x, U for y) if alternative is equal to None (deprecated; exists for backward compatibility), and U for y otherwise.</p>
</li>
<li><p>pvalue:  p-value assuming an asymptotic normal distribution.</p>
</li>
</ul>
<p>If you want to study the other methods of Non-parametric statistics, you can do it from <a href="https://www.analyticsvidhya.com/blog/2017/11/a-guide-to-conduct-analysis-using-non-parametric-tests/">here</a>.</p>
<p>The other two popular non-parametric statistical significance tests that you can use are:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Friedman_test">Friedman test</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test">Wilcoxon signed-rank test</a></li>
</ul>
<h2 id="that-calls-for-a-wrap-up-">That calls for a wrap up!</h2>
<p>You have finally made it to the end. In this article, you studied a variety of essential statistical concepts that play very crucial role in your machine learning projects. So, understanding them is just important.</p>
<p>From mere an introduction to statistics, you took it to statistical rankings that too with several implementations. That is definitely quite a feat. You studied three different datasets, exploited <code>pandas</code> and <code>numpy</code> functionalities to the fullest and moreover, you used <code>SciPy</code> as well. Next are some links for you if you want to take things further:</p>
<ul>
<li><a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">The Elements of Statistical Learning</a></li>
<li><a href="https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf">Machine Learning book by Tom Mitchell</a></li>
<li><a href="https://www.ic.unicamp.br/~wainer/cursos/1s2013/ml/livro.pdf">All for Statistics</a></li>
</ul>
<p>Following are the resources I took help from for writing this blog:</p>
<ul>
<li><a href="https://machinelearningmastery.com/statistics-for-machine-learning-mini-course/">Machine Learning Mastery mini course on Statistics</a></li>
<li><a href="https://machinelearningmastery.com/statistical-sampling-and-resampling/">A Gentle Introduction to Statistical Sampling and Resampling</a></li>
<li><a href="https://www.khanacademy.org/math/statistics-probability">https://www.khanacademy.org/math/statistics-probability</a></li>
<li><a href="https://statlearning.class.stanford.edu/">Statistical Learning course by Stanford University</a></li>
</ul>
<p>Let me know your views/queries in the comments section. Also, check out <a href="https://www.datacamp.com/courses/statistical-thinking-in-python-part-1">DataCamp's course on &quot;Statistical Thinking in Python&quot;</a> which is very practically aligned.</p>
</div><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/solarized-dark.min.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Lora" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,400i,700,700i" rel="stylesheet"/></div><div class="jsx-1464850800 social__bottom mobileOnly"><div class="jsx-1464850800 voteAndSocial"><div class="jsx-1464850800"><div class="jsx-1972554161 Upvote"><div class="jsx-1972554161"><div class="jsx-1972554161 normal"><span class="jsx-1972554161 icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12"><path d="M1 10L6 0l5 10z"></path></svg></span><span class="jsx-1972554161 count">45</span></div><div class="jsx-1972554161 voted"><span class="jsx-1972554161 icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12"><path d="M1 10L6 0l5 10z"></path></svg></span><span class="jsx-1972554161 count">45</span></div></div></div><a href="#comments" class="jsx-3293774837 CommentCounter"><span class="jsx-3293774837 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18 18"><path d="M12.595 13.364c.01-.111.02-.197.028-.251.058-.405.372-.702.74-.702h1.257c1.035-.002 1.875-.934 1.88-2.082V4.764c-.001-1.155-.842-2.092-1.878-2.094H3.38c-1.034.002-1.873.931-1.88 2.076v5.565c.001 1.156.842 2.092 1.878 2.094h6.626c.292 0 .557.189.68.484.408.977 1.07 1.576 1.94 1.85a6.004 6.004 0 0 1-.03-1.375zm1.51 1.119c.048.314.136.521.235.606.566.487.258 1.497-.458 1.497-1.87 0-3.423-.785-4.33-2.51H3.376C1.513 14.07.004 12.39 0 10.311V4.74C.014 2.673 1.52 1.004 3.378 1h11.245c1.864.004 3.373 1.686 3.377 3.763v5.57c-.01 2.07-1.518 3.744-3.378 3.748h-.551c.004.138.016.273.035.402zm-8.423-5.81a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.268 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.318 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229z"></path></svg></span><span class="jsx-3293774837 count">0</span></a></div><div class="jsx-494086174 Social"><div class="jsx-494086174 icons"><a href="https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/demystifying-crucial-statistics-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon"><svg height="12" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11.73 22.58"><path d="M7.61 22.58v-10.3h3.46l.52-4h-4V5.7c0-1.16.32-2 2-2h2.13V.16A28.47 28.47 0 0 0 8.63 0C5.56 0 3.47 1.87 3.47 5.31v3H0v4h3.47v10.3h4.14z"></path></svg></a><a href="https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/demystifying-crucial-statistics-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon centerIcon"><svg height="10" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20.42 16.67"><path d="M10 5.18c0-.28-.06-.53-.07-.78a4 4 0 0 1 .73-2.57A4.08 4.08 0 0 1 13.93 0 4 4 0 0 1 17 1.15a.43.43 0 0 0 .46.12 8.68 8.68 0 0 0 2.2-.84l.2-.1a4.36 4.36 0 0 1-1.75 2.28A9 9 0 0 0 20.42 2l-.21.3a3.83 3.83 0 0 1-.23.3A8.45 8.45 0 0 1 18.5 4a.28.28 0 0 0-.13.27A12 12 0 0 1 17 10.18a11.8 11.8 0 0 1-3.37 4.11 11.17 11.17 0 0 1-4.39 2.06 12.53 12.53 0 0 1-4.44.22 11.87 11.87 0 0 1-4.74-1.73L0 14.79a8.6 8.6 0 0 0 6.16-1.74 4.28 4.28 0 0 1-3.91-2.91h.95a6.18 6.18 0 0 0 .89-.12A4.2 4.2 0 0 1 .8 5.88a4 4 0 0 0 1.81.49 4.23 4.23 0 0 1-1.78-3A4.07 4.07 0 0 1 1.38.79 12.06 12.06 0 0 0 10 5.18z" id="iOjKBC.tif"></path></svg></a><a href="https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/demystifying-crucial-statistics-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon"><svg height="10" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16.99 17"><path d="M3.85 17H.34V5.67h3.51zM2.07 4.18a2.09 2.09 0 1 1 2.08-2.09 2.08 2.08 0 0 1-2.08 2.09zM17 17h-3.5v-5.95c0-1.63-.62-2.54-1.91-2.54s-2.14.95-2.14 2.54V17H6.09V5.67h3.36v1.52a4 4 0 0 1 3.42-1.87c2.4 0 4.12 1.47 4.12 4.5V17z"></path></svg></a></div></div></div></div></div></div><div id="comments" class="jsx-3956319705 PostAComment"><button class="jsx-1844558338 Button big seeAll same greyIcon"><div class="jsx-1844558338 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18 18"><path d="M12.595 13.364c.01-.111.02-.197.028-.251.058-.405.372-.702.74-.702h1.257c1.035-.002 1.875-.934 1.88-2.082V4.764c-.001-1.155-.842-2.092-1.878-2.094H3.38c-1.034.002-1.873.931-1.88 2.076v5.565c.001 1.156.842 2.092 1.878 2.094h6.626c.292 0 .557.189.68.484.408.977 1.07 1.576 1.94 1.85a6.004 6.004 0 0 1-.03-1.375zm1.51 1.119c.048.314.136.521.235.606.566.487.258 1.497-.458 1.497-1.87 0-3.423-.785-4.33-2.51H3.376C1.513 14.07.004 12.39 0 10.311V4.74C.014 2.673 1.52 1.004 3.378 1h11.245c1.864.004 3.373 1.686 3.377 3.763v5.57c-.01 2.07-1.518 3.744-3.378 3.748h-.551c.004.138.016.273.035.402zm-8.423-5.81a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.268 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.318 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229z"></path></svg></div><div class="jsx-1844558338 ">Post a Comment</div></button></div></main><div class="jsx-2506565400 SidebarSocial"><div class="jsx-2506565400 rss"><a href="/community/rss.xml" class="jsx-2506565400"><svg height="13" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18 18"><circle cx="3.08" cy="14.92" r="3.08"></circle><path d="M16.46 18a1.54 1.54 0 0 1-1.54-1.54c0-8.25-5.13-13.38-13.38-13.38A1.59 1.59 0 0 1 .46 1.15 1.72 1.72 0 0 1 1.54 0a16.45 16.45 0 0 1 12 4.51c3 2.95 4.51 7.08 4.51 12A1.54 1.54 0 0 1 16.46 18z"></path><path d="M10.63 18a1.54 1.54 0 0 1-1.54-1.54c0-5-2.54-7.54-7.54-7.54a1.54 1.54 0 0 1 0-3.08c6.75 0 10.63 3.87 10.63 10.62A1.54 1.54 0 0 1 10.63 18z"></path></svg>Subscribe to RSS</a></div><div class="jsx-2506565400 icons"><a href="https://www.facebook.com/pages/DataCamp/726282547396228" target="_blank" rel="noopener noreferrer" class="jsx-2506565400 icon"><svg height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11.73 22.58"><path d="M7.61 22.58v-10.3h3.46l.52-4h-4V5.7c0-1.16.32-2 2-2h2.13V.16A28.47 28.47 0 0 0 8.63 0C5.56 0 3.47 1.87 3.47 5.31v3H0v4h3.47v10.3h4.14z"></path></svg></a><a href="https://twitter.com/datacamp" target="_blank" rel="noopener noreferrer" class="jsx-2506565400 icon"><svg height="13" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20.42 16.67"><path d="M10 5.18c0-.28-.06-.53-.07-.78a4 4 0 0 1 .73-2.57A4.08 4.08 0 0 1 13.93 0 4 4 0 0 1 17 1.15a.43.43 0 0 0 .46.12 8.68 8.68 0 0 0 2.2-.84l.2-.1a4.36 4.36 0 0 1-1.75 2.28A9 9 0 0 0 20.42 2l-.21.3a3.83 3.83 0 0 1-.23.3A8.45 8.45 0 0 1 18.5 4a.28.28 0 0 0-.13.27A12 12 0 0 1 17 10.18a11.8 11.8 0 0 1-3.37 4.11 11.17 11.17 0 0 1-4.39 2.06 12.53 12.53 0 0 1-4.44.22 11.87 11.87 0 0 1-4.74-1.73L0 14.79a8.6 8.6 0 0 0 6.16-1.74 4.28 4.28 0 0 1-3.91-2.91h.95a6.18 6.18 0 0 0 .89-.12A4.2 4.2 0 0 1 .8 5.88a4 4 0 0 0 1.81.49 4.23 4.23 0 0 1-1.78-3A4.07 4.07 0 0 1 1.38.79 12.06 12.06 0 0 0 10 5.18z" id="iOjKBC.tif"></path></svg></a><a href="https://www.linkedin.com/company/datamind-org" target="_blank" rel="noopener noreferrer" class="jsx-2506565400 icon"><svg height="12" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16.99 17"><path d="M3.85 17H.34V5.67h3.51zM2.07 4.18a2.09 2.09 0 1 1 2.08-2.09 2.08 2.08 0 0 1-2.08 2.09zM17 17h-3.5v-5.95c0-1.63-.62-2.54-1.91-2.54s-2.14.95-2.14 2.54V17H6.09V5.67h3.36v1.52a4 4 0 0 1 3.42-1.87c2.4 0 4.12 1.47 4.12 4.5V17z"></path></svg></a><a href="https://www.youtube.com/channel/UC79Gv3mYp6zKiSwYemEik9A" target="_blank" rel="noopener noreferrer" class="jsx-2506565400 icon"><svg height="12" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 25.19 17.73"><path d="M24.21 1.52C23.3.44 21.62 0 18.42 0H6.78C3.5 0 1.79.47.88 1.62S0 4.4 0 6.68V11c0 4.43 1 6.68 6.78 6.68h11.64c2.78 0 4.32-.39 5.32-1.34S25.2 13.76 25.2 11V6.68c-.01-2.41-.08-4.07-.99-5.16zm-8 7.94l-5.29 2.76a.81.81 0 0 1-1.19-.72V6a.81.81 0 0 1 1.19-.72L16.17 8a.81.81 0 0 1 0 1.44z"></path></svg></a></div><div class="jsx-2506565400 menu"><a href="/about" class="jsx-2506565400 menuItem">About</a><a href="/terms-of-use" class="jsx-2506565400 menuItem">Terms</a><a href="/privacy-policy" class="jsx-2506565400 menuItem">Privacy</a></div></div><div class="jsx-879378290 BottomBar bar"><div class="jsx-879378290 barView"><div style="background-image:url(https://res.cloudinary.com/dyd911kmh/image/fetch/t_avatar_thumbnail/https://cdn.datacamp.com/community/assets/placeholder_avatar-7f673b5d40e159404a56b5931250cc73.png);border-radius:20px;min-width:40px;min-height:40px" class="jsx-3208234818 Avatar"></div><div class="jsx-879378290 blueBar">Want to leave a comment?</div></div></div></div><link href="https://fonts.googleapis.com/css?family=Lato:300,400i,400,700" rel="stylesheet"/><div class="Analytics"><script>
      ;(function(p,l,o,w,i,n,g){if(!p[i]){p.GlobalSnowplowNamespace=p.GlobalSnowplowNamespace||[];
        p.GlobalSnowplowNamespace.push(i);p[i]=function(){(p[i].q=p[i].q||[]).push(arguments)
        };p[i].q=p[i].q||[];n=l.createElement(o);g=l.getElementsByTagName(o)[0];n.async=1;
        n.src=w;g.parentNode.insertBefore(n,g)}}(window,document,"script","//d36fqcuygdrd4y.cloudfront.net/BuKMCyKUvvyXZkMi44LjI.js","snowplow"));

      window.snowplow('newTracker', 'co', 'track.datacamp.com', {
        platform: 'web',
        post: true,
        discoverRootDomain: true,
        contexts: {
          webPage: true,
          performanceTiming: true
        }
      });
      window.snowplow('enableActivityTracking', 10, 10);
      window.snowplow('enableLinkClickTracking');
    </script><script data-cfasync="false">(function(W,i,s,e,P,o,p){W['WisePopsObject']=P;W[P]=W[P]||function(){
      (W[P].q=W[P].q||[]).push(arguments)},W[P].l=1*new Date();o=i.createElement(s),
      p=i.getElementsByTagName(s)[0];o.async=1;o.src=e;p.parentNode.insertBefore(o,p)
    })(window,document,'script','//loader.wisepops.com/get-loader.js?v=1&site=VswVJn7o4J','wisepops');</script></div></div></div></div><div id="__next-error"></div><script>
          __NEXT_DATA__ = {"props":{"isServer":true,"store":{},"initialState":{"adminContent":{"isFetching":false,"isFetched":false,"statusMessage":"","content":{},"form":{"isSaving":false,"isSucceeded":false,"statusMessage":"","isAdminFormModalOpen":false,"previewSlug":""},"delete":{"isDeleting":false,"isSucceeded":false,"statusMessage":"","isDeleteAdminContentModalOpen":false},"update":{"isApprovingArticle":false,"isSucceeded":false,"statusMessage":"","isApproveArticleModalOpen":false}},"adminList":{"isFetched":false,"isFetching":false,"statusMessage":""},"auth":{"isAuthModalOpen":false,"isLogin":false,"isAuthorized":false,"isSubscriber":false,"user":{},"loginTitle":"","signUpTitle":""},"clientConfig":{"isDevelopment":false,"absoluteUrl":"https://www.datacamp.com","isNewsActive":true,"ALGOLIA_APP_ID":"7H5IORUQLD","ALGOLIA_API_KEY":"ae4f7a78b4914ef37d11dc177ad2eb13","ALGOLIA_API_INDEX":"community_prod","DC_COMMUNITY_AUTHOR_APP_URL":"https://community-author-app.new.datacamp.com/","DC_LIGHT_URL":"https://cdn.datacamp.com/datacamp-light-latest.min.js","ANALYTICS_GOOGLE_TAG_MANAGER":"GTM-TGGWB2P","ANALYTICS_SNOWPLOW_ENDPOINT":"track.datacamp.com","WISEPOPS":"VswVJn7o4J","CHAT_SUBSCRIBER_REDIRECT":"https://www.datacamp.com/slack/join","CHAT_NONSUBSCRIBER_LINK":"https://www.datacamp.com/subscribe?from_communitychat=true","CHAT_SUBSCRIBE_TEAM":"https://www.datacamp.com/groups/business"},"comment":{"isFetching":false,"isFetched":false,"statusMessage":"","comments":[],"commentsTotal":0,"form":{"isSaving":false,"isSucceeded":false,"statusMessage":"","id":"new","parentId":null,"commentText":""},"delete":{"isDeleting":false,"isSucceeded":false,"statusMessage":"","isDeleteCommentModalOpen":false},"isBottomBarOpen":true,"bottomBarView":"bar"},"content":{"content":{"id":12226,"externalId":null,"type":"Tutorial","status":"published","authorId":"spsayakpaul","title":"Demystifying Crucial Statistics in Python","slug":"demystifying-crucial-statistics-python","previewSlug":null,"description":"Learn about the basic statistics required for Data Science and Machine Learning in Python.","articles":[],"courses":[],"redirectSlug":null,"contentHtml":"\u003cp\u003eIf you have little experience in applying machine learning algorithm, you would have discovered that it does not require any knowledge of Statistics as a prerequisite.\u003c/p\u003e\r\n\u003cp\u003eHowever, knowing some statistics can be beneficial to understand machine learning technically as well intuitively. Knowing some statistics will eventually be required when you want to start validating your results and interpreting them. After all, when there is data, there are statistics. Like Mathematics is the language of Science. Statistics is one of a kind language for Data Science and Machine Learning.\u003c/p\u003e\r\n\u003cp\u003eStatistics is a field of mathematics with lots of theories and findings. However, there are various concepts, tools, techniques, and notations are taken from this field to make machine learning what it is today. You can use descriptive statistical methods to help transform observations into useful information that you will be able to understand and share with others. You can use inferential statistical techniques to reason from small samples of data to whole domains. Later in this post, you will study descriptive and inferential statistics. So, don\u0026#39;t worry.  \u003c/p\u003e\r\n\u003cp\u003eBefore getting started, let\u0026#39;s walk through ten examples where statistical methods are used in an applied machine learning project:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003cstrong\u003eProblem Framing\u003c/strong\u003e: Requires the use of exploratory data analysis and data mining.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eData Understanding\u003c/strong\u003e: Requires the use of summary statistics and data visualization.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eData Cleaning\u003c/strong\u003e: Requires the use of outlier detection, imputation and more.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eData Selection\u003c/strong\u003e: Requires the use of data sampling and feature selection methods.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eData Preparation\u003c/strong\u003e: Requires the use of data transforms, scaling, encoding and much more.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eModel Evaluation\u003c/strong\u003e: Requires experimental design and resampling methods.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eModel Configuration\u003c/strong\u003e: Requires the use of statistical hypothesis tests and estimation statistics.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eModel Selection\u003c/strong\u003e: Requires the use of statistical hypothesis tests and estimation statistics.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eModel Presentation\u003c/strong\u003e: Requires the use of estimation statistics such as confidence intervals.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eModel Predictions\u003c/strong\u003e: Requires the use of estimation statistics such as prediction intervals.\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eSource: \u003ca href=\"https://machinelearningmastery.com/statistics_for_machine_learning/\"\u003eStatistical Methods for Machine Learning\u003c/a\u003e\u003c/p\u003e\r\n\u003cp\u003eIsn\u0026#39;t that fascinating?\u003c/p\u003e\r\n\u003cp\u003eThis post will give you a solid background in the essential but necessary statistics required for becoming a good machine learning practitioner.\u003c/p\u003e\r\n\u003cp\u003eIn this post, you will study:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003eIntroduction to Statistics and its types\u003c/li\u003e\r\n\u003cli\u003eStatistics for data preparation\u003c/li\u003e\r\n\u003cli\u003eStatistics for model evaluation\u003c/li\u003e\r\n\u003cli\u003eGaussian and Descriptive stats\u003c/li\u003e\r\n\u003cli\u003eVariable correlation\u003c/li\u003e\r\n\u003cli\u003eNon-parametric Statistics\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eYou have a lot to cover, and all of the topics are equally important. Let\u0026#39;s get started!\u003c/p\u003e\r\n\u003ch2 id=\"introduction-to-statistics-and-its-types-\"\u003eIntroduction to Statistics and its types:\u003c/h2\u003e\r\n\u003cp\u003eLet\u0026#39;s briefly study how to define statistics in simple terms.\u003c/p\u003e\r\n\u003cp\u003eStatistics is considered a subfield of mathematics. It refers to a multitude of methods for working with data and using that data to answer many types of questions.\u003c/p\u003e\r\n\u003cp\u003eWhen it comes to the statistical tools that are used in practice, it can be helpful to divide the field of statistics into two broad groups of methods: descriptive statistics for summarizing data, and inferential statistics for concluding samples of data (\u003ca href=\"https://machinelearningmastery.com/statistics-for-machine-learning-mini-course/\"\u003eStatistics for Machine Learning (7-Day Mini-Course)\u003c/a\u003e).\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003cstrong\u003eDescriptive Statistics\u003c/strong\u003e: Descriptive statistics are used to describe the essential features of the data in a study. They provide simple summaries about the sample and the measures. Together with simple graphics analysis, they form the basis of virtually every quantitative analysis of data. The below infographic provides a good summary of descriptive statistics:\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003e\u003cimg src = \"https://i2.wp.com/intellspot.com/wp-content/uploads/2017/11/descriptive-statistic-spreadsheet-and-pie-chart.png?resize=720%2C437\"\u003e\u003c/img\u003e\u003c/p\u003e\r\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eSource: IntellSpot\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003eInferential Statistics\u003c/strong\u003e: Inferential statistics are methods that help in quantifying properties of the domain or population from a tinier set of obtained observations called a sample. Below is an infographic which beautifully describes inferential statistics:\u003c/p\u003e\r\n\u003cp\u003e  \u003cimg src = \"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/01/20150849/what-is-inferential-statistics.jpg\"\u003e\u003c/p\u003e\r\n\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eSource: Analytics Vidhya\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\r\n\u003cp\u003eIn the next section, you will study the use of statistics for data preparation.\u003c/p\u003e\r\n\u003ch2 id=\"statistics-for-data-preparation-\"\u003eStatistics for data preparation:\u003c/h2\u003e\r\n\u003cp\u003eStatistical methods are required in the development of train and test data for your machine learning model.\u003c/p\u003e\r\n\u003cp\u003eThis includes techniques for:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003eOutlier detection\u003c/li\u003e\r\n\u003cli\u003eMissing value imputation\u003c/li\u003e\r\n\u003cli\u003eData sampling\u003c/li\u003e\r\n\u003cli\u003eData scaling\u003c/li\u003e\r\n\u003cli\u003eVariable encoding\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eA basic understanding of data distributions, descriptive statistics, and data visualization is required to help you identify the methods to choose when performing these tasks.\u003c/p\u003e\r\n\u003cp\u003eLet\u0026#39;s analyze each of the above points briefly.\u003c/p\u003e\r\n\u003ch3 id=\"outlier-detection-\"\u003eOutlier detection:\u003c/h3\u003e\r\n\u003cp\u003eLet\u0026#39;s first see what an outlier is.\u003c/p\u003e\r\n\u003cp\u003eAn outlier is considered an observation that appears to deviate from other observations in the sample. The following figure makes the definition more prominent.\u003c/p\u003e\r\n\u003cp\u003e\u003cimg src = \"https://www.mathworks.com/matlabcentral/mlc-downloads/downloads/submissions/34795/versions/7/screenshot.png\"\u003e\u003c/p\u003e\r\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eSource: MathWorks\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\r\n\u003cp\u003eYou can spot the outliers in the data as given the above figure.\u003c/p\u003e\r\n\u003cp\u003eMany machine learning algorithms are sensitive to the range and distribution of attribute values in the input data. Outliers in input data can skew and mislead the training process of machine learning algorithms resulting in longer training times, less accurate models and ultimately more mediocre results.\u003c/p\u003e\r\n\u003cp\u003e\u003cstrong\u003eIdentification of potential outliers is vital for the following reasons:\u003c/strong\u003e\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003cp\u003eAn outlier could indicate the data is bad. In example, the data maybe coded incorrectly, or the experiment did not run correctly. If it can be determined that an outlying point is, in fact, erroneous, then the value that is outlying should be removed from the analysis. If it is possible to correct that is another option.\u003c/p\u003e\r\n\u003c/li\u003e\r\n\u003cli\u003e\u003cp\u003eIn a few cases, it may not be possible to determine whether an outlying point is a bad data point. Outliers could be due to random variation or could possibly indicate something scientifically interesting. In any event, you typically do not want to just delete the outlying observation. However, if the data contains significant outliers, you may need to consider the use of robust statistical techniques.\u003c/p\u003e\r\n\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eSo, outliers are often not good for your predictive models (Although, sometimes, these outliers can be used as an advantage. But that is out of the scope of this post). You need the statistical know-how to handle outliers efficiently.\u003c/p\u003e\r\n\u003ch3 id=\"missing-value-imputation-\"\u003eMissing value imputation:\u003c/h3\u003e\r\n\u003cp\u003eWell, most of the datasets now suffer from the problem of missing values. Your machine learning model may not get trained effectively if the data that you are feeding to the model contains missing values. Statistical tools and techniques come here for the rescue.\u003c/p\u003e\r\n\u003cp\u003eMany people tend to discard the data instances which contain a missing value. But that is not a good practice because during that course you may lose essential features/representations of the data. Although there are advanced methods for dealing with missing value problems, these are the quick  techniques that one would go for: \u003cstrong\u003eMean Imputation\u003c/strong\u003e and \u003cstrong\u003eMedian Imputation\u003c/strong\u003e.\u003c/p\u003e\r\n\u003cp\u003eIt is imperative that you understand what mean and median are.\u003c/p\u003e\r\n\u003cp\u003eSay, you have a feature \u003cstrong\u003eX1\u003c/strong\u003e which has these values - 13, 18, 13, 14, 13, 16, 14, 21, 13\u003c/p\u003e\r\n\u003cp\u003eThe \u003cstrong\u003emean\u003c/strong\u003e is the usual average, so I\u0026#39;ll add and then divide:\u003c/p\u003e\r\n\u003cp\u003e(13 + 18 + 13 + 14 + 13 + 16 + 14 + 21 + 13) / 9 = 15\u003c/p\u003e\r\n\u003cp\u003eNote that the mean, in this case, isn\u0026#39;t a value from the original list. This is a common result. You should not assume that your mean will be one of your original numbers.\u003c/p\u003e\r\n\u003cp\u003eThe \u003cstrong\u003emedian\u003c/strong\u003e is the middle value, so first, you will have to rewrite the list in numerical order:\u003c/p\u003e\r\n\u003cp\u003e13, 13, 13, 13, 14, 14, 16, 18, 21\u003c/p\u003e\r\n\u003cp\u003eThere are nine numbers in the list, so the middle one will be the (9 + 1) / 2 = 10 / 2 = 5th number:\u003c/p\u003e\r\n\u003cp\u003e13, 13, 13, 13, 14, 14, 16, 18, 21\u003c/p\u003e\r\n\u003cp\u003eSo the median is 14.\u003c/p\u003e\r\n\u003ch3 id=\"data-sampling-\"\u003eData sampling:\u003c/h3\u003e\r\n\u003cp\u003eData is considered the currency of applied machine learning. Therefore, its collection and usage both are equally significant.\u003c/p\u003e\r\n\u003cp\u003eData sampling refers to statistical methods for selecting observations from the domain with the objective of estimating a population parameter. In other words, sampling is an active process of gathering observations with the intent of estimating a population variable.\u003c/p\u003e\r\n\u003cp\u003eEach row of a dataset represents an observation that is indicative of a particular population. When working with data, you often do not have access to all possible observations. This could be for many reasons, for example:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003eIt may be difficult or expensive to make more observations.\u003c/li\u003e\r\n\u003cli\u003eIt may be challenging to gather all the observations together.\u003c/li\u003e\r\n\u003cli\u003eMore observations are expected to be made in the future.\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eMany times, you will not have the right proportion of the data samples. So, you will have to under-sample or over-sample based on the type of problem.\u003c/p\u003e\r\n\u003cp\u003eYou perform under-sampling when the data samples for a particular category are very high compared to other meaning you discard some of the data samples from the category where they are higher. You perform over-sampling when the data samples for a particular type are decidedly lower compared to the other. In this case, you generate data samples.\u003c/p\u003e\r\n\u003cp\u003eThis applies to multi-class scenarios as well.\u003c/p\u003e\r\n\u003cp\u003eStatistical sampling is a large field of study, but in applied machine learning, there may be three types of sampling that you are likely to use: simple random sampling, systematic sampling, and stratified sampling.\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003cstrong\u003eSimple Random Sampling\u003c/strong\u003e: Samples are drawn with a uniform probability from the domain.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eSystematic Sampling\u003c/strong\u003e: Samples are drawn using a pre-specified pattern, such as at intervals.\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eStratified Sampling\u003c/strong\u003e: Samples are drawn within pre-specified categories (i.e., strata).\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eAlthough these are the more common types of sampling that you may encounter, there are other techniques (\u003ca href=\"https://machinelearningmastery.com/statistical-sampling-and-resampling/\"\u003eA Gentle Introduction to Statistical Sampling and Resampling\u003c/a\u003e).\u003c/p\u003e\r\n\u003ch3 id=\"data-scaling-\"\u003eData Scaling:\u003c/h3\u003e\r\n\u003cp\u003eOften, the features of your dataset may widely vary in ranges. Some features may have a scale of 0 to 100 while the other may have ranges of 0.01 - 0.001, 10000- 20000, etc.\u003c/p\u003e\r\n\u003cp\u003eThis is very problematic for efficient modeling. Because a small change in the feature which has a lower value range than the other feature may not have a significant impact on those other features. It affects the process of good learning. Dealing with this problem is known as \u003cstrong\u003edata scaling\u003c/strong\u003e.\u003c/p\u003e\r\n\u003cp\u003eThere are different data scaling techniques such as Min-Max scaling, Absolute scaling, Standard scaling, etc.\u003c/p\u003e\r\n\u003ch3 id=\"variable-encoding-\"\u003eVariable encoding:\u003c/h3\u003e\r\n\u003cp\u003eAt times, your datasets contain a mixture of both numeric and non-numeric data. Many machine learning frameworks like \u003ccode\u003escikit-learn\u003c/code\u003e expect all the data to be present in all numeric format. This is also helpful to speed up the computation process.\u003c/p\u003e\r\n\u003cp\u003eAgain, statistics come for saving you.\u003c/p\u003e\r\n\u003cp\u003eTechniques like Label encoding, One-Hot encoding, etc. are used to convert non-numeric data to numeric.\u003c/p\u003e\r\n\u003ch2 id=\"it-s-time-to-apply-the-techniques-\"\u003eIt\u0026#39;s time to apply the techniques!\u003c/h2\u003e\r\n\u003cp\u003eYou have covered a lot of theory for now. You will apply some of these to get the real feel.\u003c/p\u003e\r\n\u003cp\u003eYou will start off by applying some statistical methods to detect \u003cstrong\u003eOutliers\u003c/strong\u003e.\u003c/p\u003e\r\n\u003cp\u003eYou will use the \u003ccode\u003eZ-Score\u003c/code\u003e index to detect outliers, and for this, you will investigate the \u003ca href=\"https://www.kaggle.com/c/boston-housing\"\u003eBoston House Price dataset\u003c/a\u003e. Let\u0026#39;s start off by importing the dataset from sklearn\u0026#39;s utilities, and as you go along, you will start the necessary concepts.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eimport pandas as pd\r\nimport numpy as np\r\nfrom sklearn.datasets import load_boston\r\n\r\n# Load the Boston dataset into a variable called boston\r\nboston = load_boston()\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Separate the features from the target\r\nx = boston.data\r\ny = boston.target\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eTo view the dataset in a standard tabular format with the all the feature names, you will convert this into a \u003ccode\u003epandas\u003c/code\u003e dataframe.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Take the columns separately in a variable\r\ncolumns = boston.feature_names\r\n\r\n# Create the dataframe\r\nboston_df = pd.DataFrame(boston.data)\r\nboston_df.columns = columns\r\nboston_df.head()\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537905729/Screen_Shot_2018-09-25_at_4.01.46_PM_jqafmv.png\" /\u003e\u003c/p\u003e\r\n\u003cp\u003eIt is a common practice to start with univariate outlier analysis where you consider just one feature at a time. Often, a simple box-plot of a particular feature can give you good starting point. You will make a box-plot using \u003ccode\u003eseaborn\u003c/code\u003e and you will use the \u003ccode\u003eDIS\u003c/code\u003e feature.  \u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eimport seaborn as sns\r\nsns.boxplot(x=boston_df[\u0026#39;DIS\u0026#39;])\r\n\r\nimport matplotlib.pyplot as plt\r\nplt.show()\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e\u0026lt;matplotlib.axes._subplots.AxesSubplot at 0x8abded0\u0026gt;\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537905647/output_24_1_plge8y.png\" /\u003e\u003c/p\u003e\r\n\u003cp\u003eTo view the box-plot, you did the second import of \u003ccode\u003ematplotlib\u003c/code\u003e since \u003ccode\u003eseaborn\u003c/code\u003e plots are displayed like ordinary matplotlib plots.\u003c/p\u003e\r\n\u003cp\u003eThe above plot shows three points between 10 to 12, these are outliers as they\u0026#39;re are not included in the box of other observations. Here you analyzed univariate outlier, i.e., you used DIS feature only to check for the outliers.\u003c/p\u003e\r\n\u003cp\u003eLet\u0026#39;s proceed with Z-Score now.\u003c/p\u003e\r\n\u003cp\u003e\u0026quot;\u003cem\u003eThe Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured.\u003c/em\u003e\u0026quot; - \u003ca href=\"\u0026quot;https://en.wikipedia.org/?title=Z-score\u0026amp;redirect=no\u0026quot;\"\u003eWikipedia\u003c/a\u003e\u003c/p\u003e\r\n\u003cp\u003eThe idea behind Z-score is to describe any data point regarding their relationship with the Standard Deviation and Mean for the group of data points. Z-score is about finding the distribution of data where the mean is 0, and the standard deviation is 1, i.e., normal distribution.\u003c/p\u003e\r\n\u003cp\u003eWait! How on earth does this help in identifying the outliers?\u003c/p\u003e\r\n\u003cp\u003eWell, while calculating the Z-score you re-scale and center the data (mean of 0 and standard deviation of 1) and look for the instances which are too far from zero. These data points that are way too far from zero are treated as the outliers. In most common cases the threshold of 3 or -3 is used. In example, say the Z-score value is greater than or less than 3 or -3 respectively. This data point will then be identified as an outlier.\u003c/p\u003e\r\n\u003cp\u003eYou will use the \u003ccode\u003eZ-score\u003c/code\u003e function defined in \u003ccode\u003escipy\u003c/code\u003e library to detect the outliers.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003efrom scipy import stats\r\n\r\nz = np.abs(stats.zscore(boston_df))\r\nprint(z)\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e[[0.41771335 0.28482986 1.2879095  ... 1.45900038 0.44105193 1.0755623 ]\r\n [0.41526932 0.48772236 0.59338101 ... 0.30309415 0.44105193 0.49243937]\r\n [0.41527165 0.48772236 0.59338101 ... 0.30309415 0.39642699 1.2087274 ]\r\n ...\r\n [0.41137448 0.48772236 0.11573841 ... 1.17646583 0.44105193 0.98304761]\r\n [0.40568883 0.48772236 0.11573841 ... 1.17646583 0.4032249  0.86530163]\r\n [0.41292893 0.48772236 0.11573841 ... 1.17646583 0.44105193 0.66905833]]\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIt is not possible to detect the outliers by just looking at the above output. You are more intelligent! You will define the threshold for yourself, and you will use a simple condition for detecting the outliers that cross your threshold.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003ethreshold = 3\r\nprint(np.where(z \u0026gt; 3))\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e(array([ 55,  56,  57, 102, 141, 142, 152, 154, 155, 160, 162, 163, 199,\r\n       200, 201, 202, 203, 204, 208, 209, 210, 211, 212, 216, 218, 219,\r\n       220, 221, 222, 225, 234, 236, 256, 257, 262, 269, 273, 274, 276,\r\n       277, 282, 283, 283, 284, 347, 351, 352, 353, 353, 354, 355, 356,\r\n       357, 358, 363, 364, 364, 365, 367, 369, 370, 372, 373, 374, 374,\r\n       380, 398, 404, 405, 406, 410, 410, 411, 412, 412, 414, 414, 415,\r\n       416, 418, 418, 419, 423, 424, 425, 426, 427, 427, 429, 431, 436,\r\n       437, 438, 445, 450, 454, 455, 456, 457, 466], dtype=int32), array([ 1,  1,  1, 11, 12,  3,  3,  3,  3,  3,  3,  3,  1,  1,  1,  1,  1,\r\n        1,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  5,  3,  3,  1,  5,\r\n        5,  3,  3,  3,  3,  3,  3,  1,  3,  1,  1,  7,  7,  1,  7,  7,  7,\r\n        3,  3,  3,  3,  3,  5,  5,  5,  3,  3,  3, 12,  5, 12,  0,  0,  0,\r\n        0,  5,  0, 11, 11, 11, 12,  0, 12, 11, 11,  0, 11, 11, 11, 11, 11,\r\n       11,  0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11],\r\n      dtype=int32))\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eAgain, a confusing output! The first array contains the list of row numbers and the second array contains their respective column numbers. For example, \u003ccode\u003ez[55][1]\u003c/code\u003e have a Z-score higher than 3.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eprint(z[55][1])\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e3.375038763517309\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eSo, the 55th record on column \u003ccode\u003eZN\u003c/code\u003e is an outlier. You can extend things from here.\u003c/p\u003e\r\n\u003cp\u003eYou saw how you could use Z-Score and set its threshold to detect potential outliers in the data. Next, you will see how to do some \u003cstrong\u003emissing value imputation\u003c/strong\u003e.\u003c/p\u003e\r\n\u003cp\u003eYou will use the famous \u003ca href=\"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\u003e\u003cstrong\u003ePima Indian Diabetes\u003c/strong\u003e\u003c/a\u003e dataset which is known to have missing values. But before proceeding any further, you will have to load the dataset into your workspace.\u003c/p\u003e\r\n\u003cp\u003eYou will load the dataset into a DataFrame object \u003cstrong\u003edata\u003c/strong\u003e.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003edata = pd.read_csv(\u0026quot;https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\u0026quot;,header=None)\r\nprint(data.describe())\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e                0           1           2           3           4           5  \\\r\ncount  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \r\nmean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \r\nstd      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \r\nmin      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \r\n25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \r\n50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \r\n75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \r\nmax     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \r\n\r\n                6           7           8  \r\ncount  768.000000  768.000000  768.000000  \r\nmean     0.471876   33.240885    0.348958  \r\nstd      0.331329   11.760232    0.476951  \r\nmin      0.078000   21.000000    0.000000  \r\n25%      0.243750   24.000000    0.000000  \r\n50%      0.372500   29.000000    0.000000  \r\n75%      0.626250   41.000000    1.000000  \r\nmax      2.420000   81.000000    1.000000  \r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou might have already noticed that the column names are numeric here. This is because you are using an already preprocessed dataset. But don\u0026#39;t worry, you will discover the names soon.\u003c/p\u003e\r\n\u003cp\u003eNow, this dataset is known to have missing values, but for your first glance at the above statistics, it might appear that the dataset does not contain missing values at all. But if you take a closer look, you will find that there are some columns where a zero value is entirely invalid. These are the values that are missing.\u003c/p\u003e\r\n\u003cp\u003eSpecifically, the below columns have an invalid zero value as the minimum:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003ePlasma glucose concentration\u003c/li\u003e\r\n\u003cli\u003eDiastolic blood pressure\u003c/li\u003e\r\n\u003cli\u003eTriceps skinfold thickness\u003c/li\u003e\r\n\u003cli\u003e2-Hour serum insulin\u003c/li\u003e\r\n\u003cli\u003eBody mass index\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eLet\u0026#39;s confirm this by looking at the raw data, the example prints the first 20 rows of data.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003edata.head(20)\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cdiv\u003e\r\n\u003cstyle scoped\u003e\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\u003c/style\u003e\r\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\r\n  \u003cthead\u003e\r\n    \u003ctr style=\"text-align: right;\"\u003e\r\n      \u003cth\u003e\u003c/th\u003e\r\n      \u003cth\u003e0\u003c/th\u003e\r\n      \u003cth\u003e1\u003c/th\u003e\r\n      \u003cth\u003e2\u003c/th\u003e\r\n      \u003cth\u003e3\u003c/th\u003e\r\n      \u003cth\u003e4\u003c/th\u003e\r\n      \u003cth\u003e5\u003c/th\u003e\r\n      \u003cth\u003e6\u003c/th\u003e\r\n      \u003cth\u003e7\u003c/th\u003e\r\n      \u003cth\u003e8\u003c/th\u003e\r\n    \u003c/tr\u003e\r\n  \u003c/thead\u003e\r\n  \u003ctbody\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e0\u003c/th\u003e\r\n      \u003ctd\u003e6\u003c/td\u003e\r\n      \u003ctd\u003e148\u003c/td\u003e\r\n      \u003ctd\u003e72\u003c/td\u003e\r\n      \u003ctd\u003e35\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e33.6\u003c/td\u003e\r\n      \u003ctd\u003e0.627\u003c/td\u003e\r\n      \u003ctd\u003e50\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e1\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e85\u003c/td\u003e\r\n      \u003ctd\u003e66\u003c/td\u003e\r\n      \u003ctd\u003e29\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e26.6\u003c/td\u003e\r\n      \u003ctd\u003e0.351\u003c/td\u003e\r\n      \u003ctd\u003e31\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e2\u003c/th\u003e\r\n      \u003ctd\u003e8\u003c/td\u003e\r\n      \u003ctd\u003e183\u003c/td\u003e\r\n      \u003ctd\u003e64\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e23.3\u003c/td\u003e\r\n      \u003ctd\u003e0.672\u003c/td\u003e\r\n      \u003ctd\u003e32\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e3\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e89\u003c/td\u003e\r\n      \u003ctd\u003e66\u003c/td\u003e\r\n      \u003ctd\u003e23\u003c/td\u003e\r\n      \u003ctd\u003e94\u003c/td\u003e\r\n      \u003ctd\u003e28.1\u003c/td\u003e\r\n      \u003ctd\u003e0.167\u003c/td\u003e\r\n      \u003ctd\u003e21\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e4\u003c/th\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e137\u003c/td\u003e\r\n      \u003ctd\u003e40\u003c/td\u003e\r\n      \u003ctd\u003e35\u003c/td\u003e\r\n      \u003ctd\u003e168\u003c/td\u003e\r\n      \u003ctd\u003e43.1\u003c/td\u003e\r\n      \u003ctd\u003e2.288\u003c/td\u003e\r\n      \u003ctd\u003e33\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e5\u003c/th\u003e\r\n      \u003ctd\u003e5\u003c/td\u003e\r\n      \u003ctd\u003e116\u003c/td\u003e\r\n      \u003ctd\u003e74\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e25.6\u003c/td\u003e\r\n      \u003ctd\u003e0.201\u003c/td\u003e\r\n      \u003ctd\u003e30\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e6\u003c/th\u003e\r\n      \u003ctd\u003e3\u003c/td\u003e\r\n      \u003ctd\u003e78\u003c/td\u003e\r\n      \u003ctd\u003e50\u003c/td\u003e\r\n      \u003ctd\u003e32\u003c/td\u003e\r\n      \u003ctd\u003e88\u003c/td\u003e\r\n      \u003ctd\u003e31.0\u003c/td\u003e\r\n      \u003ctd\u003e0.248\u003c/td\u003e\r\n      \u003ctd\u003e26\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e7\u003c/th\u003e\r\n      \u003ctd\u003e10\u003c/td\u003e\r\n      \u003ctd\u003e115\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e35.3\u003c/td\u003e\r\n      \u003ctd\u003e0.134\u003c/td\u003e\r\n      \u003ctd\u003e29\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e8\u003c/th\u003e\r\n      \u003ctd\u003e2\u003c/td\u003e\r\n      \u003ctd\u003e197\u003c/td\u003e\r\n      \u003ctd\u003e70\u003c/td\u003e\r\n      \u003ctd\u003e45\u003c/td\u003e\r\n      \u003ctd\u003e543\u003c/td\u003e\r\n      \u003ctd\u003e30.5\u003c/td\u003e\r\n      \u003ctd\u003e0.158\u003c/td\u003e\r\n      \u003ctd\u003e53\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e9\u003c/th\u003e\r\n      \u003ctd\u003e8\u003c/td\u003e\r\n      \u003ctd\u003e125\u003c/td\u003e\r\n      \u003ctd\u003e96\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0.0\u003c/td\u003e\r\n      \u003ctd\u003e0.232\u003c/td\u003e\r\n      \u003ctd\u003e54\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e10\u003c/th\u003e\r\n      \u003ctd\u003e4\u003c/td\u003e\r\n      \u003ctd\u003e110\u003c/td\u003e\r\n      \u003ctd\u003e92\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e37.6\u003c/td\u003e\r\n      \u003ctd\u003e0.191\u003c/td\u003e\r\n      \u003ctd\u003e30\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e11\u003c/th\u003e\r\n      \u003ctd\u003e10\u003c/td\u003e\r\n      \u003ctd\u003e168\u003c/td\u003e\r\n      \u003ctd\u003e74\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e38.0\u003c/td\u003e\r\n      \u003ctd\u003e0.537\u003c/td\u003e\r\n      \u003ctd\u003e34\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e12\u003c/th\u003e\r\n      \u003ctd\u003e10\u003c/td\u003e\r\n      \u003ctd\u003e139\u003c/td\u003e\r\n      \u003ctd\u003e80\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e27.1\u003c/td\u003e\r\n      \u003ctd\u003e1.441\u003c/td\u003e\r\n      \u003ctd\u003e57\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e13\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e189\u003c/td\u003e\r\n      \u003ctd\u003e60\u003c/td\u003e\r\n      \u003ctd\u003e23\u003c/td\u003e\r\n      \u003ctd\u003e846\u003c/td\u003e\r\n      \u003ctd\u003e30.1\u003c/td\u003e\r\n      \u003ctd\u003e0.398\u003c/td\u003e\r\n      \u003ctd\u003e59\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e14\u003c/th\u003e\r\n      \u003ctd\u003e5\u003c/td\u003e\r\n      \u003ctd\u003e166\u003c/td\u003e\r\n      \u003ctd\u003e72\u003c/td\u003e\r\n      \u003ctd\u003e19\u003c/td\u003e\r\n      \u003ctd\u003e175\u003c/td\u003e\r\n      \u003ctd\u003e25.8\u003c/td\u003e\r\n      \u003ctd\u003e0.587\u003c/td\u003e\r\n      \u003ctd\u003e51\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e15\u003c/th\u003e\r\n      \u003ctd\u003e7\u003c/td\u003e\r\n      \u003ctd\u003e100\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e30.0\u003c/td\u003e\r\n      \u003ctd\u003e0.484\u003c/td\u003e\r\n      \u003ctd\u003e32\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e16\u003c/th\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e118\u003c/td\u003e\r\n      \u003ctd\u003e84\u003c/td\u003e\r\n      \u003ctd\u003e47\u003c/td\u003e\r\n      \u003ctd\u003e230\u003c/td\u003e\r\n      \u003ctd\u003e45.8\u003c/td\u003e\r\n      \u003ctd\u003e0.551\u003c/td\u003e\r\n      \u003ctd\u003e31\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e17\u003c/th\u003e\r\n      \u003ctd\u003e7\u003c/td\u003e\r\n      \u003ctd\u003e107\u003c/td\u003e\r\n      \u003ctd\u003e74\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e29.6\u003c/td\u003e\r\n      \u003ctd\u003e0.254\u003c/td\u003e\r\n      \u003ctd\u003e31\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e18\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e103\u003c/td\u003e\r\n      \u003ctd\u003e30\u003c/td\u003e\r\n      \u003ctd\u003e38\u003c/td\u003e\r\n      \u003ctd\u003e83\u003c/td\u003e\r\n      \u003ctd\u003e43.3\u003c/td\u003e\r\n      \u003ctd\u003e0.183\u003c/td\u003e\r\n      \u003ctd\u003e33\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e19\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e115\u003c/td\u003e\r\n      \u003ctd\u003e70\u003c/td\u003e\r\n      \u003ctd\u003e30\u003c/td\u003e\r\n      \u003ctd\u003e96\u003c/td\u003e\r\n      \u003ctd\u003e34.6\u003c/td\u003e\r\n      \u003ctd\u003e0.529\u003c/td\u003e\r\n      \u003ctd\u003e32\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n  \u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003c/div\u003e\r\n\r\n\r\n\r\n\u003cp\u003eClearly there are 0 values in the columns 2, 3, 4, and 5.\u003c/p\u003e\r\n\u003cp\u003eAs this dataset has missing values denoted as 0, so it might be tricky to handle it by just using the conventional means. Let\u0026#39;s summarize the approach you will follow to combat this:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003eGet the count of zeros in each of the columns you saw earlier.\u003c/li\u003e\r\n\u003cli\u003eDetermine which columns have the most zero values from the previous step.\u003c/li\u003e\r\n\u003cli\u003eReplace the zero values in those columns with \u003ccode\u003eNaN\u003c/code\u003e.\u003c/li\u003e\r\n\u003cli\u003eCheck if the NaNs are getting appropriately reflected.\u003c/li\u003e\r\n\u003cli\u003eCall the fillna() function with the imputation strategy.\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Step 1: Get the count of zeros in each of the columns\r\nprint((data[[1,2,3,4,5]] == 0).sum())\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e1      5\r\n2     35\r\n3    227\r\n4    374\r\n5     11\r\ndtype: int64\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eYou can see that columns 1,2 and 5 have just a few zero values, whereas columns 3 and 4 show a lot more, nearly half of the rows.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Step -2: Mark zero values as missing or NaN\r\ndata[[1,2,3,4,5]] = data[[1,2,3,4,5]].replace(0, np.NaN)\r\n\r\n# Count the number of NaN values in each column\r\nprint(data.isnull().sum())\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e0      0\r\n1      5\r\n2     35\r\n3    227\r\n4    374\r\n5     11\r\n6      0\r\n7      0\r\n8      0\r\ndtype: int64\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eLet\u0026#39;s get sure at this point of time that your NaN replacement was a hit by taking a look at the dataset as a whole:\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Step 4\r\ndata.head(20)\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cdiv\u003e\r\n\u003cstyle scoped\u003e\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\u003c/style\u003e\r\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\r\n  \u003cthead\u003e\r\n    \u003ctr style=\"text-align: right;\"\u003e\r\n      \u003cth\u003e\u003c/th\u003e\r\n      \u003cth\u003e0\u003c/th\u003e\r\n      \u003cth\u003e1\u003c/th\u003e\r\n      \u003cth\u003e2\u003c/th\u003e\r\n      \u003cth\u003e3\u003c/th\u003e\r\n      \u003cth\u003e4\u003c/th\u003e\r\n      \u003cth\u003e5\u003c/th\u003e\r\n      \u003cth\u003e6\u003c/th\u003e\r\n      \u003cth\u003e7\u003c/th\u003e\r\n      \u003cth\u003e8\u003c/th\u003e\r\n    \u003c/tr\u003e\r\n  \u003c/thead\u003e\r\n  \u003ctbody\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e0\u003c/th\u003e\r\n      \u003ctd\u003e6\u003c/td\u003e\r\n      \u003ctd\u003e148.0\u003c/td\u003e\r\n      \u003ctd\u003e72.0\u003c/td\u003e\r\n      \u003ctd\u003e35.0\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003e33.6\u003c/td\u003e\r\n      \u003ctd\u003e0.627\u003c/td\u003e\r\n      \u003ctd\u003e50\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e1\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e85.0\u003c/td\u003e\r\n      \u003ctd\u003e66.0\u003c/td\u003e\r\n      \u003ctd\u003e29.0\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003e26.6\u003c/td\u003e\r\n      \u003ctd\u003e0.351\u003c/td\u003e\r\n      \u003ctd\u003e31\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e2\u003c/th\u003e\r\n      \u003ctd\u003e8\u003c/td\u003e\r\n      \u003ctd\u003e183.0\u003c/td\u003e\r\n      \u003ctd\u003e64.0\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003e23.3\u003c/td\u003e\r\n      \u003ctd\u003e0.672\u003c/td\u003e\r\n      \u003ctd\u003e32\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e3\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e89.0\u003c/td\u003e\r\n      \u003ctd\u003e66.0\u003c/td\u003e\r\n      \u003ctd\u003e23.0\u003c/td\u003e\r\n      \u003ctd\u003e94.0\u003c/td\u003e\r\n      \u003ctd\u003e28.1\u003c/td\u003e\r\n      \u003ctd\u003e0.167\u003c/td\u003e\r\n      \u003ctd\u003e21\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e4\u003c/th\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e137.0\u003c/td\u003e\r\n      \u003ctd\u003e40.0\u003c/td\u003e\r\n      \u003ctd\u003e35.0\u003c/td\u003e\r\n      \u003ctd\u003e168.0\u003c/td\u003e\r\n      \u003ctd\u003e43.1\u003c/td\u003e\r\n      \u003ctd\u003e2.288\u003c/td\u003e\r\n      \u003ctd\u003e33\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e5\u003c/th\u003e\r\n      \u003ctd\u003e5\u003c/td\u003e\r\n      \u003ctd\u003e116.0\u003c/td\u003e\r\n      \u003ctd\u003e74.0\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003e25.6\u003c/td\u003e\r\n      \u003ctd\u003e0.201\u003c/td\u003e\r\n      \u003ctd\u003e30\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e6\u003c/th\u003e\r\n      \u003ctd\u003e3\u003c/td\u003e\r\n      \u003ctd\u003e78.0\u003c/td\u003e\r\n      \u003ctd\u003e50.0\u003c/td\u003e\r\n      \u003ctd\u003e32.0\u003c/td\u003e\r\n      \u003ctd\u003e88.0\u003c/td\u003e\r\n      \u003ctd\u003e31.0\u003c/td\u003e\r\n      \u003ctd\u003e0.248\u003c/td\u003e\r\n      \u003ctd\u003e26\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e7\u003c/th\u003e\r\n      \u003ctd\u003e10\u003c/td\u003e\r\n      \u003ctd\u003e115.0\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003e35.3\u003c/td\u003e\r\n      \u003ctd\u003e0.134\u003c/td\u003e\r\n      \u003ctd\u003e29\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e8\u003c/th\u003e\r\n      \u003ctd\u003e2\u003c/td\u003e\r\n      \u003ctd\u003e197.0\u003c/td\u003e\r\n      \u003ctd\u003e70.0\u003c/td\u003e\r\n      \u003ctd\u003e45.0\u003c/td\u003e\r\n      \u003ctd\u003e543.0\u003c/td\u003e\r\n      \u003ctd\u003e30.5\u003c/td\u003e\r\n      \u003ctd\u003e0.158\u003c/td\u003e\r\n      \u003ctd\u003e53\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e9\u003c/th\u003e\r\n      \u003ctd\u003e8\u003c/td\u003e\r\n      \u003ctd\u003e125.0\u003c/td\u003e\r\n      \u003ctd\u003e96.0\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003e0.232\u003c/td\u003e\r\n      \u003ctd\u003e54\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e10\u003c/th\u003e\r\n      \u003ctd\u003e4\u003c/td\u003e\r\n      \u003ctd\u003e110.0\u003c/td\u003e\r\n      \u003ctd\u003e92.0\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003e37.6\u003c/td\u003e\r\n      \u003ctd\u003e0.191\u003c/td\u003e\r\n      \u003ctd\u003e30\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e11\u003c/th\u003e\r\n      \u003ctd\u003e10\u003c/td\u003e\r\n      \u003ctd\u003e168.0\u003c/td\u003e\r\n      \u003ctd\u003e74.0\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003e38.0\u003c/td\u003e\r\n      \u003ctd\u003e0.537\u003c/td\u003e\r\n      \u003ctd\u003e34\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e12\u003c/th\u003e\r\n      \u003ctd\u003e10\u003c/td\u003e\r\n      \u003ctd\u003e139.0\u003c/td\u003e\r\n      \u003ctd\u003e80.0\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003e27.1\u003c/td\u003e\r\n      \u003ctd\u003e1.441\u003c/td\u003e\r\n      \u003ctd\u003e57\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e13\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e189.0\u003c/td\u003e\r\n      \u003ctd\u003e60.0\u003c/td\u003e\r\n      \u003ctd\u003e23.0\u003c/td\u003e\r\n      \u003ctd\u003e846.0\u003c/td\u003e\r\n      \u003ctd\u003e30.1\u003c/td\u003e\r\n      \u003ctd\u003e0.398\u003c/td\u003e\r\n      \u003ctd\u003e59\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e14\u003c/th\u003e\r\n      \u003ctd\u003e5\u003c/td\u003e\r\n      \u003ctd\u003e166.0\u003c/td\u003e\r\n      \u003ctd\u003e72.0\u003c/td\u003e\r\n      \u003ctd\u003e19.0\u003c/td\u003e\r\n      \u003ctd\u003e175.0\u003c/td\u003e\r\n      \u003ctd\u003e25.8\u003c/td\u003e\r\n      \u003ctd\u003e0.587\u003c/td\u003e\r\n      \u003ctd\u003e51\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e15\u003c/th\u003e\r\n      \u003ctd\u003e7\u003c/td\u003e\r\n      \u003ctd\u003e100.0\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003e30.0\u003c/td\u003e\r\n      \u003ctd\u003e0.484\u003c/td\u003e\r\n      \u003ctd\u003e32\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e16\u003c/th\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e118.0\u003c/td\u003e\r\n      \u003ctd\u003e84.0\u003c/td\u003e\r\n      \u003ctd\u003e47.0\u003c/td\u003e\r\n      \u003ctd\u003e230.0\u003c/td\u003e\r\n      \u003ctd\u003e45.8\u003c/td\u003e\r\n      \u003ctd\u003e0.551\u003c/td\u003e\r\n      \u003ctd\u003e31\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e17\u003c/th\u003e\r\n      \u003ctd\u003e7\u003c/td\u003e\r\n      \u003ctd\u003e107.0\u003c/td\u003e\r\n      \u003ctd\u003e74.0\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003eNaN\u003c/td\u003e\r\n      \u003ctd\u003e29.6\u003c/td\u003e\r\n      \u003ctd\u003e0.254\u003c/td\u003e\r\n      \u003ctd\u003e31\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e18\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e103.0\u003c/td\u003e\r\n      \u003ctd\u003e30.0\u003c/td\u003e\r\n      \u003ctd\u003e38.0\u003c/td\u003e\r\n      \u003ctd\u003e83.0\u003c/td\u003e\r\n      \u003ctd\u003e43.3\u003c/td\u003e\r\n      \u003ctd\u003e0.183\u003c/td\u003e\r\n      \u003ctd\u003e33\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e19\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e115.0\u003c/td\u003e\r\n      \u003ctd\u003e70.0\u003c/td\u003e\r\n      \u003ctd\u003e30.0\u003c/td\u003e\r\n      \u003ctd\u003e96.0\u003c/td\u003e\r\n      \u003ctd\u003e34.6\u003c/td\u003e\r\n      \u003ctd\u003e0.529\u003c/td\u003e\r\n      \u003ctd\u003e32\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n  \u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003c/div\u003e\r\n\r\n\r\n\r\n\u003cp\u003eYou can see that marking the missing values had the intended effect.\u003c/p\u003e\r\n\u003cp\u003eUp till now, you analyzed essential trends when data is missing and how you can make use of simple statistical measures to get a hold of it. Now, you will impute the missing values using \u003cstrong\u003eMean Imputation\u003c/strong\u003e which is essentially imputing the mean of the respective column in place of missing values.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Step 5: Call the fillna() function with the imputation strategy\r\ndata.fillna(data.mean(), inplace=True)\r\n\r\n# Count the number of NaN values in each column to verify\r\nprint(data.isnull().sum())\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003e0    0\r\n1    0\r\n2    0\r\n3    0\r\n4    0\r\n5    0\r\n6    0\r\n7    0\r\n8    0\r\ndtype: int64\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eExcellent!\u003c/p\u003e\r\n\u003cp\u003eThis \u003ca href=\"https://www.datacamp.com/community/tutorials/preprocessing-in-data-science-part-1-centering-scaling-and-knn\"\u003eDataCamp article\u003c/a\u003e effectively guides you in implementing \u003cstrong\u003edata scaling\u003c/strong\u003e as a data preprocessing step. Be sure to check it out.\u003c/p\u003e\r\n\u003cp\u003eNext, you will do \u003cstrong\u003evariable encoding\u003c/strong\u003e.\u003c/p\u003e\r\n\u003cp\u003eBefore that, you need a dataset which actually contains non-numeric data. You will use the famous \u003ca href=\"http://archive.ics.uci.edu/ml/datasets/Iris\"\u003eIris dataset\u003c/a\u003e for this.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Load the dataset to a DataFrame object iris\r\niris = pd.read_csv(\u0026quot;http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\u0026quot;,header=None)\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# See first 20 rows of the dataset\r\niris.head(20)\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cdiv\u003e\r\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\r\n  \u003cthead\u003e\r\n    \u003ctr style=\"text-align: right;\"\u003e\r\n      \u003cth\u003e\u003c/th\u003e\r\n      \u003cth\u003e0\u003c/th\u003e\r\n      \u003cth\u003e1\u003c/th\u003e\r\n      \u003cth\u003e2\u003c/th\u003e\r\n      \u003cth\u003e3\u003c/th\u003e\r\n      \u003cth\u003e4\u003c/th\u003e\r\n    \u003c/tr\u003e\r\n  \u003c/thead\u003e\r\n  \u003ctbody\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e0\u003c/th\u003e\r\n      \u003ctd\u003e5.1\u003c/td\u003e\r\n      \u003ctd\u003e3.5\u003c/td\u003e\r\n      \u003ctd\u003e1.4\u003c/td\u003e\r\n      \u003ctd\u003e0.2\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e1\u003c/th\u003e\r\n      \u003ctd\u003e4.9\u003c/td\u003e\r\n      \u003ctd\u003e3.0\u003c/td\u003e\r\n      \u003ctd\u003e1.4\u003c/td\u003e\r\n      \u003ctd\u003e0.2\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e2\u003c/th\u003e\r\n      \u003ctd\u003e4.7\u003c/td\u003e\r\n      \u003ctd\u003e3.2\u003c/td\u003e\r\n      \u003ctd\u003e1.3\u003c/td\u003e\r\n      \u003ctd\u003e0.2\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e3\u003c/th\u003e\r\n      \u003ctd\u003e4.6\u003c/td\u003e\r\n      \u003ctd\u003e3.1\u003c/td\u003e\r\n      \u003ctd\u003e1.5\u003c/td\u003e\r\n      \u003ctd\u003e0.2\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e4\u003c/th\u003e\r\n      \u003ctd\u003e5.0\u003c/td\u003e\r\n      \u003ctd\u003e3.6\u003c/td\u003e\r\n      \u003ctd\u003e1.4\u003c/td\u003e\r\n      \u003ctd\u003e0.2\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e5\u003c/th\u003e\r\n      \u003ctd\u003e5.4\u003c/td\u003e\r\n      \u003ctd\u003e3.9\u003c/td\u003e\r\n      \u003ctd\u003e1.7\u003c/td\u003e\r\n      \u003ctd\u003e0.4\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e6\u003c/th\u003e\r\n      \u003ctd\u003e4.6\u003c/td\u003e\r\n      \u003ctd\u003e3.4\u003c/td\u003e\r\n      \u003ctd\u003e1.4\u003c/td\u003e\r\n      \u003ctd\u003e0.3\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e7\u003c/th\u003e\r\n      \u003ctd\u003e5.0\u003c/td\u003e\r\n      \u003ctd\u003e3.4\u003c/td\u003e\r\n      \u003ctd\u003e1.5\u003c/td\u003e\r\n      \u003ctd\u003e0.2\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e8\u003c/th\u003e\r\n      \u003ctd\u003e4.4\u003c/td\u003e\r\n      \u003ctd\u003e2.9\u003c/td\u003e\r\n      \u003ctd\u003e1.4\u003c/td\u003e\r\n      \u003ctd\u003e0.2\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e9\u003c/th\u003e\r\n      \u003ctd\u003e4.9\u003c/td\u003e\r\n      \u003ctd\u003e3.1\u003c/td\u003e\r\n      \u003ctd\u003e1.5\u003c/td\u003e\r\n      \u003ctd\u003e0.1\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e10\u003c/th\u003e\r\n      \u003ctd\u003e5.4\u003c/td\u003e\r\n      \u003ctd\u003e3.7\u003c/td\u003e\r\n      \u003ctd\u003e1.5\u003c/td\u003e\r\n      \u003ctd\u003e0.2\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e11\u003c/th\u003e\r\n      \u003ctd\u003e4.8\u003c/td\u003e\r\n      \u003ctd\u003e3.4\u003c/td\u003e\r\n      \u003ctd\u003e1.6\u003c/td\u003e\r\n      \u003ctd\u003e0.2\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e12\u003c/th\u003e\r\n      \u003ctd\u003e4.8\u003c/td\u003e\r\n      \u003ctd\u003e3.0\u003c/td\u003e\r\n      \u003ctd\u003e1.4\u003c/td\u003e\r\n      \u003ctd\u003e0.1\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e13\u003c/th\u003e\r\n      \u003ctd\u003e4.3\u003c/td\u003e\r\n      \u003ctd\u003e3.0\u003c/td\u003e\r\n      \u003ctd\u003e1.1\u003c/td\u003e\r\n      \u003ctd\u003e0.1\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e14\u003c/th\u003e\r\n      \u003ctd\u003e5.8\u003c/td\u003e\r\n      \u003ctd\u003e4.0\u003c/td\u003e\r\n      \u003ctd\u003e1.2\u003c/td\u003e\r\n      \u003ctd\u003e0.2\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e15\u003c/th\u003e\r\n      \u003ctd\u003e5.7\u003c/td\u003e\r\n      \u003ctd\u003e4.4\u003c/td\u003e\r\n      \u003ctd\u003e1.5\u003c/td\u003e\r\n      \u003ctd\u003e0.4\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e16\u003c/th\u003e\r\n      \u003ctd\u003e5.4\u003c/td\u003e\r\n      \u003ctd\u003e3.9\u003c/td\u003e\r\n      \u003ctd\u003e1.3\u003c/td\u003e\r\n      \u003ctd\u003e0.4\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e17\u003c/th\u003e\r\n      \u003ctd\u003e5.1\u003c/td\u003e\r\n      \u003ctd\u003e3.5\u003c/td\u003e\r\n      \u003ctd\u003e1.4\u003c/td\u003e\r\n      \u003ctd\u003e0.3\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e18\u003c/th\u003e\r\n      \u003ctd\u003e5.7\u003c/td\u003e\r\n      \u003ctd\u003e3.8\u003c/td\u003e\r\n      \u003ctd\u003e1.7\u003c/td\u003e\r\n      \u003ctd\u003e0.3\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e19\u003c/th\u003e\r\n      \u003ctd\u003e5.1\u003c/td\u003e\r\n      \u003ctd\u003e3.8\u003c/td\u003e\r\n      \u003ctd\u003e1.5\u003c/td\u003e\r\n      \u003ctd\u003e0.3\u003c/td\u003e\r\n      \u003ctd\u003eIris-setosa\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n  \u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003c/div\u003e\r\n\r\n\r\n\r\n\u003cp\u003eYou can easily convert the string values to integer values using the \u003ca href=\"http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\"\u003eLabelEncoder\u003c/a\u003e. The three class values (Iris-setosa, Iris-versicolor, Iris-virginica) are mapped to the integer values (0, 1, 2).\u003c/p\u003e\r\n\u003cp\u003eIn this case, the fourth column/feature of the dataset contains non-numeric values. So you need to separate it out.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Convert the DataFrame to a NumPy array\r\niris = iris.values\r\n\r\n# Separate\r\nY = iris[:,4]\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Label Encode string class values as integers\r\nfrom sklearn.preprocessing import LabelEncoder\r\n\r\nlabel_encoder = LabelEncoder()\r\nlabel_encoder = label_encoder.fit(Y)\r\nlabel_encoded_y = label_encoder.transform(Y)\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eNow, let\u0026#39;s study another area where the need for elementary knowledge of statistics is very crucial.\u003c/p\u003e\r\n\u003ch2 id=\"statistics-for-model-evaluation-\"\u003eStatistics for model evaluation:\u003c/h2\u003e\r\n\u003cp\u003eYou have designed and developed your machine learning model. Now, you want to evaluate the performance of your model on the test data. In this regards, you seek help of various statistical metrics like Precision, Recall, ROC, AUC, RMSE, etc. You also seek help from multiple data resampling techniques such as \u003cstrong\u003ek-fold Cross-Validation\u003c/strong\u003e.\u003c/p\u003e\r\n\u003cp\u003eStatistics can effectively be used to:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003ca href=\"https://machinelearningmastery.com/statistical-significance-tests-for-comparing-machine-learning-algorithms/\"\u003eEstimate a hypothesis accuracy\u003c/a\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003ca href=\"https://www.universalclass.com/articles/math/statistics/types-of-errors-in-hypothesis-testing.htm\"\u003eDetermine the error of two hypotheses\u003c/a\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003ca href=\"https://machinelearningmastery.com/mcnemars-test-for-machine-learning/\"\u003eCompare learning algorithms using McNemar\u0026#39;s test\u003c/a\u003e\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003e\u003cem\u003eIt is important to note that the hypothesis refers to learned models; the results of running a learning algorithm on a dataset. Evaluating and comparing the hypothesis means comparing learned models, which is different from evaluating and comparing machine learning algorithms, which could be trained on different samples from the same problem or various problems.\u003c/em\u003e\u003c/p\u003e\r\n\u003cp\u003eLet\u0026#39;s study Gaussian and Descriptive statistics now.\u003c/p\u003e\r\n\u003ch2 id=\"introduction-to-gaussian-and-descriptive-stats-\"\u003eIntroduction to Gaussian and Descriptive stats:\u003c/h2\u003e\r\n\u003cp\u003eA sample of data is nothing but a snapshot from a broader population of all the potential observations that could be taken from a domain or generated by a process.\u003c/p\u003e\r\n\u003cp\u003eInterestingly, many observations fit a typical pattern or distribution called the normal distribution, or more formally, the Gaussian distribution. This is the bell-shaped distribution that you may be aware of. The following figure denotes a Gaussian distribution:\u003c/p\u003e\r\n\u003cp\u003e\u003cimg src = \"http://hyperphysics.phy-astr.gsu.edu/hbase/Math/immath/gauds.gif\"\u003e\u003c/img\u003e\u003c/p\u003e\r\n\u003cp\u003e\u003cem\u003e\u003cstrong\u003eSource: HyperPhysics\u003c/strong\u003e\u003c/em\u003e\u003c/p\u003e\r\n\u003cp\u003eGaussian processes and Gaussian distributions are whole another sub-fields unto themselves. But, you will now study two of the most essential ingredients that build the entire world of Gaussian distributions in general.\u003c/p\u003e\r\n\u003cp\u003eAny sample data taken from a Gaussian distribution can be summarized with two parameters:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003cstrong\u003eMean\u003c/strong\u003e: The central tendency or most likely value in the distribution (the top of the bell).\u003c/li\u003e\r\n\u003cli\u003e\u003cstrong\u003eVariance\u003c/strong\u003e: The average difference that observations have from the mean value in the distribution (the spread).\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eThe term \u003cem\u003evariance\u003c/em\u003e also gives rise to another critical term, i.e., \u003cem\u003estandard deviation\u003c/em\u003e, which is merely the square root of the variance.\u003c/p\u003e\r\n\u003cp\u003eThe mean, variance, and standard deviation can be directly calculated from data samples using \u003ccode\u003enumpy\u003c/code\u003e.\u003c/p\u003e\r\n\u003cp\u003eYou will first generate a sample of 100 random numbers pulled from a Gaussian distribution with a mean of 50 and a standard deviation of 5. You will then calculate the summary statistics.\u003c/p\u003e\r\n\u003cp\u003eFirst, you will import all the dependencies.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e#  Dependencies\r\nfrom numpy.random import seed\r\nfrom numpy.random import randn\r\nfrom numpy import mean\r\nfrom numpy import var\r\nfrom numpy import std\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003eNext, you set the random number generator seed so that your results are reproducible.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eseed(1)\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Generate univariate observations\r\ndata = 5 * randn(10000) + 50\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Calculate statistics\r\nprint(\u0026#39;Mean: %.3f\u0026#39; % mean(data))\r\nprint(\u0026#39;Variance: %.3f\u0026#39; % var(data))\r\nprint(\u0026#39;Standard Deviation: %.3f\u0026#39; % std(data))\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003eMean: 50.049\r\nVariance: 24.939\r\nStandard Deviation: 4.994\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eClose enough, eh?\u003c/p\u003e\r\n\u003cp\u003eLet\u0026#39;s study the next topic now.\u003c/p\u003e\r\n\u003ch2 id=\"variable-correlation-\"\u003eVariable correlation:\u003c/h2\u003e\r\n\u003cp\u003eGenerally, the features that are contained in a dataset can often be related to each other which is very obvious to happen in practice. In statistical terms, this relationship between the features of your dataset (be it simple or complex) is often termed as \u003cem\u003ecorrelation\u003c/em\u003e.\u003c/p\u003e\r\n\u003cp\u003eIt is crucial to find out the degree of the correlation of the features in a dataset. This step essentially serves you as \u003cem\u003efeature selection\u003c/em\u003e which concerns selecting the most important features from a dataset. This step is one of the most vital steps in a standard machine learning pipeline as it can give you a tremendous accuracy boost that too within a lesser amount of time.\u003c/p\u003e\r\n\u003cp\u003eFor better understanding and to keep it more practical let\u0026#39;s understand why features can be related to each other:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003eOne feature can be a determinant of another feature\u003c/li\u003e\r\n\u003cli\u003eOne feature could be associated with another feature in some degree of composition\u003c/li\u003e\r\n\u003cli\u003eMultiple features can combine and give birth to another feature\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eCorrelation between the features can be of three types: - \u003cstrong\u003ePositive correlation\u003c/strong\u003e where both the feature change in the same direction, \u003cstrong\u003eNeutral correlation\u003c/strong\u003e when there is no relationship of the change in the two features, \u003cstrong\u003eNegative correlation\u003c/strong\u003e where both the features change in opposite directions.\u003c/p\u003e\r\n\u003cp\u003eCorrelation measurements form the fundamental of filter-based feature selection techniques. Check \u003ca href=\"https://www.datacamp.com/community/tutorials/feature-selection-python\"\u003ethis article\u003c/a\u003e if you want to study more about feature selection.\u003c/p\u003e\r\n\u003cp\u003eYou can mathematically the relationship between samples of two variables using a statistical method called \u003ca href=\"https://en.wikipedia.org/wiki/Pearson_correlation_coefficient\"\u003ePearsonâ€™s correlation coefficient\u003c/a\u003e, named after the developer of the method, \u003cstrong\u003eKarl Pearson\u003c/strong\u003e.\u003c/p\u003e\r\n\u003cp\u003eYou can calculate the Pearson\u0026#39;s correlation score by using the \u003ccode\u003ecorr()\u003c/code\u003e function of \u003ccode\u003epandas\u003c/code\u003e with the \u003ccode\u003emethod\u003c/code\u003e parameter as \u003ccode\u003epearson\u003c/code\u003e. Let\u0026#39;s study the correlation between the features of the Pima Indians Diabetes dataset that you used earlier. You already have the data in good shape.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Data\r\ndata.head()\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cdiv\u003e\r\n\u003cstyle scoped\u003e\r\n    .dataframe tbody tr th:only-of-type {\r\n        vertical-align: middle;\r\n    }\r\n\r\n    .dataframe tbody tr th {\r\n        vertical-align: top;\r\n    }\r\n\r\n    .dataframe thead th {\r\n        text-align: right;\r\n    }\r\n\u003c/style\u003e\r\n\u003ctable border=\"1\" class=\"dataframe\"\u003e\r\n  \u003cthead\u003e\r\n    \u003ctr style=\"text-align: right;\"\u003e\r\n      \u003cth\u003e\u003c/th\u003e\r\n      \u003cth\u003e0\u003c/th\u003e\r\n      \u003cth\u003e1\u003c/th\u003e\r\n      \u003cth\u003e2\u003c/th\u003e\r\n      \u003cth\u003e3\u003c/th\u003e\r\n      \u003cth\u003e4\u003c/th\u003e\r\n      \u003cth\u003e5\u003c/th\u003e\r\n      \u003cth\u003e6\u003c/th\u003e\r\n      \u003cth\u003e7\u003c/th\u003e\r\n      \u003cth\u003e8\u003c/th\u003e\r\n    \u003c/tr\u003e\r\n  \u003c/thead\u003e\r\n  \u003ctbody\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e0\u003c/th\u003e\r\n      \u003ctd\u003e6\u003c/td\u003e\r\n      \u003ctd\u003e148.0\u003c/td\u003e\r\n      \u003ctd\u003e72.0\u003c/td\u003e\r\n      \u003ctd\u003e35.00000\u003c/td\u003e\r\n      \u003ctd\u003e155.548223\u003c/td\u003e\r\n      \u003ctd\u003e33.6\u003c/td\u003e\r\n      \u003ctd\u003e0.627\u003c/td\u003e\r\n      \u003ctd\u003e50\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e1\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e85.0\u003c/td\u003e\r\n      \u003ctd\u003e66.0\u003c/td\u003e\r\n      \u003ctd\u003e29.00000\u003c/td\u003e\r\n      \u003ctd\u003e155.548223\u003c/td\u003e\r\n      \u003ctd\u003e26.6\u003c/td\u003e\r\n      \u003ctd\u003e0.351\u003c/td\u003e\r\n      \u003ctd\u003e31\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e2\u003c/th\u003e\r\n      \u003ctd\u003e8\u003c/td\u003e\r\n      \u003ctd\u003e183.0\u003c/td\u003e\r\n      \u003ctd\u003e64.0\u003c/td\u003e\r\n      \u003ctd\u003e29.15342\u003c/td\u003e\r\n      \u003ctd\u003e155.548223\u003c/td\u003e\r\n      \u003ctd\u003e23.3\u003c/td\u003e\r\n      \u003ctd\u003e0.672\u003c/td\u003e\r\n      \u003ctd\u003e32\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e3\u003c/th\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n      \u003ctd\u003e89.0\u003c/td\u003e\r\n      \u003ctd\u003e66.0\u003c/td\u003e\r\n      \u003ctd\u003e23.00000\u003c/td\u003e\r\n      \u003ctd\u003e94.000000\u003c/td\u003e\r\n      \u003ctd\u003e28.1\u003c/td\u003e\r\n      \u003ctd\u003e0.167\u003c/td\u003e\r\n      \u003ctd\u003e21\u003c/td\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n    \u003ctr\u003e\r\n      \u003cth\u003e4\u003c/th\u003e\r\n      \u003ctd\u003e0\u003c/td\u003e\r\n      \u003ctd\u003e137.0\u003c/td\u003e\r\n      \u003ctd\u003e40.0\u003c/td\u003e\r\n      \u003ctd\u003e35.00000\u003c/td\u003e\r\n      \u003ctd\u003e168.000000\u003c/td\u003e\r\n      \u003ctd\u003e43.1\u003c/td\u003e\r\n      \u003ctd\u003e2.288\u003c/td\u003e\r\n      \u003ctd\u003e33\u003c/td\u003e\r\n      \u003ctd\u003e1\u003c/td\u003e\r\n    \u003c/tr\u003e\r\n  \u003c/tbody\u003e\r\n\u003c/table\u003e\r\n\u003c/div\u003e\r\n\r\n\r\n\r\n\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Create the matrix of correlation score between the features and the label\r\nscoreTable = data.corr(method=\u0026#39;pearson\u0026#39;)\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Visulaize the matrix\r\ndata.corr(method=\u0026#39;pearson\u0026#39;).style.format(\u0026quot;{:.2}\u0026quot;).background_gradient(cmap=plt.get_cmap(\u0026#39;coolwarm\u0026#39;), axis=1)\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cp\u003e\u003ccenter\u003e\u003cImg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1537966124/Pearson_table_c1t5rt.png\" /\u003e\u003c/p\u003e\r\n\u003cp\u003eYou can clearly see the Pearson\u0026#39;s correlation between all the features and the label of the dataset.\u003c/p\u003e\r\n\u003cp\u003eIn the next section, you will study non-parametric statistics.\u003c/p\u003e\r\n\u003ch2 id=\"non-parametric-statistics-\"\u003eNon-parametric statistics:\u003c/h2\u003e\r\n\u003cp\u003eA large portion of the field of statistics and statistical methods is dedicated to data where the distribution is known.\u003c/p\u003e\r\n\u003cp\u003eNon-parametric statistics comes in handy when there is no or few information available about the population parameters. Non-parametric tests make no assumptions about the distribution of data.\u003c/p\u003e\r\n\u003cp\u003eIn the case where you are working with nonparametric data, specialized nonparametric statistical methods can be used that discard all information about the distribution. As such, these methods are often referred to as \u003cem\u003edistribution-free\u003c/em\u003e methods.\u003c/p\u003e\r\n\u003cp\u003eBu before a nonparametric statistical method can be applied, the data must be converted into a rank format. Statistical methods that expect data in a rank format are sometimes called \u003cem\u003erank statistics\u003c/em\u003e. Examples of rank statistics can be rank correlation and rank statistical hypothesis tests. Ranking data is exactly as its name suggests.\u003c/p\u003e\r\n\u003cp\u003eA widely used nonparametric statistical hypothesis test for checking for a difference between two independent samples is the \u003cem\u003e\u003cstrong\u003eMann-Whitney U test\u003c/strong\u003e\u003c/em\u003e, named for Henry Mann and Donald Whitney.\u003c/p\u003e\r\n\u003cp\u003eYou will implement this test in Python via the \u003ccode\u003emannwhitneyu()\u003c/code\u003e which is provided by \u003ccode\u003eSciPy\u003c/code\u003e.\u003c/p\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# The dependencies that you need\r\nfrom scipy.stats import mannwhitneyu\r\nfrom numpy.random import rand\r\n\r\n# seed the random number generator\r\nseed(1)\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Generate two independent samples\r\ndata1 = 50 + (rand(100) * 10)\r\ndata2 = 51 + (rand(100) * 10)\r\n# Compare samples\r\nstat, p = mannwhitneyu(data1, data2)\r\nprint(\u0026#39;Statistics = %.3f, p = %.3f\u0026#39; % (stat, p))\r\n# Interpret\r\nalpha = 0.05\r\nif p \u0026gt; alpha:\r\n    print(\u0026#39;Same distribution (fail to reject H0)\u0026#39;)\r\nelse:\r\n    print(\u0026#39;Different distribution (reject H0)\u0026#39;)\r\n\u003c/code\u003e\u003c/pre\u003e\r\n\u003cpre\u003e\u003ccode\u003eStatistics = 4077.000, p = 0.012\r\nDifferent distribution (reject H0)\r\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003e\u003ccode\u003ealpha\u003c/code\u003e is the threshold parameter which is decided by you. The \u003ccode\u003emannwhitneyu()\u003c/code\u003e returns two things:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003cp\u003estatistic: The Mann-Whitney U statistic, equal to min(U for x, U for y) if alternative is equal to None (deprecated; exists for backward compatibility), and U for y otherwise.\u003c/p\u003e\r\n\u003c/li\u003e\r\n\u003cli\u003e\u003cp\u003epvalue:  p-value assuming an asymptotic normal distribution.\u003c/p\u003e\r\n\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eIf you want to study the other methods of Non-parametric statistics, you can do it from \u003ca href=\"https://www.analyticsvidhya.com/blog/2017/11/a-guide-to-conduct-analysis-using-non-parametric-tests/\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\r\n\u003cp\u003eThe other two popular non-parametric statistical significance tests that you can use are:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Friedman_test\"\u003eFriedman test\u003c/a\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test\"\u003eWilcoxon signed-rank test\u003c/a\u003e\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003ch2 id=\"that-calls-for-a-wrap-up-\"\u003eThat calls for a wrap up!\u003c/h2\u003e\r\n\u003cp\u003eYou have finally made it to the end. In this article, you studied a variety of essential statistical concepts that play very crucial role in your machine learning projects. So, understanding them is just important.\u003c/p\u003e\r\n\u003cp\u003eFrom mere an introduction to statistics, you took it to statistical rankings that too with several implementations. That is definitely quite a feat. You studied three different datasets, exploited \u003ccode\u003epandas\u003c/code\u003e and \u003ccode\u003enumpy\u003c/code\u003e functionalities to the fullest and moreover, you used \u003ccode\u003eSciPy\u003c/code\u003e as well. Next are some links for you if you want to take things further:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003ca href=\"https://web.stanford.edu/~hastie/Papers/ESLII.pdf\"\u003eThe Elements of Statistical Learning\u003c/a\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003ca href=\"https://www.cs.ubbcluj.ro/~gabis/ml/ml-books/McGrawHill%20-%20Machine%20Learning%20-Tom%20Mitchell.pdf\"\u003eMachine Learning book by Tom Mitchell\u003c/a\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003ca href=\"https://www.ic.unicamp.br/~wainer/cursos/1s2013/ml/livro.pdf\"\u003eAll for Statistics\u003c/a\u003e\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eFollowing are the resources I took help from for writing this blog:\u003c/p\u003e\r\n\u003cul\u003e\r\n\u003cli\u003e\u003ca href=\"https://machinelearningmastery.com/statistics-for-machine-learning-mini-course/\"\u003eMachine Learning Mastery mini course on Statistics\u003c/a\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003ca href=\"https://machinelearningmastery.com/statistical-sampling-and-resampling/\"\u003eA Gentle Introduction to Statistical Sampling and Resampling\u003c/a\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003ca href=\"https://www.khanacademy.org/math/statistics-probability\"\u003ehttps://www.khanacademy.org/math/statistics-probability\u003c/a\u003e\u003c/li\u003e\r\n\u003cli\u003e\u003ca href=\"https://statlearning.class.stanford.edu/\"\u003eStatistical Learning course by Stanford University\u003c/a\u003e\u003c/li\u003e\r\n\u003c/ul\u003e\r\n\u003cp\u003eLet me know your views/queries in the comments section. Also, check out \u003ca href=\"https://www.datacamp.com/courses/statistical-thinking-in-python-part-1\"\u003eDataCamp\u0026#39;s course on \u0026quot;Statistical Thinking in Python\u0026quot;\u003c/a\u003e which is very practically aligned.\u003c/p\u003e\r\n","contentUrl":"https://www.datacamp.com/community/tutorials/demystifying-crucial-statistics-python","userContentUrl":null,"illustrationUrl":null,"seoTitle":"Demystifying Crucial Statistics in Python","seoMetaDescription":"Learn about the basic statistics required for Data Science and Machine Learning in Python.","seoKeyword":"statistics machine learning data science python","mustRead":true,"programmingLanguage":null,"submissionDate":"2018-09-25T19:56:06.550Z","publishDate":"2018-09-27T16:00:00.000Z","episode":null,"isLatest":null,"externalUrl":null,"transcriptUrl":null,"guests":[],"links":null,"isSpam":false,"xp":0,"flaggingUsers":[],"isDisabled":false,"connectedInternalContentId":12323,"createdAt":"2018-09-25T19:56:06.545Z","updatedAt":"2018-10-15T13:25:28.191Z","upvoting":{"voteCount":45,"voted":false},"tags":["python","machine learning","statistical modeling"],"author":{"id":535025,"slug":"spsayakpaul","avatarUrlSquare":"https://assets.datacamp.com/users/avatars/000/535/025/square/Capture.PNG?1556868932","fullName":"Sayak  Paul","nameFromEmail":"spsayakpaul","isAdmin":false},"recommendedArticles":[]},"isFetched":true,"isFetching":false,"statusMessage":""},"countdownBanner":{"banner":{"showBanner":false,"title":"","text":"","startDate":"","endDate":"","link":"","nonPromo":false},"isBannerOpen":false},"form":{},"list":{"isFetched":false,"isFetching":false,"statusMessage":""},"menu":{"isSidebarMenuOpen":false},"notifications":{"isFetched":false,"isFetching":false,"isReadFetched":false,"isReadFetching":false,"statusMessage":"","readStatusMessage":"","Notifications":[],"NotificationsTotal":0,"unReadCount":0},"preview":{"isFetching":false,"isFetched":false,"statusMessage":"","content":{}},"recommendCS":{"isPosting":false,"isPosted":false,"statusMessage":"","isModalOpen":false,"currentStep":"form"},"spam":{"isFlagging":false,"isSucceeded":false,"statusMessage":"","isSpamModalOpen":false,"isUnSpamModalOpen":false},"tag":{"isRequesting":false,"isSucceeded":false,"statusMessage":"","isDeleteTagModalOpen":false},"tagList":{"isFetched":false,"isFetching":false,"statusMessage":"","list":[],"total":0},"tagSearch":{"isFetching":false,"isFetched":false,"statusMessage":"","content":{}},"user":{"isFetching":false,"isFetched":false,"statusMessage":"","unBan":{"isUnBanning":false,"isSucceeded":false,"statusMessage":"","isUnBanUserModalOpen":false},"ban":{"isBanning":false,"isSucceeded":false,"statusMessage":"","isBanUserModalOpen":false}},"submitArticle":{"isPosting":false,"isPosted":false,"statusMessage":"","timer":0,"articleSlug":"","isModalOpen":false,"currentStep":"form","slug":"","externalUrl":""},"rss":{"isCreating":false,"isSucceeded":false,"statusMessage":""},"rssFeedList":{"isFetched":false,"isFetching":false,"statusMessage":"","list":[],"disconnectModal":{"isFetched":true,"isFetching":false,"isOpen":false,"rssFeedIdToDisconnect":null,"statusMessage":""}},"setAsHomePage":{"isSetAsHomePageModalOpen":false},"analytics":{}},"initialProps":{"asPath":"/community/tutorials/demystifying-crucial-statistics-python"}},"pathname":"/community/tutorial","query":{"slug":"demystifying-crucial-statistics-python"},"buildId":"f80eab32-3c58-4b52-a08c-d754687b7a03","buildStats":{"app.js":{"hash":"b782294ddb8d954b4c94ee4c23476b23"}},"assetPrefix":"/community","nextExport":false,"err":null,"chunks":[]}
          module={}
          __NEXT_LOADED_PAGES__ = []
          __NEXT_LOADED_CHUNKS__ = []

          __NEXT_REGISTER_PAGE = function (route, fn) {
            __NEXT_LOADED_PAGES__.push({ route: route, fn: fn })
          }

          __NEXT_REGISTER_CHUNK = function (chunkName, fn) {
            __NEXT_LOADED_CHUNKS__.push({ chunkName: chunkName, fn: fn })
          }
        </script><script async="" id="__NEXT_PAGE__/community/tutorial" type="text/javascript" src="/community/_next/f80eab32-3c58-4b52-a08c-d754687b7a03/page/community/tutorial.js"></script><script async="" id="__NEXT_PAGE__/_error" type="text/javascript" src="/community/_next/f80eab32-3c58-4b52-a08c-d754687b7a03/page/_error.js"></script><script type="text/javascript" src="/community/_next/b782294ddb8d954b4c94ee4c23476b23/app.js" async=""></script></body></html>