<!DOCTYPE html><html><head><meta charSet="utf-8" class="next-head"/><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" class="next-head"/><meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" class="next-head"/><meta name="google-site-verification" content="NXbTi7gyLQESV4NIskeE9Ka0Am8KjAtzg5gm8g38HbU" class="next-head"/><meta name="keywords" content="linear regression, python" class="next-head"/><meta name="description" content="Learn what formulates a regression problem and how a linear regression algorithm works in Python." class="next-head"/><title class="next-head">Essentials of Linear Regression in Python (article) - DataCamp</title><link rel="canonical" href="https://www.datacamp.com/community/tutorials/essentials-linear-regression-python" class="next-head"/><link rel="author" href="https://plus.google.com/u/0/+Datacamp/" class="next-head"/><link rel="shortcut icon" type="image/x-icon" href="https://cdn.datacamp.com/main-app/assets/favicon-335cd0394b32102a39221d79e5fd7e51078e6d32a0c8aea59676a6869f84e9d8.ico" class="next-head"/><link rel="chrome-webstore-item" href="https://chrome.google.com/webstore/detail/lbbhbkehmgbndgfdbncbmikooblghdbi" class="next-head"/><meta property="og:title" content="Essentials of Linear Regression in Python" class="next-head"/><meta property="og:image" content="https://s3.amazonaws.com/datacamp-community-prod/social-share-tutorials.jpg" class="next-head"/><meta property="og:url" content="https://www.datacamp.com/community/tutorials/essentials-linear-regression-python" class="next-head"/><meta property="og:type" content="article" class="next-head"/><meta property="og:published_time" content="2018-10-31T16:00:00.000Z" class="next-head"/><meta property="og:author" content="Sayak  Paul" class="next-head"/><meta property="og:description" content="Learn what formulates a regression problem and how a linear regression algorithm works in Python." class="next-head"/><meta property="og:site_name" content="DataCamp Community" class="next-head"/><meta name="twitter:title" content="Essentials of Linear Regression in Python" class="next-head"/><meta name="twitter:description" content="Learn what formulates a regression problem and how a linear regression algorithm works in Python." class="next-head"/><meta name="twitter:card" content="summary" class="next-head"/><meta name="twitter:site" content="@DataCamp" class="next-head"/><meta name="twitter:creator" content="@DataCamp" class="next-head"/><meta name="twitter:domain" content="www.datacamp.com" class="next-head"/><meta name="twitter:image" content="https://s3.amazonaws.com/datacamp-community-prod/social-share-tutorials.jpg" class="next-head"/><meta name="twitter:image:width" content="1200" class="next-head"/><meta name="twitter:image:height" content="628" class="next-head"/><meta name="twitter:image:alt" content="Essentials of Linear Regression in Python" class="next-head"/><meta name="article:publisher" content="https://www.facebook.com/DataCamp-726282547396228" class="next-head"/><meta name="fb:app_id" content="726282547396228" class="next-head"/><script class="next-head">
      (function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-TGGWB2P');
    </script><link rel="preload" href="/community/_next/f80eab32-3c58-4b52-a08c-d754687b7a03/page/community/tutorial.js" as="script"/><link rel="preload" href="/community/_next/f80eab32-3c58-4b52-a08c-d754687b7a03/page/_error.js" as="script"/><link rel="preload" href="/community/_next/b782294ddb8d954b4c94ee4c23476b23/app.js" as="script"/><style id="__jsx-396080995">.Logo.jsx-396080995{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;}
.Logo__image.jsx-396080995{display:block;width:122px;height:28px;margin-left:8px;}
.Logo__image.jsx-396080995 svg{fill:#FFFFFF;}
@media (min-width:800px){.Logo.jsx-396080995{height:59px;}.Logo__image.jsx-396080995{margin:17px auto 0;}}</style><style id="__jsx-499055511">.SidebarMenu.jsx-499055511{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;position:fixed;z-index:300;width:100vw;height:50px;background-image: linear-gradient(207deg, #2388B0, #33AACC);}
.Layout--banner .SidebarMenu.jsx-499055511{top:55px;}
.icon.jsx-499055511{text-align:right;}
.icon.jsx-499055511 svg{margin-right:9px;width:20px;height:20px;fill:#FFFFFF;}
@media (min-width:800px){.SidebarMenu.jsx-499055511{z-index:200;width:220px;top:0;left:0;background-image:none;}.Layout--banner .SidebarMenu.jsx-499055511{top:80px;}}</style><style id="__jsx-2769082546">.Menu.jsx-2769082546{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;margin-top:50px;}
.Layout--banner .Menu.jsx-2769082546{margin-top:105px;}
.Layout--openMenu .Menu.jsx-2769082546{min-height:calc(100vh - 50px - 134px);}
.Layout--openMenu.Layout--banner .Menu.jsx-2769082546{min-height:calc(100vh - 105px - 134px);}
.section.jsx-2769082546{margin-bottom:20px;}
.section.jsx-2769082546 h5.jsx-2769082546{margin:0;padding-left:17px;font-size:13px;-webkit-letter-spacing:3.3px;-moz-letter-spacing:3.3px;-ms-letter-spacing:3.3px;letter-spacing:3.3px;line-height:36px;text-align:left;text-transform:uppercase;background-color:#195B73;color:#7ECCE2;}
.item.jsx-2769082546{margin-bottom:1px;padding-left:12px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-content:center;-ms-flex-line-pack:center;align-content:center;font-size:15px;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:40px;text-decoration:none;color:#FFFFFF;}
.statusIcon.jsx-2769082546{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;padding:10px;}
.active.jsx-2769082546{background-color:#55AECB;}
a.jsx-2769082546:hover{background-color:#55AECB;}
.image.jsx-2769082546{margin-top:2px;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;width:30px;height:30px;text-align:center;}
.image.jsx-2769082546 svg{fill:#195B73;}
.active.jsx-2769082546 svg,a.jsx-2769082546:hover svg{fill:#FFFFFF;}
.text.jsx-2769082546{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;}
.subMenu.jsx-2769082546{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}
@media (min-width:800px){.Menu.jsx-2769082546{position:fixed;width:220px;margin-top:50px;}.Layout--banner .Menu.jsx-2769082546{margin-top:130px;}.section.jsx-2769082546 h5.jsx-2769082546{padding-left:0;text-align:center;}}</style><style id="__jsx-1844558338">.Button.jsx-1844558338{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;height:33px;margin:auto 5px;padding:0 15px;font-size:13px;font-weight:bold;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;white-space:nowrap;color:#3A3A3A;border:1px solid transparent;border-radius:4px;background-color:transparent;cursor:pointer;outline:none;}
.Button.jsx-1844558338::before,.Button.jsx-1844558338::after{content:'';-webkit-flex:1 0 auto;-ms-flex:1 0 auto;flex:1 0 auto;}
.icon.jsx-1844558338{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-height:20px;}
.icon.jsx-1844558338 svg{-webkit-flex:1 1 0;-ms-flex:1 1 0;flex:1 1 0;height:20px;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;fill:#33AACC;}
.greyIcon.jsx-1844558338 .icon.jsx-1844558338 svg{min-width:16px;min-height:16px;fill:#3A3A3A;}
.same.jsx-1844558338 .icon.jsx-1844558338{min-width:13px;height:13px;margin-right:5px;}
.same.jsx-1844558338 .icon.jsx-1844558338 svg{height:13px;}
.Button.jsx-1844558338:disabled,.Button.jsx-1844558338:hover.jsx-1844558338:disabled{color:#D1D3D8;background-color:#E6EAEB;}
.primary.jsx-1844558338{background-color:#FFC844;}
.primary.jsx-1844558338:hover{background-color:#FBE28D;}
.secondary.jsx-1844558338{color:#FFFFFF;background-color:#33AACC;}
.secondary.jsx-1844558338:hover{background-color:#7ECCE2;}
.red.jsx-1844558338{color:#FFFFFF;background-color:#FE5C5C;}
.green.jsx-1844558338{height:35px;color:#FFFFFF;background-color:#FFFFFF;}
.green.jsx-1844558338 .icon.jsx-1844558338 svg{fill:#36D57D;width:35px;height:35px;}
.grey.jsx-1844558338{color:#3D4251;background-color:#D1D3D8;}
.grey.jsx-1844558338:hover{color:#3D4251;background-color:#E6EAEB;}
.big.jsx-1844558338{font-size:15px;height:42px;}
.extra.jsx-1844558338{font-size:17px;height:45px;}
.border.jsx-1844558338{border:1px solid #E3E7E8;}
.border.jsx-1844558338:hover{border:1px solid #33AACC;}
.seeAll.jsx-1844558338{border:1px solid #33AACC;}
.seeAll.jsx-1844558338:hover{border:1px solid #FFC844;}
.iconButton.jsx-1844558338:hover{color:#33AACC;}
.minWidth.jsx-1844558338{min-width:85px;}
.noPadding.jsx-1844558338{padding:0;}
@media (min-width:800px){.icon.jsx-1844558338{min-width:13px;height:13px;margin-right:5px;}.icon.jsx-1844558338 svg{height:13px;}.big.jsx-1844558338 .icon.jsx-1844558338{min-width:15px;height:15px;}.big.jsx-1844558338 svg{height:15px;}.extra.jsx-1844558338 .icon.jsx-1844558338,.extraIcon.jsx-1844558338{min-width:17px;height:17px;}.extra.jsx-1844558338 svg,.extraIcon.jsx-1844558338 svg{height:17px;}.green.jsx-1844558338{padding:0 15px;color:#FFFFFF;background-color:#36D57D;}.green.jsx-1844558338 .icon.jsx-1844558338 svg{width:13px;height:13px;fill:#FFFFFF;}.forcePadding.jsx-1844558338{padding:0 15px;}}</style><style id="__jsx-3863678361">.ActionBarSearch.jsx-3863678361{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;}</style><style id="__jsx-728636942">.SubmitAnArticleButton.jsx-728636942{margin-left:5px;}
.mobileButton.jsx-728636942{display:block !important;}
.mobileButton.jsx-728636942 svg{fill:#36D57D;width:35px;height:35px;}
.SubmitAnArticleButton.jsx-728636942 .desktopButton{display:none !important;}
@media (min-width:800px){.mobileButton.jsx-728636942{display:none !important;}.SubmitAnArticleButton.jsx-728636942 .desktopButton{display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;}}</style><style id="__jsx-3196442269">.ActionBarAuth.jsx-3196442269{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;}
.wrapper.jsx-3196442269{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;cursor:pointer;}
.name.jsx-3196442269{margin-right:9px;font-size:13px;font-weight:bold;color:#3D4251;text-decoration:none;}
.name.jsx-3196442269:hover{color:#33AACC;}
.logout.jsx-3196442269{font-size:15px;padding:10px;color:#3D4251;display:inline-block;min-width:100px;}
.logout.jsx-3196442269:hover{background-color:#F0F4F5;border-bottom:solid 1px #E3E7E8;}
.menuList.jsx-3196442269 a.jsx-3196442269{display:block;}</style><style id="__jsx-2159026896">.ActionBar.jsx-2159026896{height:50px;margin-top:50px;padding:0 5px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;background-color:#FFFFFF;border-bottom:1px solid #E3E7E8;}
.authBlock.jsx-2159026896{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-fles-direction:row;-ms-fles-direction:row;fles-direction:row;}
.Page.content .ActionBar{margin-bottom:10px;}
@media (min-width:800px){.ActionBar.jsx-2159026896{width:calc(100% - 220px);height:50px;margin-top:0;margin-bottom:0;padding:0 25px;position:fixed;z-index:300;}}</style><style id="__jsx-3889859319">.Title.jsx-3889859319{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
.Title.jsx-3889859319 .icon.jsx-3889859319{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;min-width:18px;height:18px;}
.icon.jsx-3889859319 svg{-webkit-flex:1 1 0;-ms-flex:1 1 0;flex:1 1 0;height:18px;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;fill:#33AACC;}
.Title.jsx-3889859319 .h1.jsx-3889859319,.Title.jsx-3889859319 h1.jsx-3889859319{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;margin:auto 0 auto 9px;font-size:22px;text-transform:capitalize;}
.Title.jsx-3889859319 .status.jsx-3889859319{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;padding:10px;}</style><style id="__jsx-1514242801">.TitleBar.jsx-1514242801{height:50px;padding:0 5px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-wrap:wrap;-ms-flex-wrap:wrap;flex-wrap:wrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;line-height:50px;background-color:#FFFFFF;border-bottom:1px solid #E3E7E8;margin-bottom:65px;}
.filter.jsx-1514242801{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;height:48px;}
.action.jsx-1514242801{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;line-height:normal;}
.title.jsx-1514242801{height:65px;line-height:65px;-webkit-flex:0 0 100%;-ms-flex:0 0 100%;flex:0 0 100%;-webkit-order:1;-ms-flex-order:1;order:1;text-align:center;}
h1.jsx-1514242801{margin:0 0;}
.Page.content .TitleBar{display:none;}
@media (min-width:800px){.TitleBar.jsx-1514242801{height:50px;padding:0 25px;display:-webkit-box !important;display:-webkit-flex !important;display:-ms-flexbox !important;display:flex !important;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-flex-wrap:nowrap;-ms-flex-wrap:nowrap;flex-wrap:nowrap;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin-bottom:29px;margin-top:50px;background-color:#FFFFFF;border-bottom:1px solid #E3E7E8;}.filter.jsx-1514242801{-webkit-flex:0 0 33%;-ms-flex:0 0 33%;flex:0 0 33%;line-height:normal;}.action.jsx-1514242801{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:end;-webkit-justify-content:flex-end;-ms-flex-pack:end;justify-content:flex-end;line-height:normal;-webkit-flex:0 0 33%;-ms-flex:0 0 33%;flex:0 0 33%;}.action.jsx-1514242801 a{line-height:0;}.title.jsx-1514242801{-webkit-order:0;-ms-flex-order:0;order:0;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;text-align:center;}h1.jsx-1514242801{margin:0 0;}}</style><style id="__jsx-3293774837">.CommentCounter.jsx-3293774837{width:54px;height:54px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;border-radius:4px;border:1px solid #E6EAEB;background-color:#F0F4F5;cursor:pointer;}
.CommentCounter.jsx-3293774837 .icon.jsx-3293774837{font-size:13px;line-height:0;color:#33AACC;}
.CommentCounter.jsx-3293774837 .icon.jsx-3293774837 svg{width:16px;height:16px;fill:#33AACC;}
.CommentCounter.jsx-3293774837 .count.jsx-3293774837{font-size:13px;font-weight:bold;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;color:#686F75;}
.CommentCounter.jsx-3293774837:hover{background-color:#FFFFFF;}</style><style id="__jsx-1972554161">.Upvote.jsx-1972554161{position:relative;width:54px;height:54px;overflow:hidden;border-radius:4px;border:1px solid #E6EAEB;background-color:#F0F4F5;cursor:pointer;}
.Upvote.news.jsx-1972554161{width:45px;height:35px;border:none;background-color:transparent;}
.Upvote.comment.jsx-1972554161{width:45px;height:25px;border:none;background-color:transparent;}
.Upvote.jsx-1972554161>div.jsx-1972554161{position:absolute;top:0;-webkit-transition:all 0.15s ease-in-out;transition:all 0.15s ease-in-out;}
.Upvote.upvoted.jsx-1972554161>div.jsx-1972554161{top:-54px;}
.Upvote.upvoted.news.jsx-1972554161>div.jsx-1972554161{top:-35px;}
.Upvote.upvoted.comment.jsx-1972554161>div.jsx-1972554161{top:-25px;}
.Upvote.jsx-1972554161>div.jsx-1972554161>div.jsx-1972554161{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;width:54px;height:54px;}
.Upvote.news.jsx-1972554161>div.jsx-1972554161>div.jsx-1972554161{width:45px;height:35px;}
.Upvote.comment.jsx-1972554161>div.jsx-1972554161>div.jsx-1972554161{width:45px;height:25px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}
.Upvote.jsx-1972554161 .icon.jsx-1972554161{font-size:13px;line-height:0;color:#33AACC;}
.Upvote.comment.jsx-1972554161 .icon.jsx-1972554161{padding-right:5px;}
.Upvote.comment.jsx-1972554161 .count.jsx-1972554161{padding-bottom:1px;}
.Upvote.jsx-1972554161 .icon.jsx-1972554161 svg{width:12px;height:12px;fill:#33AACC;}
.Upvote.jsx-1972554161 .count.jsx-1972554161{font-size:13px;font-weight:bold;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;color:#686F75;}
.Upvote.jsx-1972554161:hover{background-color:#FFFFFF;}
.Upvote.news.jsx-1972554161:hover,.Upvote.comment.jsx-1972554161:hover{background-color:#EBF4F7;}
.Upvote.news.jsx-1972554161:hover .count.jsx-1972554161,.Upvote.comment.jsx-1972554161:hover .count.jsx-1972554161{color:#33AACC;}
.Upvote.jsx-1972554161 .voted.jsx-1972554161{background-color:#33AACC;border-color:#33AACC;}
.Upvote.news.jsx-1972554161 .voted.jsx-1972554161,.Upvote.comment.jsx-1972554161 .voted.jsx-1972554161{background-color:#FFFFFF;border:none;}
.Upvote.jsx-1972554161 .voted.jsx-1972554161 .icon.jsx-1972554161 svg{fill:#195B73;}
.Upvote.news.jsx-1972554161 .voted.jsx-1972554161 .icon.jsx-1972554161 svg,.Upvote.comment.jsx-1972554161 .voted.jsx-1972554161 .icon.jsx-1972554161 svg{fill:#36D57D;}
.Upvote.jsx-1972554161 .voted.jsx-1972554161 .count.jsx-1972554161{color:#FFFFFF;}
.Upvote.news.jsx-1972554161 .voted.jsx-1972554161 .count.jsx-1972554161,.Upvote.comment.jsx-1972554161 .voted.jsx-1972554161 .count.jsx-1972554161{color:#36D57D;}
@media (min-width:800px){.Upvote.news.jsx-1972554161{height:45px;}.Upvote.comment.jsx-1972554161{height:25px;}.Upvote.upvoted.news.jsx-1972554161>div.jsx-1972554161{top:-45px;}.Upvote.upvoted.comment.jsx-1972554161>div.jsx-1972554161{top:-25px;}.Upvote.news.jsx-1972554161>div.jsx-1972554161>div.jsx-1972554161{height:45px;}.Upvote.comment.jsx-1972554161>div.jsx-1972554161>div.jsx-1972554161{height:25px;}}</style><style id="__jsx-494086174">.Social.jsx-494086174{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;}
.icons.jsx-494086174{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
.icon.jsx-494086174{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;width:28px;height:28px;border:1px solid #E3E7E8;background-color:#FFFFFF;border-radius:50%;}
.icon.jsx-494086174:hover{background-color:#F0F4F5;}
.centerIcon.jsx-494086174{margin:0 10px;}
.icon.jsx-494086174 svg{fill:#686F75;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;}
@media (min-width:800px){.Social.jsx-494086174{margin-top:18px;}.vertical.jsx-494086174{margin-top:10px;}.vertical.jsx-494086174 .icons.jsx-494086174{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.vertical.jsx-494086174 .centerIcon.jsx-494086174{margin:10px 0;}}</style><style id="__jsx-3208234818">.Avatar.jsx-3208234818{display:inline-block;background-size:cover;background-color:#E6EAEB;background-repeat:no-repeat;}</style><style id="__jsx-566588255">.Author.jsx-566588255 a.jsx-566588255{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;cursor:pointer;}
.info.jsx-566588255{margin-left:9px;white-space:nowrap;}
.mirrored.jsx-566588255 .info.jsx-566588255{margin:0 9px 0 0;-webkit-order:-1;-ms-flex-order:-1;order:-1;}
.name.jsx-566588255{font-size:13px;font-weight:bold;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;color:#3D4251;text-decoration:none;}
.name.jsx-566588255:hover{color:#33AACC;}
.date.jsx-566588255{font-size:11px;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;color:#686F75;}</style><style id="__jsx-1764811326">.Tag.jsx-1764811326{display:inline-block;border-radius:4px;background-color:#F0F4F5;border:solid 1px #E6EAEB;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;color:#686F75;cursor:default;}
.title.jsx-1764811326{line-height:20px;padding:0 8px;font-size:11px;text-transform:uppercase;}
.Tag.mustRead.jsx-1764811326{background-color:#AD86CE;border-color:#AD86CE;color:#FFFFFF;font-weight:bold;}
.Tag.jsx-1764811326:hover{background-color:#FFFFFF;}
.Tag.mustRead.jsx-1764811326:hover{background-color:#CEABEC;border-color:#CEABEC;}</style><style id="__jsx-1022557955">.more.jsx-1022557955{font-size:11px;cursor:default;color:#686F75;}
.more.jsx-1022557955:hover{text-decoration:underline;}</style><style id="__jsx-422934526">.tooltipInner .Tag{margin:4px;}
.rc-tooltip{position:absolute;z-index:200;display:block;visibility:visible;line-height:1.5;font-size:12px;border-radius:4px;}
.rc-tooltip-hidden{display:none;}
.rc-tooltip-inner{padding:12px;color:#333333;text-align:left;text-decoration:none;background-color:#ffffff;border-radius:4px;min-height:34px;border:1px solid #e3e7e8;}
.rc-tooltip-arrow,.rc-tooltip-arrow-inner{position:absolute;width:0;height:0;border-color:transparent;border-style:solid;}
.rc-tooltip-placement-top .rc-tooltip-arrow,.rc-tooltip-placement-topLeft .rc-tooltip-arrow,.rc-tooltip-placement-topRight .rc-tooltip-arrow{bottom:-5px;margin-left:-6px;border-width:6px 6px 0;border-top-color:#e3e7e8;}
.rc-tooltip-placement-top .rc-tooltip-arrow-inner,.rc-tooltip-placement-topLeft .rc-tooltip-arrow-inner,.rc-tooltip-placement-topRight .rc-tooltip-arrow-inner{bottom:1px;margin-left:-6px;border-width:6px 6px 0;border-top-color:#ffffff;}
.rc-tooltip-placement-top .rc-tooltip-arrow{left:50%;}
.rc-tooltip-placement-topLeft .rc-tooltip-arrow{left:15%;}
.rc-tooltip-placement-topRight .rc-tooltip-arrow{right:15%;}
.rc-tooltip-placement-right .rc-tooltip-arrow,.rc-tooltip-placement-rightTop .rc-tooltip-arrow,.rc-tooltip-placement-rightBottom .rc-tooltip-arrow{left:-5px;margin-top:-6px;border-width:6px 6px 6px 0;border-right-color:#e3e7e8;}
.rc-tooltip-placement-right .rc-tooltip-arrow-inner,.rc-tooltip-placement-rightTop .rc-tooltip-arrow-inner,.rc-tooltip-placement-rightBottom .rc-tooltip-arrow-inner{left:1px;margin-top:-6px;border-width:6px 6px 6px 0;border-right-color:#ffffff;}
.rc-tooltip-placement-right .rc-tooltip-arrow{top:50%;}
.rc-tooltip-placement-rightTop .rc-tooltip-arrow{top:15%;margin-top:0;}
.rc-tooltip-placement-rightBottom .rc-tooltip-arrow{bottom:15%;}
.rc-tooltip-placement-left .rc-tooltip-arrow,.rc-tooltip-placement-leftTop .rc-tooltip-arrow,.rc-tooltip-placement-leftBottom .rc-tooltip-arrow{right:-5px;margin-top:-6px;border-width:6px 0 6px 6px;border-left-color:#e3e7e8;}
.rc-tooltip-placement-left .rc-tooltip-arrow-inner,.rc-tooltip-placement-leftTop .rc-tooltip-arrow-inner,.rc-tooltip-placement-leftBottom .rc-tooltip-arrow-inner{right:1px;margin-top:-6px;border-width:6px 0 6px 6px;border-left-color:#ffffff;}
.rc-tooltip-placement-left .rc-tooltip-arrow{top:50%;}
.rc-tooltip-placement-leftTop .rc-tooltip-arrow{top:15%;margin-top:0;}
.rc-tooltip-placement-leftBottom .rc-tooltip-arrow{bottom:15%;}
.rc-tooltip-placement-bottom .rc-tooltip-arrow,.rc-tooltip-placement-bottomLeft .rc-tooltip-arrow,.rc-tooltip-placement-bottomRight .rc-tooltip-arrow{top:-5px;margin-left:-6px;border-width:0 6px 6px;border-bottom-color:#e3e7e8;}
.rc-tooltip-placement-bottom .rc-tooltip-arrow-inner,.rc-tooltip-placement-bottomLeft .rc-tooltip-arrow-inner,.rc-tooltip-placement-bottomRight .rc-tooltip-arrow-inner{top:1px;margin-left:-6px;border-width:0 6px 6px;border-bottom-color:#ffffff;}
.rc-tooltip-placement-bottom .rc-tooltip-arrow{left:50%;}
.rc-tooltip-placement-bottomLeft .rc-tooltip-arrow{left:15%;}
.rc-tooltip-placement-bottomRight .rc-tooltip-arrow{right:15%;}
.rc-tooltip.rc-tooltip-zoom-enter,.rc-tooltip.rc-tooltip-zoom-leave{display:block;}
.rc-tooltip-zoom-enter,.rc-tooltip-zoom-appear{opacity:0;-webkit-animation-duration:0.3s;-webkit-animation-duration:0.3s;animation-duration:0.3s;-webkit-animation-fill-mode:both;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-timing-function:cubic-bezier(0.18,0.89,0.32,1.28);-webkit-animation-timing-function:cubic-bezier(0.18,0.89,0.32,1.28);animation-timing-function:cubic-bezier(0.18,0.89,0.32,1.28);-webkit-animation-play-state:paused;-webkit-animation-play-state:paused;animation-play-state:paused;}
.rc-tooltip-zoom-leave{-webkit-animation-duration:0.3s;-webkit-animation-duration:0.3s;animation-duration:0.3s;-webkit-animation-fill-mode:both;-webkit-animation-fill-mode:both;animation-fill-mode:both;-webkit-animation-timing-function:cubic-bezier(0.6,-0.3,0.74,0.05);-webkit-animation-timing-function:cubic-bezier(0.6,-0.3,0.74,0.05);animation-timing-function:cubic-bezier(0.6,-0.3,0.74,0.05);-webkit-animation-play-state:paused;-webkit-animation-play-state:paused;animation-play-state:paused;}
.rc-tooltip-zoom-enter.rc-tooltip-zoom-enter-active,.rc-tooltip-zoom-appear.rc-tooltip-zoom-appear-active{-webkit-animation-name:rcToolTipZoomIn;-webkit-animation-name:rcToolTipZoomIn;animation-name:rcToolTipZoomIn;-webkit-animation-play-state:running;-webkit-animation-play-state:running;animation-play-state:running;}
.rc-tooltip-zoom-leave.rc-tooltip-zoom-leave-active{-webkit-animation-name:rcToolTipZoomOut;-webkit-animation-name:rcToolTipZoomOut;animation-name:rcToolTipZoomOut;-webkit-animation-play-state:running;-webkit-animation-play-state:running;animation-play-state:running;}
@-webkit-keyframes rcToolTipZoomIn{0%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}100%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}}
@-webkit-keyframes rcToolTipZoomIn{0%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}100%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}}
@keyframes rcToolTipZoomIn{0%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}100%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}}
@-webkit-keyframes rcToolTipZoomOut{0%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}100%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}}
@-webkit-keyframes rcToolTipZoomOut{0%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}100%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}}
@keyframes rcToolTipZoomOut{0%{opacity:1;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(1,1);-webkit-transform:scale(1,1);-ms-transform:scale(1,1);transform:scale(1,1);}100%{opacity:0;-webkit-transform-origin:50% 50%;-webkit-transform-origin:50% 50%;-ms-transform-origin:50% 50%;transform-origin:50% 50%;-webkit-transform:scale(0,0);-webkit-transform:scale(0,0);-ms-transform:scale(0,0);transform:scale(0,0);}}</style><style id="__jsx-2792531181">.TagLine.jsx-2792531181{display:inline-block;white-space:nowrap;}
.TagLine.jsx-2792531181>.Tag{margin-right:10px;}
.more.jsx-2792531181{font-size:11px;}</style><style id="__jsx-1091791248">.markdown{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:flex-start;-webkit-box-align:flex-start;-ms-flex-align:flex-start;align-items:flex-start;font-family:'Lora',serif;font-size:16px;padding:1.5em;}
.markdown [data-datacamp-exercise]{overflow:hidden;}
.markdown>div{width:100%;}
.markdown div{outline:none;}
.markdown hr{border:0;border-bottom:1px solid #E6EAEB;margin:3em 0;}
.markdown div[data-type="mathjax"]{margin:1.5em 0;}
.markdown p{font-family:'Lora',serif;font-size:1em;line-height:1.8em;color:#3D4251;}
.markdown .powered-by-datacamp+p{margin-top:1.5em !important;}
.markdown p+p,.markdown p+img,.markdown p+div,.markdown p+table,.markdown p+ol,.markdown p+ul,.markdown p+nav,.markdown div+p,.markdown p+iframe,.markdown iframe+p,.markdown pre+img,.markdown pre+p{margin-top:1.5em !important;}
.markdown h2{font-family:'Lato',sans-serif;font-size:1.5em;font-weight:700;color:#3D4251;line-height:1.3em;margin:1.5em 0 0.5em;}
.markdown h3{font-family:'Lato',sans-serif;font-size:1.1em;font-weight:700;color:#3D4251;line-height:1.2em;margin:1.5em 0 0.5em;}
.markdown h4{font-family:'Lato',sans-serif;font-size:1em;font-weight:700;color:#3D4251;line-height:1.2em;margin:1.5em 0 0.5em;}
.markdown .videoWrapper{position:relative;padding-bottom:47.25%;padding-top:25px;height:0;margin-bottom:1.5em;}
.markdown .videoWrapper iframe{position:absolute;top:0;left:0;width:100%;height:100%;}
.markdown p code,.markdown li code{display:inline-block;padding:0 5px;border-radius:4px;font-family:'Roboto Mono',monospace;font-size:0.9em;line-height:1.6em;color:#3D4251;background-color:#E6EAEB;}
.markdown a code{color:#33AACC;}
.markdown pre{padding:1em 1.5em;font-family:'Roboto Mono',monospace;font-size:0.9em;background-color:#002B36 !important;border-radius:4px;overflow-x:auto;-webkit-overflow-scrolling:touch;}
.markdown pre code{padding:0;font-size:0.9em;line-height:2em;background-color:#002B36;overflow-x:visible;-webkit-overflow-scrolling:touch;}
.markdown img{display:block;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;margin:auto;max-width:100%;height:auto !important;}
.markdown img+p{margin-top:1.5em !important;}
.markdown nav+p,.markdown nav+div{margin-top:1.5em !important;}
.markdown iframe{width:100%;}
.markdown ul,.markdown ol{font-family:'Lora',serif;color:#3D4251;padding:0 0 0 1em;}
.markdown ul+p,.markdown ol+p,.markdown ul+div,.markdown ol+div,.markdown ul+pre,.markdown ol+pre{margin-top:1.5em !important;}
.markdown div.datacamp-exercise ul,.markdown div.datacamp-exercise ol{background-color:initial;margin:initial;padding:initial;width:initial;list-style-position:initial;line-height:initial;}
.markdown ul.oneliner,.markdown ol.oneliner{padding:0;list-style-position:inside;line-height:1.5em;}
.markdown li{line-height:1.8em;}
.markdown li+li{margin-top:1em;}
.markdown div.datacamp-exercise li{padding-left:initial;background-color:initial;font-size:initial;font-weight:initial;line-height:initial;}
.markdown li p{margin:0;font-size:1em;line-height:1.8em;}
.markdown ul.oneliner li,.markdown ol.oneliner li{padding-left:0;white-space:nowrap;text-overflow:ellipsis;overflow:hidden;}
.markdown ul ul{margin:0;list-style:circle;}
.markdown ol ol{margin:0;}
.markdown ol ul,.markdown ul ol{margin:0;}
.markdown a{font-weight:400;text-decoration:none;color:#33AACC;}
.markdown a:hover{text-decoration:underline;}
.markdown div.datacamp-exercise a{font-weight:initial;text-decoration:initial;}
.markdown div.datacamp-exercise a:hover{text-decoration:initial;}
.markdown div.datacamp-exercise li+li{margin-top:unset;}
.markdown blockquote{margin:1.5em 0;font-family:'Lato',sans-serif;color:#3D4251;font-weight:300;font-size:1.5em;font-style:italic;line-height:2em;}
.markdown blockquote::before{display:block;margin-bottom:15px;width:35px;height:35px;font-family:'Lora',serif;font-size:36px;font-weight:bold;font-style:normal;line-height:60px;text-align:center;color:#33AACC;content:'“';border:1px solid #E3E7E8;border-radius:50%;}
.markdown table{width:100% !important;border:1px solid #E3E7E8;border-radius:4px;overflow:hidden;border-collapse:separate;border-spacing:0;}
.markdown table th,.markdown table td{padding:0.75em;}
.markdown table tr th,.markdown table tr td{border:1px solid #E3E7E8;vertical-align:middle;}
.markdown table thead tr th{font-family:'Lato',sans-serif;background-color:#F0F4F5;}
.markdown table tr:nth-child(even){background-color:#F0F4F5;}
.markdown table+p{margin-top:1.5em !important;}
.markdown table+img{margin-top:1.5em !important;}
.markdown table+div{margin-top:1.5em !important;}
.markdown .dcl-content--tab-body{margin-top:0 !important;}
@media (min-width:800px){.markdown{font-size:20px;padding:0;}.markdown h2,.markdown h3,.markdown h4{margin:1.5em 0 0.5em;}.markdown p{margin:0;}.markdown li p{font-size:inherit;line-height:inherit;}.markdown ul+p,.markdown ol+p,.markdown ul+div,.markdown ol+div,.markdown ul+pre,.markdown ol+pre{margin-top:1.5em !important;}.markdown blockquote{position:relative;margin:50px 55px;}.markdown blockquote::before{position:absolute;left:-55px;}}
.output_wrapper{overflow-x:auto;-webkit-overflow-scrolling:touch;}</style><style id="__jsx-3956319705">.PostAComment.jsx-3956319705{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;padding:0 0 60px;}</style><style id="__jsx-2506565400">.SidebarSocial.jsx-2506565400{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;font-size:11px;font-weight:bold;}
.rss.jsx-2506565400{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;line-height:50px;}
.rss.jsx-2506565400 svg{padding-right:7px;fill:#FFC844;}
.rss.jsx-2506565400 a.jsx-2506565400{text-decoration:none;color:#FFFFFF;}
.rss.jsx-2506565400 a.jsx-2506565400:hover{text-decoration:none;color:#FFC844;}
.icons.jsx-2506565400{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
.icon.jsx-2506565400{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;margin:0 7px;width:30px;height:30px;background-color:#195B73;border-radius:50%;}
.icon.jsx-2506565400 svg{fill:#7ECCE2;-webkit-align-self:center;-ms-flex-item-align:center;align-self:center;-webkit-flex:1 1 0;-ms-flex:1 1 0;flex:1 1 0;}
.icon.jsx-2506565400:hover svg{fill:#FFFFFF;}
.menu.jsx-2506565400{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:center;-webkit-justify-content:center;-ms-flex-pack:center;justify-content:center;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;line-height:44px;}
.menuItem.jsx-2506565400{-webkit-flex:0 0 auto;-ms-flex:0 0 auto;flex:0 0 auto;padding:5px;text-decoration:none;color:#195B73;}
.menuItem--active.jsx-2506565400,a.jsx-2506565400:hover{color:#F0F4F5;}
@media (min-width:800px) and (min-height:585px){.SidebarSocial.jsx-2506565400{width:220px;position:fixed;bottom:0;left:0;}}</style><style id="__jsx-879378290">.BottomBar.jsx-879378290{position:fixed;bottom:0;width:100vw;padding:15px;z-index:300;background-color:#FFFFFF;box-shadow:0 -2px 26px 0 rgba(168,168,168,0.5);}
.BottomBar.editor.jsx-879378290{padding-top:5px;}
.barView.jsx-879378290{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
.editor.jsx-879378290 .barView.jsx-879378290{display:none;}
.blueBar.jsx-879378290{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;height:50px;margin-left:15px;padding:0 15px;color:#33AACC;line-height:50px;border-radius:4px;background-color:#ebf4f7;}
.editorView.jsx-879378290{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;}
.editorView.jsx-879378290 .avatar{display:none;}
.bar.jsx-879378290 .editorView.jsx-879378290{display:none;}
.hideBar.jsx-879378290{display:none;}
@media (min-width:800px){.BottomBar.jsx-879378290{width:calc(100% - 220px);margin-left:220px;}.editorView.jsx-879378290 .avatar{display:block;}}</style><style id="__jsx-1028385822">.Layout.jsx-1028385822{-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;}
.Layout--openMenu.jsx-1028385822{min-height:100vh;}
.Main.jsx-1028385822{min-height:100vh;-webkit-flex:1 1 auto;-ms-flex:1 1 auto;flex:1 1 auto;background-color:#F0F4F5;}
.Layout--banner.jsx-1028385822 .Main.jsx-1028385822{margin-top:55px;min-height:calc(100vh - 55px);}
.Layout.bar.jsx-1028385822:not(.Layout--openMenu) .SidebarSocial{margin-bottom:90px;}
.Layout.editor.jsx-1028385822:not(.Layout--openMenu) .SidebarSocial{margin-bottom:300px;}
@media (min-width:800px){.Main.jsx-1028385822{margin-left:220px;}.Layout--banner.jsx-1028385822 .Main.jsx-1028385822{margin-top:80px;min-height:calc(100vh - 80px);}.Layout.bar.jsx-1028385822:not(.Layout--openMenu) .SidebarSocial,.Layout.editor.jsx-1028385822:not(.Layout--openMenu) .SidebarSocial{margin-bottom:0;}.Layout.bar.jsx-1028385822 .Main > div:last-child{margin-bottom:90px;}.Layout.editor.jsx-1028385822 .Main > div:last-child{margin-bottom:300px;}}</style><style id="__jsx-1464850800">.Tutorial.jsx-1464850800{margin:0 0px 30px;padding:20px 0 0;background-color:#FFFFFF;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}
.preface.jsx-1464850800{margin:0 20px;}
.author.jsx-1464850800{margin-bottom:10px;}
h1.jsx-1464850800{margin-top:20px;}
.illustration.jsx-1464850800{margin-bottom:30px;}
.illustration.jsx-1464850800 img.jsx-1464850800{max-width:100%;}
.Tutorial.jsx-1464850800 .social__top .voteAndSocial,.social__bottom.jsx-1464850800 .voteAndSocial.jsx-1464850800{padding:40px 20px 20px;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;-webkit-align-items:flex-end;-webkit-box-align:flex-end;-ms-flex-align:flex-end;align-items:flex-end;}
.Tutorial.jsx-1464850800 .voteAndSocial.jsx-1464850800>div.jsx-1464850800{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}
.Tutorial.jsx-1464850800 .voteAndSocial.jsx-1464850800>div.jsx-1464850800 .CommentCounter{margin-left:10px;}
@media (min-width:800px){.Tutorial.jsx-1464850800{margin:0 auto 30px;padding:30px 100px 100px;max-width:1120px;border-radius:4px;border:1px solid #E3E7E8;}.preface.jsx-1464850800{margin:0;}.Tutorial.jsx-1464850800 .social__top{position:absolute;margin-top:220px;}.Tutorial.jsx-1464850800 .social__top .voteAndSocial{position:absolute;left:-100px;top:0;width:100px;height:auto;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-box-pack:start;-webkit-justify-content:flex-start;-ms-flex-pack:start;justify-content:flex-start;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}.Tutorial.jsx-1464850800 .voteAndSocial.jsx-1464850800>div.jsx-1464850800{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}.Tutorial.jsx-1464850800 .voteAndSocial > div .CommentCounter{margin-left:0;margin-bottom:10px;}}</style><style id="__jsx-2612661084">*{box-sizing:border-box;}
html,body{min-height:100vh;margin:0;padding:0;background-image: linear-gradient(207deg, #2388B0, #33AACC); background-size: 100vw 100vh; background-attachment: fixed; background-repeat: no-repeat;}
body.ReactModal__Body--open{overflow:hidden;}
.ReactModal__Content{width:100%;}
img{margin:auto;}
.mobileOnlyShow{display:block !important;}
.mobileOnlyHide{display:none !important;}
.mobileOnly{display:block !important;}
.desktopOnly{display:none !important;}
@media (min-width:800px){body{background-image: linear-gradient(207deg, #2388B0, #33AACC); background-size: 220px 100vh; background-attachment: fixed; background-repeat: no-repeat;}.ReactModal__Content{width:auto;}.mobileOnlyShow{display:block !important;}.mobileOnlyHide{display:block !important;}.mobileOnly{display:none !important;}.desktopOnly{display:block !important;}}</style><style id="__jsx-63629563">body,input,button,select,textarea{font-family:'Lato',sans-serif;color:#686F75;font-size:15px;}
h1,.h1,h2,h3,h4,h5{font-family:'Lato',sans-serif;}
.pageTitle{font-family:'Lato',sans-serif;font-size:32px;font-weight:bold;line-height:1.3em;margin-bottom:0.5em;}
.pageDescription{font-family:'Lora',serif;font-size:20.8px;line-height:1.5em;margin-bottom:1.4em;color:#3D4251;}
h1,.h1{font-size:29px;color:#3D4251;font-weight:bold;}
h2{font-size:20px;-webkit-letter-spacing:0.3px;-moz-letter-spacing:0.3px;-ms-letter-spacing:0.3px;letter-spacing:0.3px;line-height:1.33;font-weight:bold;margin:18px 0px;color:#3D4251;}
h2.blue{color:#33AACC;}
a{color:#33AACC;text-decoration:none;}
.blocText{font-size:15px;-webkit-letter-spacing:0.2px;-moz-letter-spacing:0.2px;-ms-letter-spacing:0.2px;letter-spacing:0.2px;line-height:1.47;color:#686F75;}
label{display:block;width:100%;margin-bottom:8px;font-size:13px;}
label span{float:right;font-weight:300;}
input,textarea{padding:15px;font-weight:300;color:#3D4251;background-color:#F0F4F5;border:1px solid transparent;border-radius:4px;outline-style:none;}
input:disabled,textarea:disabled{color:#686F75;background-color:#E6EAEB;}
input.error,textarea.error{border:1px solid #FE5C5C;}
input:focus,textarea:focus{border:1px solid #33AACC;-webkit-transition:border 150ms ease-out;transition:border 150ms ease-out;}
input::-webkit-input-placeholder,textarea::-webkit-input-placeholder{color:#33AACC;-webkit-transition:color 150ms ease-out;transition:color 150ms ease-out;}
input::-moz-placeholder,textarea::-moz-placeholder{color:#33AACC;-webkit-transition:color 150ms ease-out;transition:color 150ms ease-out;}
input:-ms-input-placeholder,textarea:-ms-input-placeholder{color:#33AACC;-webkit-transition:color 150ms ease-out;transition:color 150ms ease-out;}
input::placeholder,textarea::placeholder{color:#33AACC;-webkit-transition:color 150ms ease-out;transition:color 150ms ease-out;}
input:focus::-webkit-input-placeholder,textarea:focus::-webkit-input-placeholder{color:transparent;}
input:focus::-moz-placeholder,textarea:focus::-moz-placeholder{color:transparent;}
input:focus:-ms-input-placeholder,textarea:focus:-ms-input-placeholder{color:transparent;}
input:focus::placeholder,textarea:focus::placeholder{color:transparent;}
input.small,textarea.small{margin-bottom:19px;padding:8px 10px;font-size:13px;}
textarea.small{min-height:55px;}
@media (min-width:800px){h1,.h1{font-size:36px;}h2{font-size:32px;}.pageTitle{font-size:40px;}.pageDescription{font-size:26px;}}</style></head><body><div id="__next"><div data-reactroot=""><div class="Page content"><div class="jsx-1028385822 Layout bar"><div class="jsx-499055511 SidebarMenu"><div class="jsx-396080995 Logo"><a href="https://www.datacamp.com" class="jsx-396080995 Logo__image"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1367.47 306.77"><path d="M201.49 130.16c2.64-10.87 2.31-21.09-3.62-35.38 0 0 9.55 3.71 8.56-3.65S194.91 76 187 71.11c-3.48-2.17-12.89-6.79-25.52-9.6l2.36-5.32-.66-.32c-19.59-9.54-42.24-.89-42.47-.8l-.7.27 2.24 5.92a78 78 0 0 0-19 7.09C50 96.68 77 152 93.5 165.17s8.89 40 0 64.31H165c1-4.55.13 2.24 3.46-15.91s16.18-11.41 26.3-12 8.52-7.66 8.4-15.56c4.94-2.22 4.94-4.57 1.64-7.41 4.28-4 3.62-4 1.32-7.83s-.66-5.68-.66-5.68 6.92-1.35 7.9-5c.32-7.89-14.5-19.09-11.87-29.93zm-14.78-2.28l-10.59-3.6c-.56 1.45-6 14.9-13 17.27l-6.68-8.84c-3.21 2.51-10.54 4.72-14.07 3.69v-.32h-.19l.2 24.36c-14.05 1.45-30.84-7.06-38.88-18.77l-8.78 6.67c-10-12.07-11.51-26.62-11.62-35h11.77c0-9.27 3.62-21.34 8.72-28l9.23 6.52c.75-.93 7.72-9.27 17.05-12.28h.14l-6.67-17.64c14.09-4.06 24.44-4 35.7.19a3.87 3.87 0 0 1 1 .48l.18.13-1.99 5.26c.53.23 8 3.59 11.09 6.74l2.49-3.44a42.47 42.47 0 0 1 19 25.34L187 98v-.1a55.32 55.32 0 0 1-.28 29.98z"></path><path d="M141.69 95.69a11.77 11.77 0 1 0 11.76 11.77 11.78 11.78 0 0 0-11.76-11.77zM486.07 147.93a85.48 85.48 0 0 1-5.79 31.92 71.67 71.67 0 0 1-41.54 41.32 89 89 0 0 1-32.79 5.83h-60.24V68.9h60.23a88.33 88.33 0 0 1 32.79 5.85A74 74 0 0 1 464 91a72.76 72.76 0 0 1 16.29 25 85.49 85.49 0 0 1 5.78 31.93zm-30.17 0a73.61 73.61 0 0 0-3.44-23.34 48.4 48.4 0 0 0-9.95-17.49 43.45 43.45 0 0 0-15.74-11 54 54 0 0 0-20.82-3.83h-30.72v111.3h30.72a54 54 0 0 0 20.82-3.83 43.39 43.39 0 0 0 15.74-11 48.37 48.37 0 0 0 9.95-17.49 73.6 73.6 0 0 0 3.44-23.32zM505.36 130.44q19.35-17.71 46.57-17.71a45.54 45.54 0 0 1 17.6 3.22 37.21 37.21 0 0 1 13.12 9 38.4 38.4 0 0 1 8.14 13.72 52.72 52.72 0 0 1 2.79 17.49V227h-12.25a12.38 12.38 0 0 1-5.9-1.15q-2.08-1.15-3.28-4.65l-2.4-8.09a97.29 97.29 0 0 1-8.31 6.72 48.91 48.91 0 0 1-8.42 4.86 45.32 45.32 0 0 1-9.35 3 53.93 53.93 0 0 1-11 1 43.08 43.08 0 0 1-13.12-1.91 28.78 28.78 0 0 1-10.38-5.74 25.73 25.73 0 0 1-6.78-9.51 33.72 33.72 0 0 1-2.4-13.23 26.32 26.32 0 0 1 1.42-8.47 24.76 24.76 0 0 1 4.65-8 38.63 38.63 0 0 1 8.36-7.21 54.43 54.43 0 0 1 12.63-5.9 109.08 109.08 0 0 1 17.44-4.1 174.35 174.35 0 0 1 22.74-1.91v-6.56q0-11.26-4.81-16.67t-13.88-5.41a33 33 0 0 0-10.88 1.53 44.91 44.91 0 0 0-7.6 3.44q-3.28 1.91-6 3.44a11.78 11.78 0 0 1-6 1.53 7.79 7.79 0 0 1-4.81-1.48 12.33 12.33 0 0 1-3.17-3.44zm61.87 48.64a149.08 149.08 0 0 0-19.68 2 52.42 52.42 0 0 0-12.79 3.77 16.8 16.8 0 0 0-6.89 5.36 11.63 11.63 0 0 0-2.08 6.67q0 7.11 4.21 10.17t11 3.06a32 32 0 0 0 14.37-3 42.69 42.69 0 0 0 11.86-9.11zM657.79 228.72q-14.65 0-22.46-8.25t-7.82-22.79V135H616a5.26 5.26 0 0 1-3.72-1.42 5.53 5.53 0 0 1-1.53-4.26v-10.68l18-3 5.68-30.61a6 6 0 0 1 2.08-3.39 6.18 6.18 0 0 1 3.94-1.2h14v35.36h30V135h-30v60.78q0 5.25 2.57 8.2a8.85 8.85 0 0 0 7 3 12.7 12.7 0 0 0 4.21-.6 23 23 0 0 0 3-1.26q1.26-.66 2.24-1.26a3.75 3.75 0 0 1 2-.6 3.1 3.1 0 0 1 2 .6 9.23 9.23 0 0 1 1.64 1.8l8.09 13.12a40.87 40.87 0 0 1-13.55 7.43 50.47 50.47 0 0 1-15.86 2.51zM702.11 130.44q19.35-17.71 46.57-17.71a45.54 45.54 0 0 1 17.6 3.22 37.19 37.19 0 0 1 13.12 9 38.37 38.37 0 0 1 8.14 13.72 52.72 52.72 0 0 1 2.79 17.49V227h-12.25a12.37 12.37 0 0 1-5.9-1.15q-2.08-1.15-3.28-4.65l-2.4-8.09a97.21 97.21 0 0 1-8.31 6.72 49 49 0 0 1-8.42 4.86 45.33 45.33 0 0 1-9.35 3 53.92 53.92 0 0 1-11 1 43.08 43.08 0 0 1-13.12-1.91 28.78 28.78 0 0 1-10.38-5.74 25.74 25.74 0 0 1-6.78-9.51 33.72 33.72 0 0 1-2.4-13.23 26.31 26.31 0 0 1 1.42-8.47 24.76 24.76 0 0 1 4.65-8 38.66 38.66 0 0 1 8.36-7.21 54.45 54.45 0 0 1 12.63-5.9 109.07 109.07 0 0 1 17.44-4.1 174.32 174.32 0 0 1 22.76-1.93v-6.56q0-11.26-4.81-16.67t-13.88-5.41a33 33 0 0 0-10.88 1.53 44.89 44.89 0 0 0-7.6 3.44q-3.28 1.91-6 3.44a11.79 11.79 0 0 1-6 1.53 7.79 7.79 0 0 1-4.81-1.48 12.33 12.33 0 0 1-3.17-3.44zM764 179.09a149.06 149.06 0 0 0-19.68 2 52.43 52.43 0 0 0-12.79 3.77 16.81 16.81 0 0 0-6.89 5.36 11.63 11.63 0 0 0-2.08 6.67q0 7.11 4.21 10.17t11 3.06a32 32 0 0 0 14.37-3A42.68 42.68 0 0 0 764 198zM929.42 189.69a5.87 5.87 0 0 1 4.26 1.86l11.59 12.57a63.64 63.64 0 0 1-23.67 18.26q-14 6.34-33.72 6.34-17.6 0-31.65-6a69.71 69.71 0 0 1-24-16.73A73.19 73.19 0 0 1 817 180.4a96 96 0 0 1-5.3-32.47 90.36 90.36 0 0 1 5.68-32.63 75 75 0 0 1 16-25.52A72.5 72.5 0 0 1 858 73.11a81.77 81.77 0 0 1 31.7-6q17.27 0 30.66 5.68a73.08 73.08 0 0 1 22.83 14.91l-9.84 13.66a9 9 0 0 1-2.24 2.29 6.35 6.35 0 0 1-3.77 1 7.2 7.2 0 0 1-3.39-.93q-1.75-.93-3.83-2.29t-4.81-3a39.67 39.67 0 0 0-6.34-3 51.92 51.92 0 0 0-8.36-2.29 57.87 57.87 0 0 0-11-.93A48.74 48.74 0 0 0 870.23 96a42.16 42.16 0 0 0-15.14 10.93 50.38 50.38 0 0 0-9.84 17.49 73.27 73.27 0 0 0-3.5 23.56 69.25 69.25 0 0 0 3.77 23.72 52.12 52.12 0 0 0 10.22 17.49 43.61 43.61 0 0 0 15.2 10.81 47 47 0 0 0 18.8 3.77 81.89 81.89 0 0 0 10.88-.66 46.17 46.17 0 0 0 9-2.08 39.7 39.7 0 0 0 7.76-3.66 48.48 48.48 0 0 0 7.27-5.52 10.31 10.31 0 0 1 2.29-1.58 5.56 5.56 0 0 1 2.48-.58zM961.6 130.44q19.35-17.71 46.57-17.71a45.54 45.54 0 0 1 17.6 3.22 37.19 37.19 0 0 1 13.12 9 38.37 38.37 0 0 1 8.14 13.72 52.72 52.72 0 0 1 2.79 17.49V227h-12.24a12.37 12.37 0 0 1-5.9-1.15q-2.08-1.15-3.28-4.65l-2.4-8.09a97.21 97.21 0 0 1-8.31 6.72 49 49 0 0 1-8.42 4.86 45.33 45.33 0 0 1-9.35 3 53.92 53.92 0 0 1-11 1 43.08 43.08 0 0 1-13.12-1.91 28.78 28.78 0 0 1-10.38-5.74 25.74 25.74 0 0 1-6.78-9.51 33.73 33.73 0 0 1-2.4-13.23 26.31 26.31 0 0 1 1.42-8.47 24.76 24.76 0 0 1 4.65-8 38.66 38.66 0 0 1 8.36-7.21 54.45 54.45 0 0 1 12.63-5.9 109.07 109.07 0 0 1 17.44-4.1 174.32 174.32 0 0 1 22.74-1.91v-6.56q0-11.26-4.81-16.67t-13.88-5.41a33 33 0 0 0-10.88 1.53 44.89 44.89 0 0 0-7.6 3.44q-3.28 1.91-6 3.44a11.79 11.79 0 0 1-6 1.53 7.79 7.79 0 0 1-4.81-1.48 12.33 12.33 0 0 1-3.17-3.44zm61.87 48.64a149.06 149.06 0 0 0-19.68 2 52.43 52.43 0 0 0-12.79 3.8 16.81 16.81 0 0 0-6.89 5.36 11.63 11.63 0 0 0-2.08 6.67q0 7.11 4.21 10.17t11 3.06a32 32 0 0 0 14.37-3 42.68 42.68 0 0 0 11.86-9.13zM1078.18 227V114.81h16.51a6.63 6.63 0 0 1 6.89 4.92l1.75 8.31a63.82 63.82 0 0 1 6.18-6 39.66 39.66 0 0 1 6.89-4.7 37.29 37.29 0 0 1 7.87-3.12 35.05 35.05 0 0 1 9.24-1.15q10.6 0 17.43 5.74a34.56 34.56 0 0 1 10.22 15.25 34.23 34.23 0 0 1 6.56-9.57 35.9 35.9 0 0 1 8.63-6.5 39.77 39.77 0 0 1 10-3.72 48.07 48.07 0 0 1 10.66-1.2 44.82 44.82 0 0 1 16.51 2.84 31.69 31.69 0 0 1 12.13 8.31 36.63 36.63 0 0 1 7.49 13.34 58.1 58.1 0 0 1 2.57 18V227h-27v-71.41q0-10.71-4.7-16.12t-13.77-5.41a20.57 20.57 0 0 0-7.71 1.42 18.37 18.37 0 0 0-6.23 4.1 18.73 18.73 0 0 0-4.21 6.72 26.13 26.13 0 0 0-1.53 9.29V227h-27.11v-71.41q0-11.26-4.54-16.4t-13.28-5.14a21.83 21.83 0 0 0-11 2.9 36.66 36.66 0 0 0-9.46 7.92V227zM1289.85 130.12a58.51 58.51 0 0 1 15.63-12.57q8.74-4.81 20.55-4.81a36.79 36.79 0 0 1 16.78 3.83 37.68 37.68 0 0 1 13.12 11.09 53.26 53.26 0 0 1 8.53 17.93 90.21 90.21 0 0 1 3 24.43 76.84 76.84 0 0 1-3.39 23.28 57 57 0 0 1-9.67 18.58 44.72 44.72 0 0 1-15.19 12.3 44.22 44.22 0 0 1-19.95 4.43 40.23 40.23 0 0 1-16.07-2.9 39.23 39.23 0 0 1-11.92-8v45.91h-27V114.81h16.51a6.63 6.63 0 0 1 6.89 4.92zm1.42 67.77a26.57 26.57 0 0 0 10 7.87 29.76 29.76 0 0 0 11.75 2.3 25.79 25.79 0 0 0 11-2.3 21.63 21.63 0 0 0 8.36-7 34.62 34.62 0 0 0 5.3-11.86 67.79 67.79 0 0 0 1.86-16.89 75 75 0 0 0-1.58-16.67 32.22 32.22 0 0 0-4.54-11.09 17.92 17.92 0 0 0-7.16-6.23 22.34 22.34 0 0 0-9.57-2 27.28 27.28 0 0 0-14.32 3.55 43.46 43.46 0 0 0-11.15 10z"></path><path d="M144.06 306.77a9.83 9.83 0 0 1-4.72-1.21L23.79 242a9.82 9.82 0 0 1-5-7.56L.06 57.52a9.77 9.77 0 0 1 6.5-10.29L139.37.55a9.88 9.88 0 0 1 6.43 0L281.5 47.2a9.86 9.86 0 0 1 6.5 10.47L266.47 234.6a9.85 9.85 0 0 1-4.91 7.35l-112.68 63.56a9.84 9.84 0 0 1-4.82 1.26zm-110-77l110 60.49 107.22-60.48L271.89 60.5 142.64 16 16.15 60.5z"></path></svg></a></div><div class="jsx-499055511 icon mobileOnly"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 80 70"><path d="M6 1a6 6 0 1 0 0 12h68a6 6 0 0 0 0-12H6zm0 28a6 6 0 1 0 0 12h68a6 6 0 0 0 0-12H6zm0 28a6 6 0 1 0 0 12h68a6 6 0 0 0 0-12H6z"></path></svg></div></div><div class="jsx-2769082546 Menu mobileOnlyHide"><div class="jsx-2769082546 section"><h5 class="jsx-2769082546">community</h5><nav class="jsx-2769082546"><div><a target="_self" class="jsx-2769082546 item" href="/community"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 23 23"><path id="a" d="M2 4.5h20a1.5 1.5 0 0 1 0 3H2a1.5 1.5 0 0 1 0-3zm0 6h14a1.5 1.5 0 0 1 0 3H2a1.5 1.5 0 0 1 0-3zm0 6h20a1.5 1.5 0 0 1 0 3H2a1.5 1.5 0 0 1 0-3z"></path></svg></div><div class="jsx-2769082546 text">News</div><div class="jsx-2769082546 statusIcon"><svg height="20" xmlns="http://www.w3.org/2000/svg" width="40" viewBox="0 0 40 20"><g fill="none" fill-rule="evenodd"><rect width="40" height="20" fill="#FFC844" rx="4"></rect><text fill="#3D4251" font-family="Lato-Bold, Lato" font-size="11" font-weight="bold" letter-spacing=".5"><tspan x="5" y="14">BETA</tspan></text></g></svg></div></a></div><div><a target="_self" class="jsx-2769082546 item active" href="/community/tutorials"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 24.22"><path d="M16.23 24.22a2 2 0 0 1-.73-.14L7 20.79a2 2 0 0 1-1.29-1.88v-4a1.5 1.5 0 0 1 3 0v3.36l7.54 2.92 7.54-2.92v-3.45a1.5 1.5 0 0 1 3 0v4.09a2 2 0 0 1-1.29 1.88L17 24.08a2 2 0 0 1-.77.14zm-.35-2.94zm.7 0z"></path><path d="M16.23 13.35a2 2 0 0 1-.62-.1C9.17 11.16 2.36 9 1.61 8.76a2 2 0 0 1-.25-3.87l14-4.78a2 2 0 0 1 1.3 0l14 4.78a2 2 0 0 1 0 3.81l-13.8 4.55a2 2 0 0 1-.63.1zm-.31-3zM5.21 6.74c3.49 1.11 9.07 2.92 11 3.56l10.68-3.53L16 3.05z"></path></svg></div><div class="jsx-2769082546 text">Tutorials</div></a></div><div><a target="_self" class="jsx-2769082546 item" href="/community/data-science-cheatsheets"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 26"><path d="M18.5 26h-13A5.51 5.51 0 0 1 0 20.5v-15A5.51 5.51 0 0 1 5.5 0h13A5.51 5.51 0 0 1 24 5.5v15a5.51 5.51 0 0 1-5.5 5.5zM5.5 3A2.5 2.5 0 0 0 3 5.5v15A2.5 2.5 0 0 0 5.5 23h13a2.5 2.5 0 0 0 2.5-2.5v-15A2.5 2.5 0 0 0 18.5 3z"></path><path d="M16 11H8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3zM16 18H8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3z"></path></svg></div><div class="jsx-2769082546 text">Cheat Sheets</div></a></div><div><a target="_self" class="jsx-2769082546 item" href="/community/open-courses"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 34 26"><path d="M28.5 26h-23A5.51 5.51 0 0 1 0 20.5v-15A5.51 5.51 0 0 1 5.5 0h23A5.51 5.51 0 0 1 34 5.5v15a5.51 5.51 0 0 1-5.5 5.5zM5.5 3A2.5 2.5 0 0 0 3 5.5v15A2.5 2.5 0 0 0 5.5 23h23a2.5 2.5 0 0 0 2.5-2.5v-15A2.5 2.5 0 0 0 28.5 3z"></path><path d="M13.5 26a1.5 1.5 0 0 1-1.5-1.5v-22a1.5 1.5 0 0 1 3 0v22a1.5 1.5 0 0 1-1.5 1.5zM27 11h-8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3zM27 18h-8a1.5 1.5 0 0 1 0-3h8a1.5 1.5 0 0 1 0 3z"></path></svg></div><div class="jsx-2769082546 text">Open Courses</div></a></div><div><a target="_self" class="jsx-2769082546 item" href="/community/podcast"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" width="18" viewBox="0 0 18 18"><path d="M9.415 11.077h-.369a2.777 2.777 0 0 1-2.769-2.77V2.77A2.777 2.777 0 0 1 9.047 0h.368a2.777 2.777 0 0 1 2.77 2.77v5.538a2.777 2.777 0 0 1-2.77 2.769zm5.008-7.615c.573 0 1.039.464 1.039 1.038v3.462c0 3.08-2.25 5.64-5.193 6.136v1.825h2.077a1.038 1.038 0 1 1 0 2.077h-6.23a1.038 1.038 0 1 1 0-2.077h2.076v-1.825C5.25 13.602 3 11.042 3 7.962V4.5a1.038 1.038 0 1 1 2.077 0v3.462a4.158 4.158 0 0 0 4.154 4.153 4.158 4.158 0 0 0 4.154-4.153V4.5c0-.574.465-1.038 1.038-1.038z"></path></svg></div><div class="jsx-2769082546 text">Podcast - DataFramed</div></a></div><div><a target="_self" class="jsx-2769082546 item" href="/community/chat"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18 16"><g transform="translate(0 -1)"><path id="path-1" d="M12.595 13.364c.01-.111.02-.197.028-.251.058-.405.372-.702.74-.702h1.257c1.035-.002 1.875-.934 1.88-2.082V4.764c-.001-1.155-.842-2.092-1.878-2.094H3.38c-1.034.002-1.873.931-1.88 2.076v5.565c.001 1.156.842 2.092 1.878 2.094h6.626c.292 0 .557.189.68.484.408.977 1.07 1.576 1.94 1.85a6.004 6.004 0 0 1-.03-1.375h.001zm1.51 1.119c.048.314.136.521.235.606.566.487.258 1.497-.458 1.497-1.87 0-3.423-.785-4.33-2.51H3.376C1.513 14.07.004 12.39 0 10.311V4.74C.014 2.673 1.52 1.004 3.378 1h11.245c1.864.004 3.373 1.686 3.377 3.763v5.57c-.01 2.07-1.518 3.744-3.378 3.748h-.551c.004.138.016.273.035.402h-.001zm-8.423-5.81a1.115 1.115 0 1 0 0-2.229 1.115 1.115 0 0 0 0 2.229zm3.268 0a1.115 1.115 0 1 0 0-2.229 1.115 1.115 0 0 0 0 2.229zm3.318 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229z"></path></g></svg></div><div class="jsx-2769082546 text">Chat</div><div class="jsx-2769082546 statusIcon"><svg height="20" xmlns="http://www.w3.org/2000/svg" width="40" viewBox="0 0 40 17"><g fill="none" fill-rule="evenodd"><rect width="40" height="17" fill="#36D57D" rx="4"></rect><text fill="#FFF" font-family="Lato-Bold, Lato" font-size="12" font-weight="bold" letter-spacing=".4"><tspan x="5" y="13">NEW</tspan></text></g></svg></div></a></div></nav></div><div class="jsx-2769082546 section"><h5 class="jsx-2769082546">datacamp</h5><nav class="jsx-2769082546"><div><a target="_self" class="jsx-2769082546 item" href="/community/blog"><div class="jsx-2769082546 image"><svg height="14" id="Réteg_1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 17 18"><path id="path-1_1_" d="M12.3 9.8s-.1.1 0 .4c.1.2.2.2-.1.5.2.2.2.3-.1.5 0 .5.1.9-.5 1-.6 0-1.4-.4-1.7.8-.2 1.1-.2.7-.2 1H5.3c.6-1.5 1-3.2 0-4-1.1-1-2.7-4.4.6-6.2.4-.2.8-.3 1.2-.4L6.9 3s1.4-.5 2.7.1v.2c.8.2 1.4.5 1.6.6.5.3 1.2.8 1.2 1.3.1.5-.5.2-.5.2.4.9.4 1.5.2 2.2-.2.7.8 1.4.7 1.9-.1.3-.5.3-.5.3m4.2-7L8.6 0h-.4L.4 2.8c-.3.1-.4.3-.4.6l1.1 10.4c0 .2.1.4.3.4l6.8 3.7c.2.1.4.1.6 0l6.6-3.7c.2-.1.3-.3.3-.4L17 3.4c-.1-.3-.2-.5-.5-.6M8.3 7c-.4 0-.7-.4-.7-.8s.3-.7.7-.7c.4 0 .7.3.7.7 0 .4-.3.8-.7.8zm2.9-1.4l.2-.1c-.3-1-1.2-1.5-1.2-1.5l-.2.2c-.1-.2-.6-.4-.7-.5l.1-.3h-.1c-.6-.3-1.3-.3-2.1 0l.4 1.1c-.6.2-1.1.7-1.1.7l-.6-.4c-.3.5-.5 1.2-.5 1.8h-.8c0 .5.1 1.4.7 2.2l.6-.4c.5.7 1.6 1.3 2.4 1.2V8.1c.3 0 .7-.1.9-.3l.4.6c.4-.1.8-1 .8-1.1l.7.2c.2-.6.2-1.3.1-1.9z"></path></svg></div><div class="jsx-2769082546 text">Official Blog</div></a></div><div><a target="_self" class="jsx-2769082546 item" href="/community/tech"><div class="jsx-2769082546 image"><svg height="14" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 21.75 29.76"><path d="M15.56 22.57a1.5 1.5 0 0 1-1.5-1.5 7.66 7.66 0 0 1 2.47-5.29 7.38 7.38 0 0 0 2.21-5.31A7.48 7.48 0 0 0 11.28 3h-.82a7.47 7.47 0 0 0-5.2 12.83 7.63 7.63 0 0 1 2.42 5.23 1.5 1.5 0 0 1-3 0 4.65 4.65 0 0 0-1.45-3A10.47 10.47 0 0 1 10.47 0h.82a10.47 10.47 0 0 1 7.28 18 4.68 4.68 0 0 0-1.5 3.08 1.5 1.5 0 0 1-1.51 1.49zM16.5 24.26a5.5 5.5 0 0 1-11 0"></path><path d="M10.89 22.56a1.5 1.5 0 0 1-1.5-1.5v-8.84a1.5 1.5 0 0 1 3 0v8.84a1.5 1.5 0 0 1-1.5 1.5z"></path></svg></div><div class="jsx-2769082546 text">Tech Thoughts</div></a></div></nav></div></div><main class="jsx-1028385822 Main"><div class="jsx-2159026896 ActionBar"><div><div class="jsx-3863678361 ActionBarSearch"><button style="font-weight:normal" class="jsx-1844558338 Button extra noPadding"><div class="jsx-1844558338 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20.03 23"><path d="M10.39 19.29A9.65 9.65 0 1 1 20 9.65a9.66 9.66 0 0 1-9.61 9.64zm0-17.06a7.42 7.42 0 1 0 7.42 7.42 7.43 7.43 0 0 0-7.42-7.42z"></path><path d="M1.11 23a1.11 1.11 0 0 1-.89-1.78l4.1-5.47a1.11 1.11 0 1 1 1.78 1.34L2 22.56a1.11 1.11 0 0 1-.89.44z"></path></svg></div><div class="jsx-1844558338 desktopOnly">Search</div></button></div></div><div class="jsx-2159026896 authBlock"><div></div><div class="jsx-3196442269 ActionBarAuth"><div class="jsx-3196442269"><a href="https://www.datacamp.com/users/sign_in?redirect=https://www.datacamp.com/community" class="jsx-3196442269"><button class="jsx-1844558338 Button border minWidth"><div class="jsx-1844558338 ">Log in</div></button></a><button class="jsx-1844558338 Button primary"><div class="jsx-1844558338 ">Create Account</div></button></div><div class="jsx-728636942 SubmitAnArticleButton"><button class="jsx-1844558338 Button desktopButton green noPadding"><div class="jsx-1844558338 icon"><svg id="Layer_1" xmlns="http://www.w3.org/2000/svg" viewBox="-90 92 18 18"><path id="a" d="M-80.4 100.4V97c0-.3-.3-.6-.6-.6s-.6.3-.6.6v3.4H-85c-.3 0-.6.3-.6.6s.3.6.6.6h3.4v3.4c0 .3.3.6.6.6s.6-.3.6-.6v-3.4h3.4c.3 0 .6-.3.6-.6s-.3-.6-.6-.6h-3.4zM-81 92c5 0 9 4 9 9s-4 9-9 9-9-4-9-9 4-9 9-9z"></path></svg></div><div class="jsx-1844558338 desktopOnly">Share an Article</div></button><div class="jsx-728636942 mobileButton"><svg xmlns="http://www.w3.org/2000/svg" width="40" height="40" viewBox="0 0 40 40"><path d="M20.86 19.15v-4.822c0-.47-.385-.851-.86-.851s-.86.38-.86.85v4.822h-4.878c-.475 0-.86.381-.86.851s.385.85.86.85h4.877v4.822c0 .47.386.851.861.851s.86-.38.86-.85V20.85h4.878c.475 0 .86-.381.86-.851s-.385-.85-.86-.85h-4.877zM20 0c11.045 0 20 8.953 20 20 0 11.045-8.955 20-20 20C8.953 40 0 31.045 0 20 0 8.953 8.953 0 20 0z"></path></svg></div></div></div></div></div><div class="jsx-1514242801 TitleBar"><div class="jsx-1514242801 filter"><button class="jsx-1844558338 Button iconButton noPadding"><div class="jsx-1844558338 icon"><svg id="Réteg_1" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 7 12"><path id="path-1_1_" d="M5.9 0c.4 0 .9.3 1 .7s.1.9-.2 1.2L2.7 6l4 4.1c.3.5.3 1.1-.1 1.6s-1.1.4-1.5.1l-4.8-5c-.4-.4-.4-1.2 0-1.6L5.1.3c.2-.2.5-.3.8-.3z"></path></svg></div><div class="jsx-1844558338 desktopOnly">Back to Tutorials</div></button></div><div class="jsx-1514242801 title"><div class="jsx-3889859319 Title"><div class="jsx-3889859319 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 24.22"><path d="M16.23 24.22a2 2 0 0 1-.73-.14L7 20.79a2 2 0 0 1-1.29-1.88v-4a1.5 1.5 0 0 1 3 0v3.36l7.54 2.92 7.54-2.92v-3.45a1.5 1.5 0 0 1 3 0v4.09a2 2 0 0 1-1.29 1.88L17 24.08a2 2 0 0 1-.77.14zm-.35-2.94zm.7 0z"></path><path d="M16.23 13.35a2 2 0 0 1-.62-.1C9.17 11.16 2.36 9 1.61 8.76a2 2 0 0 1-.25-3.87l14-4.78a2 2 0 0 1 1.3 0l14 4.78a2 2 0 0 1 0 3.81l-13.8 4.55a2 2 0 0 1-.63.1zm-.31-3zM5.21 6.74c3.49 1.11 9.07 2.92 11 3.56l10.68-3.53L16 3.05z"></path></svg></div><div class="jsx-3889859319 h1">Tutorials</div></div></div><div class="jsx-1514242801 action"></div></div><div class="jsx-1464850800 Tutorial"><div><div><div></div><div class="jsx-undefined social__top desktopOnly"><div class="jsx-undefined voteAndSocial"><div class="jsx-undefined"><a href="#comments" class="jsx-3293774837 CommentCounter"><span class="jsx-3293774837 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18 18"><path d="M12.595 13.364c.01-.111.02-.197.028-.251.058-.405.372-.702.74-.702h1.257c1.035-.002 1.875-.934 1.88-2.082V4.764c-.001-1.155-.842-2.092-1.878-2.094H3.38c-1.034.002-1.873.931-1.88 2.076v5.565c.001 1.156.842 2.092 1.878 2.094h6.626c.292 0 .557.189.68.484.408.977 1.07 1.576 1.94 1.85a6.004 6.004 0 0 1-.03-1.375zm1.51 1.119c.048.314.136.521.235.606.566.487.258 1.497-.458 1.497-1.87 0-3.423-.785-4.33-2.51H3.376C1.513 14.07.004 12.39 0 10.311V4.74C.014 2.673 1.52 1.004 3.378 1h11.245c1.864.004 3.373 1.686 3.377 3.763v5.57c-.01 2.07-1.518 3.744-3.378 3.748h-.551c.004.138.016.273.035.402zm-8.423-5.81a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.268 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.318 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229z"></path></svg></span><span class="jsx-3293774837 count">0</span></a><div class="jsx-1972554161 Upvote"><div class="jsx-1972554161"><div class="jsx-1972554161 normal"><span class="jsx-1972554161 icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12"><path d="M1 10L6 0l5 10z"></path></svg></span><span class="jsx-1972554161 count">39</span></div><div class="jsx-1972554161 voted"><span class="jsx-1972554161 icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12"><path d="M1 10L6 0l5 10z"></path></svg></span><span class="jsx-1972554161 count">39</span></div></div></div></div><div class="jsx-494086174 Social vertical"><div class="jsx-494086174 icons"><a href="https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/essentials-linear-regression-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon"><svg height="12" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11.73 22.58"><path d="M7.61 22.58v-10.3h3.46l.52-4h-4V5.7c0-1.16.32-2 2-2h2.13V.16A28.47 28.47 0 0 0 8.63 0C5.56 0 3.47 1.87 3.47 5.31v3H0v4h3.47v10.3h4.14z"></path></svg></a><a href="https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/essentials-linear-regression-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon centerIcon"><svg height="10" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20.42 16.67"><path d="M10 5.18c0-.28-.06-.53-.07-.78a4 4 0 0 1 .73-2.57A4.08 4.08 0 0 1 13.93 0 4 4 0 0 1 17 1.15a.43.43 0 0 0 .46.12 8.68 8.68 0 0 0 2.2-.84l.2-.1a4.36 4.36 0 0 1-1.75 2.28A9 9 0 0 0 20.42 2l-.21.3a3.83 3.83 0 0 1-.23.3A8.45 8.45 0 0 1 18.5 4a.28.28 0 0 0-.13.27A12 12 0 0 1 17 10.18a11.8 11.8 0 0 1-3.37 4.11 11.17 11.17 0 0 1-4.39 2.06 12.53 12.53 0 0 1-4.44.22 11.87 11.87 0 0 1-4.74-1.73L0 14.79a8.6 8.6 0 0 0 6.16-1.74 4.28 4.28 0 0 1-3.91-2.91h.95a6.18 6.18 0 0 0 .89-.12A4.2 4.2 0 0 1 .8 5.88a4 4 0 0 0 1.81.49 4.23 4.23 0 0 1-1.78-3A4.07 4.07 0 0 1 1.38.79 12.06 12.06 0 0 0 10 5.18z" id="iOjKBC.tif"></path></svg></a><a href="https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/essentials-linear-regression-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon"><svg height="10" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16.99 17"><path d="M3.85 17H.34V5.67h3.51zM2.07 4.18a2.09 2.09 0 1 1 2.08-2.09 2.08 2.08 0 0 1-2.08 2.09zM17 17h-3.5v-5.95c0-1.63-.62-2.54-1.91-2.54s-2.14.95-2.14 2.54V17H6.09V5.67h3.36v1.52a4 4 0 0 1 3.42-1.87c2.4 0 4.12 1.47 4.12 4.5V17z"></path></svg></a></div></div></div></div></div><div class="jsx-1464850800 preface"><div class="jsx-1464850800 author"><div class="jsx-566588255 Author"><a href="/profile/spsayakpaul" target="_blank" class="jsx-566588255"><div style="background-image:url(https://res.cloudinary.com/dyd911kmh/image/fetch/t_avatar_thumbnail/https://assets.datacamp.com/users/avatars/000/535/025/square/Capture.PNG?1556868932);border-radius:20px;min-width:40px;min-height:40px" class="jsx-3208234818 Avatar"></div><div class="jsx-566588255 info"><div class="jsx-566588255 name">Sayak  Paul</div><div class="jsx-566588255 date"><span>October 31st, 2018</span></div></div></a></div></div><div class="jsx-1464850800 tags"><div class="jsx-2792531181 TagLine"><div class="jsx-1764811326 Tag"><span class="jsx-1764811326 title">statistical modeling</span></div><a class="jsx-1022557955 more">+<!-- -->1</a></div></div><h1 class="jsx-1464850800 pageTitle">Essentials of Linear Regression in Python</h1><div class="jsx-1464850800 description pageDescription">Learn what formulates a regression problem and how a linear regression algorithm works in Python.</div></div><div class="markdown"><div><p>The field of Data Science has progressed like nothing before. It incorporates so many different domains like Statistics, Linear Algebra, Machine Learning, Databases into its account and merges them in the most meaningful way possible. But, in its core, what makes this domain one of the craziest ones? - <strong>The powerful statistical algorithms</strong></p>
<p>One such very primitive statistical algorithm is <strong>Linear Regression</strong>. Although it is very old, it is never too old to be neglected for a budding data scientist like you. Understanding the principle behind the working of linear regression is very important as to reason the evolution of a whole class of statistical algorithms called <strong>Generalized Linear Models</strong>. Moreover, it will also help you understand other aspects of a typical statistical/machine learning algorithm for example - cost functions, coefficients, optimization, etc.</p>
<p>As the title of this tutorial suggests, you will cover Linear Regression in details in this tutorial. Specifically, you will cover:</p>
<ul>
<li>Understanding a regression problem</li>
<li>Coming up with a hypothesis for the regression problem</li>
<li>What is Linear Regression</li>
<li>Cost functions</li>
<li>Optimizing Linear Regression</li>
<li>Why it is still important to understand even in the world of neural networks</li>
<li>A case study in Python</li>
</ul>
<p>Before diving deep into the theories behind Linear Regression let's have a clear view of the term <em>regression</em>.</p>
<h2 id="understanding-a-regression-problem-">Understanding a regression problem:</h2>
<p>Regression belongs to the class of <a href="https://en.wikipedia.org/wiki/Supervised_learning">Supervised Learning</a> tasks where the datasets that are used for predictive/statistical modeling contain <a href="http://www.statisticshowto.com/continuous-variable/">continuous labels</a>. But, let's define a regression problem more mathematically.</p>
<p>Let's consider the following image below:
<img src="https://image.ibb.co/jRH8E9/Capture.jpg" />
<strong>Source: Andrew Ng's lecture notes</strong></p>
<p>So, in the above image, <i>X</i> is the set of values that correspond to the living areas of various houses (also considered as the space of input values) and <i>y</i> is the price of the respective houses but note that these values are predicted by <i>h</i>. <i>h</i> is the function that maps the <i>X</i> values to <i>y</i> (often called as predictor). For historical reasons, this <i>h</i> is referred to as a <i>hypothesis</i> function. Keep in mind that, this dataset has only featured, i.e., the living areas of various houses and consider this to be a toy dataset for the sake of understanding.</p>
<p>Note that, the predicted values here are continuous in nature. So, your ultimate goal is, given a training set, to learn a function
$h : \mathcal{X}  \rightarrow \mathcal{Y}$ so that h(x) is a &quot;good&quot; predictor for the corresponding value of y. Also, keep in mind that the domain of values that both X and Y accept are all real numbers and you can define it like this: $\mathcal{X}  = \mathcal{Y} = \mathbb{IR}$ where, $\mathbb{IR}$ is the set of all real numbers.</p>
<p>A pair (x<sup>(i)</sup>, y<sup>(i)</sup>) is called a training example. You can define the training set as {(x<sup>(i)</sup>, y<sup>(i)</sup>) ; i = 1,...,m} (in case the training set contains m instances and there is only one feature <i>x</i> in the dataset).</p>
<p>A bit of mathematics there for you so that you don't go wrong even in the simplest of things. So, according to Han, Kamber, and Pei-</p>
<p><em>&quot;In general, these methods are used to predict the value of a response
(dependent) variable from one or more predictor (independent) variables, where the
variables are numeric.</em>&quot; - <a href="https://scholar.google.com/scholar?q=Han%2C%20J.%2C%20Kamber%2C%20M.%2C%20Pei%2C%20J.%3A%20Data%20Mining%3A%20Concepts%20and%20Techniques%20%283rd%20edn.%29%2C%20pp.%2056.%20Morgan%20Kaufmann%20Publishers%20Inc.%20San%20Francisco%20%282011%29">Data Mining: Concepts and Techniques (3rd edn.)</a></p>
<p>As simple as that!</p>
<p>So, in the course of understanding a typical regression problem you also saw how to define a hypothesis for it as well. Brilliant going. You have set the mood just perfect! Now, you will straight dive into the mechanics of Linear Regression.</p>
<h2 id="linear-regression-how-it-works-">Linear Regression - how it works?</h2>
<p>Before going into its details, won't it be good to take a look when it was discovered? Well, that goes way-way back to 18th Century. The mighty <strong>Carl Friedrich Gauss</strong> first proposed the most trivial form of statistical regression, but there are many arguments on this. Let's not get into those. But if you are interested to see the arguments that took place for this between Gauss and <strong>Adrien-Marie Legendre</strong>, <a href="&quot;https://priceonomics.com/the-discovery-of-statistical-regression/&quot;">this is the link</a> to check out.</p>
<p>Linear regression is perhaps one of the most well known and well-understood algorithms in statistics and machine learning. Linear regression was developed in the field of statistics and is studied as a model for understanding the relationship between input and output numerical variables, but with the course of time, it has become an integral part of modern machine learning toolbox.</p>
<p>Let's have a toy dataset for it. You will use the same house price prediction dataset to investigate this but this time with two features. The task remains the same i.e., predicting the house price.</p>
<p><img src="https://image.ibb.co/hXTenU/Capture.jpg" />
<strong>Source: Andrew Ng's lecture notes</strong></p>
<p>As mentioned earlier, now the x’s are two-dimensional which means your dataset contains two features. For instance, x<sub>1</sub><sup>(i)</sup> is the living area of the i-th house in the training set, and x<sub>2</sub><sup>(i)</sup> is its number of bedrooms.</p>
<p>To perform regression, you must decide the way you are going to represent h. As an initial choice, let’s say you decide to approximate y as a linear function of x:</p>
<center><b>h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x<sub>1</sub> + θ<sub>2</sub>x<sub>2</sub></b></center>

<p>Here, the θ<sub>i</sub>’s are the parameters (also called weights) parameterizing the space of linear functions mapping from $\mathcal{X}$ to $\mathcal{Y}$. In a simpler sense, these parameters are used for accurately mapping $\mathcal{X}$ to $\mathcal{Y}$. But to keep things simple for your understanding, you will drop the θ subscript in h<sub>θ</sub>(x), and write it simply as h(x). To simplify your notation even further, you will also introduce the convention of letting x<sub>0</sub> = 1 (this is the <b>intercept</b> term), so that</p>
<p><img src="https://image.ibb.co/i94zMp/Capture.jpg" /></p>
<p>where on the right-hand side above you are considering θ and x both as vectors, and here n is the number of input instances (not counting x<sub>0</sub>).</p>
<p>But the main question that gets raised at this point is how do you pick or learn the parameters θ? You cannot change your input instances as to predict the prices. You have only these θ parameters to tune/adjust.</p>
<p>One prominent method seems to be to make h(x) close to y, at least for the training examples you have. To understand this more formally, let's try defining a function that determines, for each value of the θ’s, how close the h(x<sup>(i)</sup>)’s are to the corresponding y<sup>(i)</sup> ’s. The function should look like the following:</p>
<p><img src="https://image.ibb.co/kW8e49/Capture.jpg" /></p>
<p><a href="https://stackoverflow.com/questions/32274474/machine-learning-linear-regression-using-batch-gradient-descent">Source: StackOverflow</a></p>
<p>To understand the reason behind taking the squared value instead of the absolute value, consider this squared-term as an advantage for the future operations to be performed for training the regression model. But if you want to dig deeper, <a href="https://www.quora.com/Why-do-we-use-square-error-instead-of-absolute-value-when-we-calculate-R-2-in-regression-analysis">help yourself</a>.</p>
<p>You just saw one of the most important formulas in the world of Data Science/Machine Learning/Statistics. It is called as <strong>cost function</strong>.</p>
<p>This is an essential derivation because not only it gives birth to the next evolution of the linear regression (Ordinary Least Squares) but also formulates the foundations of a whole class of linear modeling algorithms (remember you came across a term called <em>Generalized Linear Models</em>).</p>
<p>It is important to note that, linear regression can often be divided into two basic forms:</p>
<ul>
<li>Simple Linear Regression (SLR) which deals with just two variables (the one you saw at first)</li>
<li>Multi-linear Regression (MLR) which deals with more than two variables (the one you just saw)</li>
</ul>
<p>These things are very straightforward but can often cause confusion.</p>
<p>You have already laid your foundations of linear regression. Now you will study more about the ways of estimating the parameters you saw in the above section. This estimation of parameters is essentially known as the training of linear regression. Now, there are many methods to train a linear regression model <strong>Ordinary Least Squares</strong> (OLS) being the most popular among them. So, it is good to refer a linear regression model trained using OLS as Ordinary Least Squares Linear Regression or just Least Squares Regression.</p>
<p><strong>Note that the parameters here in this context are also called model coefficients.</strong></p>
<h2 id="optimizing-a-linear-regression-model-various-approaches-">Optimizing a linear regression model - various approaches:</h2>
<p>Learning/training a linear regression model essentially means estimating the values of the coefficients/parameters used in the representation with the data you have.</p>
<p>In this section, you will take a brief look at some techniques to prepare a linear regression model.</p>
<h2 id="least-squares-regression-">Least squares' regression:</h2>
<p>You left the previous section with a notion to choose θ so as to minimize J(θ). To do so, let’s use a search algorithm that starts with some &quot;initial guess&quot; for θ, and that iteratively changes θ to make J(θ) smaller, until hopefully, you converge to a value of θ that minimizes J(θ). Specifically, let’s consider the <strong>gradient descent algorithm</strong>, which starts with some initial θ, and repeatedly performs the update:</p>
<p><img src="https://image.ibb.co/jUXWj9/Capture.jpg" />
<strong>Source: Andrew Ng's lecture notes</strong></p>
<p>(This update is simultaneously performed for all values of j = 0, . . . , n.) Here, α is called the <strong>learning rate</strong>. This is a very natural algorithm that repeatedly takes a step in the direction of steepest decrease of J. This term α effectively controls how steep your algorithm would move to the decrease of J. It can be pictorially expressed as the following:</p>
<p><img src="https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png" />
<strong> Source: <a href="https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png">ml-cheatsheet</a></strong></p>
<p>Intuitively speaking, the above formula denotes the small change that happens in J w.r.t the θ<sub>j</sub> parameter and how it affects the initial value of θ<sub>j</sub>. But look carefully, you have a partial derivative here to deal with. The whole derivation process is out of the scope of this tutorial.</p>
<p>Just note that for a single training example, this gives the update rule:</p>
<p><img src="https://image.ibb.co/gpfOxU/Capture.jpg" />
<strong>Source: <a href="https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png">ml-cheatsheet</a></strong></p>
<p>The rule is called the LMS update rule (LMS stands for “least mean squares”) and is also known as the <strong>Widrow-Hoff learning rule</strong>.</p>
<p>Let's summarize a few things in the context of OLS.</p>
<p><em>&quot;The Ordinary Least Squares procedure seeks to minimize the sum of the squared residuals. This means that given a regression line through the data we calculate the distance from each data point to the regression line, square it, and sum all of the squared errors together. This is the quantity that ordinary least squares seeks to minimize.&quot;</em> - <a href="https://machinelearningmastery.com/linear-regression-for-machine-learning/">Jason Brownlee</a></p>
<h2 id="optimization-with-gradient-descent-">Optimization with gradient descent:</h2>
<p>In the previous training rule, you already got the notion of how gradient descent can be incorporated in this context. Essentially, gradient descent is a process of optimizing the values of the coefficients by iteratively minimizing the error of the model on your training data.</p>
<p>More briefly speaking, it works by starting with random values for each coefficient. The sum of the squared errors is calculated for each pair of input and output values. A learning rate is used as a scale factor, and the coefficients are updated in the direction towards minimizing the error. The process is repeated until a minimum sum squared error is achieved or no further improvement is possible.</p>
<p>The term α (learning rate) is very important here since it determines the size of the improvement step to take on each iteration of the procedure.</p>
<p>Now there are commonly two variants of gradient descent:</p>
<ul>
<li>The method that looks at every example in the entire training set on every step and is called <strong>batch gradient descent</strong>.</li>
<li>The method where you repeatedly run through the training set, and each time you encounter a training example, you update the parameters according to the gradient of the error with respect to that single training example only. This algorithm is called <strong>stochastic gradient descent</strong> (also incremental gradient descent).</li>
</ul>
<p>That is all for gradient descent for this tutorial. Now, you take a look at another way of optimizing a linear regression model, i.e. Regularization.</p>
<h2 id="regularization-">Regularization:</h2>
<p>DataCamp already has a good introductory <a href="https://www.datacamp.com/community/tutorials/towards-preventing-overfitting-regularization">article on Regularization</a>. You might want to check that out before proceeding with this one.</p>
<p>Generally, regularization methods work by penalizing the coefficients of features having extremely large values and thereby try to reduce the error. It not only results in an enhanced error rate but also, it reduces the model complexity. This is particularly very useful when you are dealing with a dataset that has a large number of features, and your baseline model is not able to distinguish between the importance of the features (not all features in a dataset are equally important, right?).</p>
<p>There are two variants of regularization procedures for linear regression are:</p>
<p><strong>Lasso Regression</strong>: adds a penalty term which is equivalent to the <strong>absolute value of the magnitude of the coefficients</strong> (also called <em>L1 regularization</em>). The penalty terms look like:
<img src="https://image.ibb.co/bzZ2cf/Capture.jpg" /><img /></p>
<p>where,</p>
<ul>
<li>$\lambda$ is the constant factor that you add in order to control the speed of the improvement in error (<strong>learning rate</strong>)</li>
<li>the dataset has (M+1) features, so it runs from 0 to M. w<sub><b>j</b></sub> is the weight/coefficient.</li>
</ul>
<p><strong>Ridge Regression</strong>: adds a penalty term which is equivalent to the <strong>square of the magnitude of coefficients</strong> (also called <em>L2 regularization</em>). The penalty terms look like:
<img src="https://image.ibb.co/hY4oiL/Capture.jpg" /><img /></p>
<h2 id="is-it-still-worth-the-time-to-learn-linear-regression-">Is it still worth the time to learn linear regression?</h2>
<p>Is it not? You already saw how gracefully linear regression introduces some of the most critical concepts of Machine Learning such as cost functions, optimization, variable relationships and what not? All these things are vital even if you are constructing a neural network. The applicability may differ in some places, but the overall concepts remain precisely the same. So, without understanding these fundamental things, you will never be able to reason why your neural net is not performing well.</p>
<p>Moreover, a simple concept of deriving the relationships among variables gave birth to so many concepts and most importantly it created a whole new family of algorithms - <a href="https://onlinecourses.science.psu.edu/stat504/node/216/">Generalized Linear Models</a>. So for aspiring Data Science/Machine Learning/Artificial Intelligence practitioner, this algorithm is something which cannot be neglected. You already have understood that by now!</p>
<p>No, you will implement a simple linear regression in Python for yourself now. It should be fun!</p>
<h2 id="a-case-study-in-python-">A case study in Python:</h2>
<p>For this case study first, you will use the <a href="http://www.statsmodels.org/stable/index.html">Statsmodel</a> library for Python. It is a very popular library which provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. For the data, you will use the famous <a href="https://www.kaggle.com/c/boston-housing">Boston House dataset</a>. The mighty scikit-learn comes with this dataset, so you don't need to download it separately.</p>
<p>Let's start the case study by importing the <code>statsmodels</code> library and your dataset:</p>
<pre><code class="lang-python">import statsmodels.api as sm
from sklearn import datasets

data = datasets.load_boston()
</code></pre>
<p>Scikit-learn provides a handy description of the dataset, and it can be easily viewed by:</p>
<pre><code class="lang-python">print (data.DESCR)
</code></pre>
<pre><code>Boston House Prices dataset
===========================

Notes
------
Data Set Characteristics:  

    :Number of Instances: 506

    :Number of Attributes: 13 numeric/categorical predictive

    :Median Value (attribute 14) is usually the target

    :Attribute Information (in order):
        - CRIM     per capita crime rate by town
        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.
        - INDUS    proportion of non-retail business acres per town
        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
        - NOX      nitric oxides concentration (parts per 10 million)
        - RM       average number of rooms per dwelling
        - AGE      proportion of owner-occupied units built prior to 1940
        - DIS      weighted distances to five Boston employment centres
        - RAD      index of accessibility to radial highways
        - TAX      full-value property-tax rate per $10,000
        - PTRATIO  pupil-teacher ratio by town
        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
        - LSTAT    % lower status of the population
        - MEDV     Median value of owner-occupied homes in $1000's

    :Missing Attribute Values: None

    :Creator: Harrison, D. and Rubinfeld, D.L.

This is a copy of UCI ML housing dataset.
http://archive.ics.uci.edu/ml/datasets/Housing


This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.

The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic
prices and the demand for clean air', J. Environ. Economics &amp; Management,
vol.5, 81-102, 1978.   Used in Belsley, Kuh &amp; Welsch, 'Regression diagnostics
...', Wiley, 1980.   N.B. Various transformations are used in the table on
pages 244-261 of the latter.

The Boston house-price data has been used in many machine learning papers that address regression
problems.   

**References**

   - Belsley, Kuh &amp; Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.
   - Quinlan, R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.
   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)
</code></pre><p>Now, before applying linear regression, you will have to prepare the data and segregate the features and the label of the dataset. <code>MEDV</code> (median home value) is the label in this case. You can access the features of the dataset using <code>feature_names</code> attribute.</p>
<p>A bit of <code>pandas</code> knowledge will come in handy here. <a href="https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf">This cheat sheet</a> is a must-see if you are looking for ways to refresh basic <code>pandas</code> concepts.</p>
<pre><code class="lang-python"># Pandas and NumPy import
import numpy as np
import pandas as pd

# Set the features  
df = pd.DataFrame(data.data, columns=data.feature_names)

# Set the target
target = pd.DataFrame(data.target, columns=[&quot;MEDV&quot;])
</code></pre>
<p>At this point, you need to consider a few important things about linear regression before applying it to the data. You could have studied this earlier in this tutorial, but studying these factors at this particular point of time will help you get the real feel.</p>
<ul>
<li><strong>Linear Assumption</strong>: Linear regression is best employed to capture the relationship between the input variables and the outputs. In order to do so, linear regression assumes this relationship to be linear (which might not be the case all the time). But you can always transform your data so that a linear relationship is maintained. For example, if your data has an exponential relationship, you can apply <a href="https://www.dummies.com/programming/big-data/data-science/data-science-using-python-to-modify-data-distributions/"><em>log-transform</em></a> to make the relationship linear.
<br /><br /></li>
<li><strong>Collinearity between the features</strong>: Collinearity is a measure to calculate the importance of a feature of a dataset mathematically. When you have a dataset in which the features are very correlated to each other, linear regression fails to approximate the relationship appropriately and tends to overfit. So, it is efficient to detect the highly correlated features and to drop them before you apply linear regression. If you want to know more about this, feel free to check <a href="https://www.kaggle.com/robertoruiz/dealing-with-multicollinearity">this excellent Kaggle kernel</a>.</li>
</ul>
<p>Let's do some hands-on now. To keep things simple you will just take <code>RM — the average number of rooms</code> feature for now. Note that <code>Statsmodels</code> does not add a constant term (recall the factor θ<sub>0</sub>) by default. Let’s see it first without the constant term in your regression model:</p>
<pre><code class="lang-python">X = df[&quot;RM&quot;]
y = target[&quot;MEDV&quot;]

# Fit and make the predictions by the model
model = sm.OLS(y, X).fit()
predictions = model.predict(X)

# Print out the statistics
model.summary()
</code></pre>
<p><center><img src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1540929849/image_es1tjp.png" /></center></p>
<p>What is this output! It is way too big to understand when you are seeing it for the first time. Let's go through the most critical points step by step:</p>
<ul>
<li>The first observation you should make here is you are using <code>OLS</code> method to train your linear regression model.
<br /><br /></li>
<li>There's a value corresponding to <strong>R-Squared</strong>. R-squared is the “percent of variance explained” by the model.  That is, R-squared is the fraction by which the variance of the errors is less than the variance of the dependent variable. R-squared values range from 0 to 1 and are commonly stated as percentages from 0% to 100%. R-squared will give you an estimate of the relationship between movements of a dependent variable based on an independent variable's movements. It doesn't tell you whether your chosen model is good or bad, nor will it tell you whether the data and predictions are biased. A high or low R-square isn't necessarily good or bad, as it doesn't convey the reliability of the model, nor whether you've chosen the right regression. You can get a low R-squared for a good model, or a high R-square for a poorly fitted model, and vice versa.
<br /><br /></li>
<li>The coefficient (<em>coef</em>) of 3.634 means that if the <code>RM</code> variable increases by 1, the predicted value of <code>MEDV</code> increases by 3.634.
<br /><br /></li>
<li>There is a 95% confidence intervals for the RM which means that the model predicts at a 95% percent confidence that the value of RM is between 3.548 to 3.759).</li>
</ul>
<p>These are the most important points you should take care of for the time being (and you can ignore the warning as well).</p>
<p>A constant term can easily be added to the linear regression model. You can do it by <code>X = sm.add_constant(X)</code> (X is the name of the dataframe containing the input (independent variables).</p>
<pre><code class="lang-python">X = sm.add_constant(X)

model = sm.OLS(y, X).fit()
predictions = model.predict(X)

model.summary()
</code></pre>
<p><center><img src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1540930228/Screen_Shot_2018-10-30_at_4.09.02_PM_gb0e9n.png" /></center></p>
<p>It can be clearly seen that the addition of the constant term has a direct effect on the coefficient term. Without the constant term, your model was passing through the origin, but now you have a y-intercept at -34.67. Now the slope of the <code>RM</code> predictor is also changed from 3.634 to 9.1021 (<em>coef</em> of <code>RM</code>).</p>
<p>Now you will fit a regression model with more than one variable — you will add <code>LSTAT</code> (percentage of lower status of the population) along with the <code>RM</code> variable. The model training (fitting) procedure remains the exact same as previous:</p>
<pre><code class="lang-python">X = df[[&quot;RM&quot;, &quot;LSTAT&quot;]]
y = target[&quot;MEDV&quot;]

model = sm.OLS(y, X).fit()
predictions = model.predict(X)

model.summary()
</code></pre>
<p><center><img src="https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1540930228/Screen_Shot_2018-10-30_at_4.09.15_PM_oamdc9.png" /></center></p>
<p>Let's interpret this one now:</p>
<p>This model has a much higher R-squared value — 0.948, which essentially means that this model captures 94.8% of the variance in the dependent variable. Now, let's try to figure out the relationship between the two variables <code>RM</code> and <code>LSTAT</code> and median house value. As <code>RM</code> increases by 1, <code>MEDV</code> will increase by 4.9069, and when <code>LSTAT</code> increases by 1, <code>MEDV</code> will <em>decrease</em> by 0.6557. This indicates that <code>RM</code> and <code>LSTAT</code> are statistically significant in predicting (or estimating) the median house value.</p>
<p>You can interpret this relationship in plain English as well:</p>
<ul>
<li>Houses having a small number of rooms are likely to have low price values.</li>
<li>In the areas where the status of the population, is lower the house prices are likely to be low.</li>
</ul>
<p>Makes more sense now! Isn't it?</p>
<p>This was the example of both single and multiple linear regression in <code>Statsmodels</code>. Your homework will be to investigate and interpret the results with the further features.</p>
<p>Next, let's see how linear regression can be implemented using your very own <code>scikit-learn</code>. You already have the dataset imported, but you will have to import the <code>linear_model</code> class.</p>
<pre><code class="lang-python">from sklearn import linear_model
</code></pre>
<pre><code class="lang-python">X = df
y = target[&quot;MEDV&quot;]

lm = linear_model.LinearRegression()
model = lm.fit(X,y)
</code></pre>
<p>The model training is completed. This <code>sklearn</code> implementation also uses OLS. Let's make some predictions of <code>MEDV</code> values for the first five samples.</p>
<pre><code class="lang-python">predictions = lm.predict(X)
print(predictions[0:5])
</code></pre>
<pre><code>[30.00821269 25.0298606  30.5702317  28.60814055 27.94288232]
</code></pre><p>If you want to know some more details (such as the R-squared, coefficients, etc.) of your model, you can easily do so.</p>
<pre><code class="lang-python">lm.score(X,y)
</code></pre>
<pre><code>0.7406077428649427
</code></pre><pre><code class="lang-python">lm.coef_
</code></pre>
<pre><code>array([-1.07170557e-01,  4.63952195e-02,  2.08602395e-02,  2.68856140e+00,
       -1.77957587e+01,  3.80475246e+00,  7.51061703e-04, -1.47575880e+00,
        3.05655038e-01, -1.23293463e-02, -9.53463555e-01,  9.39251272e-03,
       -5.25466633e-01])
</code></pre><h2 id="wrap-up-">Wrap up!</h2>
<p>Beautiful! You have made it to the end. Covering one of the simplest and the most fundamental algorithms was not that easy, but you did it pretty well. You not only got familiarized with simple linear regression but also studied many fundamental aspects, terms, factors of machine learning. You did an in-depth case study in Python as well.</p>
<p>This tutorial can also be treated as a motivation for you to implement Linear Regression from scratch. Following are the brief steps if anyone wants to do it for real:</p>
<ul>
<li>Calculate the mean and variance of the data</li>
<li>Calculate covariance</li>
<li>Estimate coefficients</li>
<li>Make predictions</li>
</ul>
<p>Following are some references that were used in order to prepare this tutorial:</p>
<ul>
<li><a href="https://www.coursera.org/learn/machine-learning">Machine Learning course by Coursera (taught by the great Andrew Ng)</a></li>
<li><a href="https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/">Implementing linear regression from scratch</a></li>
<li><a href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">The Elements of Statistical Learning</a></li>
<li><a href="https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9">Simple and Multiple Linear Regression in Python by Adi Bronshtein</a></li>
</ul>
<p>If you would like to learn more about linear classifiers, take DataCamp's <a href="https://www.datacamp.com/courses/linear-classifiers-in-python">Linear Classifiers in Python</a> course.</p></div><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/solarized-dark.min.css" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Lora" rel="stylesheet"/><link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,400i,700,700i" rel="stylesheet"/></div><div class="jsx-1464850800 social__bottom mobileOnly"><div class="jsx-1464850800 voteAndSocial"><div class="jsx-1464850800"><div class="jsx-1972554161 Upvote"><div class="jsx-1972554161"><div class="jsx-1972554161 normal"><span class="jsx-1972554161 icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12"><path d="M1 10L6 0l5 10z"></path></svg></span><span class="jsx-1972554161 count">39</span></div><div class="jsx-1972554161 voted"><span class="jsx-1972554161 icon"><svg xmlns="http://www.w3.org/2000/svg" width="12" height="12" viewBox="0 0 12 12"><path d="M1 10L6 0l5 10z"></path></svg></span><span class="jsx-1972554161 count">39</span></div></div></div><a href="#comments" class="jsx-3293774837 CommentCounter"><span class="jsx-3293774837 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18 18"><path d="M12.595 13.364c.01-.111.02-.197.028-.251.058-.405.372-.702.74-.702h1.257c1.035-.002 1.875-.934 1.88-2.082V4.764c-.001-1.155-.842-2.092-1.878-2.094H3.38c-1.034.002-1.873.931-1.88 2.076v5.565c.001 1.156.842 2.092 1.878 2.094h6.626c.292 0 .557.189.68.484.408.977 1.07 1.576 1.94 1.85a6.004 6.004 0 0 1-.03-1.375zm1.51 1.119c.048.314.136.521.235.606.566.487.258 1.497-.458 1.497-1.87 0-3.423-.785-4.33-2.51H3.376C1.513 14.07.004 12.39 0 10.311V4.74C.014 2.673 1.52 1.004 3.378 1h11.245c1.864.004 3.373 1.686 3.377 3.763v5.57c-.01 2.07-1.518 3.744-3.378 3.748h-.551c.004.138.016.273.035.402zm-8.423-5.81a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.268 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.318 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229z"></path></svg></span><span class="jsx-3293774837 count">0</span></a></div><div class="jsx-494086174 Social"><div class="jsx-494086174 icons"><a href="https://www.facebook.com/sharer.php?u=https://www.datacamp.com/community/tutorials/essentials-linear-regression-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon"><svg height="12" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11.73 22.58"><path d="M7.61 22.58v-10.3h3.46l.52-4h-4V5.7c0-1.16.32-2 2-2h2.13V.16A28.47 28.47 0 0 0 8.63 0C5.56 0 3.47 1.87 3.47 5.31v3H0v4h3.47v10.3h4.14z"></path></svg></a><a href="https://twitter.com/intent/tweet?url=https://www.datacamp.com/community/tutorials/essentials-linear-regression-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon centerIcon"><svg height="10" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20.42 16.67"><path d="M10 5.18c0-.28-.06-.53-.07-.78a4 4 0 0 1 .73-2.57A4.08 4.08 0 0 1 13.93 0 4 4 0 0 1 17 1.15a.43.43 0 0 0 .46.12 8.68 8.68 0 0 0 2.2-.84l.2-.1a4.36 4.36 0 0 1-1.75 2.28A9 9 0 0 0 20.42 2l-.21.3a3.83 3.83 0 0 1-.23.3A8.45 8.45 0 0 1 18.5 4a.28.28 0 0 0-.13.27A12 12 0 0 1 17 10.18a11.8 11.8 0 0 1-3.37 4.11 11.17 11.17 0 0 1-4.39 2.06 12.53 12.53 0 0 1-4.44.22 11.87 11.87 0 0 1-4.74-1.73L0 14.79a8.6 8.6 0 0 0 6.16-1.74 4.28 4.28 0 0 1-3.91-2.91h.95a6.18 6.18 0 0 0 .89-.12A4.2 4.2 0 0 1 .8 5.88a4 4 0 0 0 1.81.49 4.23 4.23 0 0 1-1.78-3A4.07 4.07 0 0 1 1.38.79 12.06 12.06 0 0 0 10 5.18z" id="iOjKBC.tif"></path></svg></a><a href="https://www.linkedin.com/cws/share?url=https://www.datacamp.com/community/tutorials/essentials-linear-regression-python" target="_blank" rel="noopener noreferrer" class="jsx-494086174 icon"><svg height="10" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16.99 17"><path d="M3.85 17H.34V5.67h3.51zM2.07 4.18a2.09 2.09 0 1 1 2.08-2.09 2.08 2.08 0 0 1-2.08 2.09zM17 17h-3.5v-5.95c0-1.63-.62-2.54-1.91-2.54s-2.14.95-2.14 2.54V17H6.09V5.67h3.36v1.52a4 4 0 0 1 3.42-1.87c2.4 0 4.12 1.47 4.12 4.5V17z"></path></svg></a></div></div></div></div></div></div><div id="comments" class="jsx-3956319705 PostAComment"><button class="jsx-1844558338 Button big seeAll same greyIcon"><div class="jsx-1844558338 icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18 18"><path d="M12.595 13.364c.01-.111.02-.197.028-.251.058-.405.372-.702.74-.702h1.257c1.035-.002 1.875-.934 1.88-2.082V4.764c-.001-1.155-.842-2.092-1.878-2.094H3.38c-1.034.002-1.873.931-1.88 2.076v5.565c.001 1.156.842 2.092 1.878 2.094h6.626c.292 0 .557.189.68.484.408.977 1.07 1.576 1.94 1.85a6.004 6.004 0 0 1-.03-1.375zm1.51 1.119c.048.314.136.521.235.606.566.487.258 1.497-.458 1.497-1.87 0-3.423-.785-4.33-2.51H3.376C1.513 14.07.004 12.39 0 10.311V4.74C.014 2.673 1.52 1.004 3.378 1h11.245c1.864.004 3.373 1.686 3.377 3.763v5.57c-.01 2.07-1.518 3.744-3.378 3.748h-.551c.004.138.016.273.035.402zm-8.423-5.81a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.268 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229zm3.318 0a1.114 1.114 0 1 0 0-2.229 1.114 1.114 0 0 0 0 2.229z"></path></svg></div><div class="jsx-1844558338 ">Post a Comment</div></button></div></main><div class="jsx-2506565400 SidebarSocial"><div class="jsx-2506565400 rss"><a href="/community/rss.xml" class="jsx-2506565400"><svg height="13" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 18 18"><circle cx="3.08" cy="14.92" r="3.08"></circle><path d="M16.46 18a1.54 1.54 0 0 1-1.54-1.54c0-8.25-5.13-13.38-13.38-13.38A1.59 1.59 0 0 1 .46 1.15 1.72 1.72 0 0 1 1.54 0a16.45 16.45 0 0 1 12 4.51c3 2.95 4.51 7.08 4.51 12A1.54 1.54 0 0 1 16.46 18z"></path><path d="M10.63 18a1.54 1.54 0 0 1-1.54-1.54c0-5-2.54-7.54-7.54-7.54a1.54 1.54 0 0 1 0-3.08c6.75 0 10.63 3.87 10.63 10.62A1.54 1.54 0 0 1 10.63 18z"></path></svg>Subscribe to RSS</a></div><div class="jsx-2506565400 icons"><a href="https://www.facebook.com/pages/DataCamp/726282547396228" target="_blank" rel="noopener noreferrer" class="jsx-2506565400 icon"><svg height="16" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 11.73 22.58"><path d="M7.61 22.58v-10.3h3.46l.52-4h-4V5.7c0-1.16.32-2 2-2h2.13V.16A28.47 28.47 0 0 0 8.63 0C5.56 0 3.47 1.87 3.47 5.31v3H0v4h3.47v10.3h4.14z"></path></svg></a><a href="https://twitter.com/datacamp" target="_blank" rel="noopener noreferrer" class="jsx-2506565400 icon"><svg height="13" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20.42 16.67"><path d="M10 5.18c0-.28-.06-.53-.07-.78a4 4 0 0 1 .73-2.57A4.08 4.08 0 0 1 13.93 0 4 4 0 0 1 17 1.15a.43.43 0 0 0 .46.12 8.68 8.68 0 0 0 2.2-.84l.2-.1a4.36 4.36 0 0 1-1.75 2.28A9 9 0 0 0 20.42 2l-.21.3a3.83 3.83 0 0 1-.23.3A8.45 8.45 0 0 1 18.5 4a.28.28 0 0 0-.13.27A12 12 0 0 1 17 10.18a11.8 11.8 0 0 1-3.37 4.11 11.17 11.17 0 0 1-4.39 2.06 12.53 12.53 0 0 1-4.44.22 11.87 11.87 0 0 1-4.74-1.73L0 14.79a8.6 8.6 0 0 0 6.16-1.74 4.28 4.28 0 0 1-3.91-2.91h.95a6.18 6.18 0 0 0 .89-.12A4.2 4.2 0 0 1 .8 5.88a4 4 0 0 0 1.81.49 4.23 4.23 0 0 1-1.78-3A4.07 4.07 0 0 1 1.38.79 12.06 12.06 0 0 0 10 5.18z" id="iOjKBC.tif"></path></svg></a><a href="https://www.linkedin.com/company/datamind-org" target="_blank" rel="noopener noreferrer" class="jsx-2506565400 icon"><svg height="12" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16.99 17"><path d="M3.85 17H.34V5.67h3.51zM2.07 4.18a2.09 2.09 0 1 1 2.08-2.09 2.08 2.08 0 0 1-2.08 2.09zM17 17h-3.5v-5.95c0-1.63-.62-2.54-1.91-2.54s-2.14.95-2.14 2.54V17H6.09V5.67h3.36v1.52a4 4 0 0 1 3.42-1.87c2.4 0 4.12 1.47 4.12 4.5V17z"></path></svg></a><a href="https://www.youtube.com/channel/UC79Gv3mYp6zKiSwYemEik9A" target="_blank" rel="noopener noreferrer" class="jsx-2506565400 icon"><svg height="12" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 25.19 17.73"><path d="M24.21 1.52C23.3.44 21.62 0 18.42 0H6.78C3.5 0 1.79.47.88 1.62S0 4.4 0 6.68V11c0 4.43 1 6.68 6.78 6.68h11.64c2.78 0 4.32-.39 5.32-1.34S25.2 13.76 25.2 11V6.68c-.01-2.41-.08-4.07-.99-5.16zm-8 7.94l-5.29 2.76a.81.81 0 0 1-1.19-.72V6a.81.81 0 0 1 1.19-.72L16.17 8a.81.81 0 0 1 0 1.44z"></path></svg></a></div><div class="jsx-2506565400 menu"><a href="/about" class="jsx-2506565400 menuItem">About</a><a href="/terms-of-use" class="jsx-2506565400 menuItem">Terms</a><a href="/privacy-policy" class="jsx-2506565400 menuItem">Privacy</a></div></div><div class="jsx-879378290 BottomBar bar"><div class="jsx-879378290 barView"><div style="background-image:url(https://res.cloudinary.com/dyd911kmh/image/fetch/t_avatar_thumbnail/https://cdn.datacamp.com/community/assets/placeholder_avatar-7f673b5d40e159404a56b5931250cc73.png);border-radius:20px;min-width:40px;min-height:40px" class="jsx-3208234818 Avatar"></div><div class="jsx-879378290 blueBar">Want to leave a comment?</div></div></div></div><link href="https://fonts.googleapis.com/css?family=Lato:300,400i,400,700" rel="stylesheet"/><div class="Analytics"><script>
      ;(function(p,l,o,w,i,n,g){if(!p[i]){p.GlobalSnowplowNamespace=p.GlobalSnowplowNamespace||[];
        p.GlobalSnowplowNamespace.push(i);p[i]=function(){(p[i].q=p[i].q||[]).push(arguments)
        };p[i].q=p[i].q||[];n=l.createElement(o);g=l.getElementsByTagName(o)[0];n.async=1;
        n.src=w;g.parentNode.insertBefore(n,g)}}(window,document,"script","//d36fqcuygdrd4y.cloudfront.net/BuKMCyKUvvyXZkMi44LjI.js","snowplow"));

      window.snowplow('newTracker', 'co', 'track.datacamp.com', {
        platform: 'web',
        post: true,
        discoverRootDomain: true,
        contexts: {
          webPage: true,
          performanceTiming: true
        }
      });
      window.snowplow('enableActivityTracking', 10, 10);
      window.snowplow('enableLinkClickTracking');
    </script><script data-cfasync="false">(function(W,i,s,e,P,o,p){W['WisePopsObject']=P;W[P]=W[P]||function(){
      (W[P].q=W[P].q||[]).push(arguments)},W[P].l=1*new Date();o=i.createElement(s),
      p=i.getElementsByTagName(s)[0];o.async=1;o.src=e;p.parentNode.insertBefore(o,p)
    })(window,document,'script','//loader.wisepops.com/get-loader.js?v=1&site=VswVJn7o4J','wisepops');</script></div></div></div></div><div id="__next-error"></div><script>
          __NEXT_DATA__ = {"props":{"isServer":true,"store":{},"initialState":{"adminContent":{"isFetching":false,"isFetched":false,"statusMessage":"","content":{},"form":{"isSaving":false,"isSucceeded":false,"statusMessage":"","isAdminFormModalOpen":false,"previewSlug":""},"delete":{"isDeleting":false,"isSucceeded":false,"statusMessage":"","isDeleteAdminContentModalOpen":false},"update":{"isApprovingArticle":false,"isSucceeded":false,"statusMessage":"","isApproveArticleModalOpen":false}},"adminList":{"isFetched":false,"isFetching":false,"statusMessage":""},"auth":{"isAuthModalOpen":false,"isLogin":false,"isAuthorized":false,"isSubscriber":false,"user":{},"loginTitle":"","signUpTitle":""},"clientConfig":{"isDevelopment":false,"absoluteUrl":"https://www.datacamp.com","isNewsActive":true,"ALGOLIA_APP_ID":"7H5IORUQLD","ALGOLIA_API_KEY":"ae4f7a78b4914ef37d11dc177ad2eb13","ALGOLIA_API_INDEX":"community_prod","DC_COMMUNITY_AUTHOR_APP_URL":"https://community-author-app.new.datacamp.com/","DC_LIGHT_URL":"https://cdn.datacamp.com/datacamp-light-latest.min.js","ANALYTICS_GOOGLE_TAG_MANAGER":"GTM-TGGWB2P","ANALYTICS_SNOWPLOW_ENDPOINT":"track.datacamp.com","WISEPOPS":"VswVJn7o4J","CHAT_SUBSCRIBER_REDIRECT":"https://www.datacamp.com/slack/join","CHAT_NONSUBSCRIBER_LINK":"https://www.datacamp.com/subscribe?from_communitychat=true","CHAT_SUBSCRIBE_TEAM":"https://www.datacamp.com/groups/business"},"comment":{"isFetching":false,"isFetched":false,"statusMessage":"","comments":[],"commentsTotal":0,"form":{"isSaving":false,"isSucceeded":false,"statusMessage":"","id":"new","parentId":null,"commentText":""},"delete":{"isDeleting":false,"isSucceeded":false,"statusMessage":"","isDeleteCommentModalOpen":false},"isBottomBarOpen":true,"bottomBarView":"bar"},"content":{"content":{"id":13472,"externalId":null,"type":"Tutorial","status":"published","authorId":"spsayakpaul","title":"Essentials of Linear Regression in Python","slug":"essentials-linear-regression-python","previewSlug":"ca100cb0-70e6-476a-83f4-a574d5e13d16","description":"Learn what formulates a regression problem and how a linear regression algorithm works in Python.","articles":[],"courses":[],"redirectSlug":null,"contentHtml":"\u003cp\u003eThe field of Data Science has progressed like nothing before. It incorporates so many different domains like Statistics, Linear Algebra, Machine Learning, Databases into its account and merges them in the most meaningful way possible. But, in its core, what makes this domain one of the craziest ones? - \u003cstrong\u003eThe powerful statistical algorithms\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eOne such very primitive statistical algorithm is \u003cstrong\u003eLinear Regression\u003c/strong\u003e. Although it is very old, it is never too old to be neglected for a budding data scientist like you. Understanding the principle behind the working of linear regression is very important as to reason the evolution of a whole class of statistical algorithms called \u003cstrong\u003eGeneralized Linear Models\u003c/strong\u003e. Moreover, it will also help you understand other aspects of a typical statistical/machine learning algorithm for example - cost functions, coefficients, optimization, etc.\u003c/p\u003e\n\u003cp\u003eAs the title of this tutorial suggests, you will cover Linear Regression in details in this tutorial. Specifically, you will cover:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eUnderstanding a regression problem\u003c/li\u003e\n\u003cli\u003eComing up with a hypothesis for the regression problem\u003c/li\u003e\n\u003cli\u003eWhat is Linear Regression\u003c/li\u003e\n\u003cli\u003eCost functions\u003c/li\u003e\n\u003cli\u003eOptimizing Linear Regression\u003c/li\u003e\n\u003cli\u003eWhy it is still important to understand even in the world of neural networks\u003c/li\u003e\n\u003cli\u003eA case study in Python\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBefore diving deep into the theories behind Linear Regression let\u0026#39;s have a clear view of the term \u003cem\u003eregression\u003c/em\u003e.\u003c/p\u003e\n\u003ch2 id=\"understanding-a-regression-problem-\"\u003eUnderstanding a regression problem:\u003c/h2\u003e\n\u003cp\u003eRegression belongs to the class of \u003ca href=\"https://en.wikipedia.org/wiki/Supervised_learning\"\u003eSupervised Learning\u003c/a\u003e tasks where the datasets that are used for predictive/statistical modeling contain \u003ca href=\"http://www.statisticshowto.com/continuous-variable/\"\u003econtinuous labels\u003c/a\u003e. But, let\u0026#39;s define a regression problem more mathematically.\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s consider the following image below:\n\u003cimg src = \"https://image.ibb.co/jRH8E9/Capture.jpg\"\u003e\u003c/img\u003e\n\u003cstrong\u003eSource: Andrew Ng\u0026#39;s lecture notes\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eSo, in the above image, \u003ci\u003eX\u003c/i\u003e is the set of values that correspond to the living areas of various houses (also considered as the space of input values) and \u003ci\u003ey\u003c/i\u003e is the price of the respective houses but note that these values are predicted by \u003ci\u003eh\u003c/i\u003e. \u003ci\u003eh\u003c/i\u003e is the function that maps the \u003ci\u003eX\u003c/i\u003e values to \u003ci\u003ey\u003c/i\u003e (often called as predictor). For historical reasons, this \u003ci\u003eh\u003c/i\u003e is referred to as a \u003ci\u003ehypothesis\u003c/i\u003e function. Keep in mind that, this dataset has only featured, i.e., the living areas of various houses and consider this to be a toy dataset for the sake of understanding.\u003c/p\u003e\n\u003cp\u003eNote that, the predicted values here are continuous in nature. So, your ultimate goal is, given a training set, to learn a function\n$h : \\mathcal{X}  \\rightarrow \\mathcal{Y}$ so that h(x) is a \u0026quot;good\u0026quot; predictor for the corresponding value of y. Also, keep in mind that the domain of values that both X and Y accept are all real numbers and you can define it like this: $\\mathcal{X}  = \\mathcal{Y} = \\mathbb{IR}$ where, $\\mathbb{IR}$ is the set of all real numbers.\u003c/p\u003e\n\u003cp\u003eA pair (x\u003csup\u003e(i)\u003c/sup\u003e, y\u003csup\u003e(i)\u003c/sup\u003e) is called a training example. You can define the training set as {(x\u003csup\u003e(i)\u003c/sup\u003e, y\u003csup\u003e(i)\u003c/sup\u003e) ; i = 1,...,m} (in case the training set contains m instances and there is only one feature \u003ci\u003ex\u003c/i\u003e in the dataset).\u003c/p\u003e\n\u003cp\u003eA bit of mathematics there for you so that you don\u0026#39;t go wrong even in the simplest of things. So, according to Han, Kamber, and Pei-\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u0026quot;In general, these methods are used to predict the value of a response\n(dependent) variable from one or more predictor (independent) variables, where the\nvariables are numeric.\u003c/em\u003e\u0026quot; - \u003ca href=\"https://scholar.google.com/scholar?q=Han%2C%20J.%2C%20Kamber%2C%20M.%2C%20Pei%2C%20J.%3A%20Data%20Mining%3A%20Concepts%20and%20Techniques%20%283rd%20edn.%29%2C%20pp.%2056.%20Morgan%20Kaufmann%20Publishers%20Inc.%20San%20Francisco%20%282011%29\"\u003eData Mining: Concepts and Techniques (3rd edn.)\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAs simple as that!\u003c/p\u003e\n\u003cp\u003eSo, in the course of understanding a typical regression problem you also saw how to define a hypothesis for it as well. Brilliant going. You have set the mood just perfect! Now, you will straight dive into the mechanics of Linear Regression.\u003c/p\u003e\n\u003ch2 id=\"linear-regression-how-it-works-\"\u003eLinear Regression - how it works?\u003c/h2\u003e\n\u003cp\u003eBefore going into its details, won\u0026#39;t it be good to take a look when it was discovered? Well, that goes way-way back to 18th Century. The mighty \u003cstrong\u003eCarl Friedrich Gauss\u003c/strong\u003e first proposed the most trivial form of statistical regression, but there are many arguments on this. Let\u0026#39;s not get into those. But if you are interested to see the arguments that took place for this between Gauss and \u003cstrong\u003eAdrien-Marie Legendre\u003c/strong\u003e, \u003ca href=\"\u0026quot;https://priceonomics.com/the-discovery-of-statistical-regression/\u0026quot;\"\u003ethis is the link\u003c/a\u003e to check out.\u003c/p\u003e\n\u003cp\u003eLinear regression is perhaps one of the most well known and well-understood algorithms in statistics and machine learning. Linear regression was developed in the field of statistics and is studied as a model for understanding the relationship between input and output numerical variables, but with the course of time, it has become an integral part of modern machine learning toolbox.\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s have a toy dataset for it. You will use the same house price prediction dataset to investigate this but this time with two features. The task remains the same i.e., predicting the house price.\u003c/p\u003e\n\u003cp\u003e\u003cimg src = \"https://image.ibb.co/hXTenU/Capture.jpg\"\u003e\u003c/img\u003e\n\u003cstrong\u003eSource: Andrew Ng\u0026#39;s lecture notes\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eAs mentioned earlier, now the x’s are two-dimensional which means your dataset contains two features. For instance, x\u003csub\u003e1\u003c/sub\u003e\u003csup\u003e(i)\u003c/sup\u003e is the living area of the i-th house in the training set, and x\u003csub\u003e2\u003c/sub\u003e\u003csup\u003e(i)\u003c/sup\u003e is its number of bedrooms.\u003c/p\u003e\n\u003cp\u003eTo perform regression, you must decide the way you are going to represent h. As an initial choice, let’s say you decide to approximate y as a linear function of x:\u003c/p\u003e\n\u003ccenter\u003e\u003cb\u003eh\u003csub\u003eθ\u003c/sub\u003e(x) = θ\u003csub\u003e0\u003c/sub\u003e + θ\u003csub\u003e1\u003c/sub\u003ex\u003csub\u003e1\u003c/sub\u003e + θ\u003csub\u003e2\u003c/sub\u003ex\u003csub\u003e2\u003c/sub\u003e\u003c/b\u003e\u003c/center\u003e\n\n\u003cp\u003eHere, the θ\u003csub\u003ei\u003c/sub\u003e’s are the parameters (also called weights) parameterizing the space of linear functions mapping from $\\mathcal{X}$ to $\\mathcal{Y}$. In a simpler sense, these parameters are used for accurately mapping $\\mathcal{X}$ to $\\mathcal{Y}$. But to keep things simple for your understanding, you will drop the θ subscript in h\u003csub\u003eθ\u003c/sub\u003e(x), and write it simply as h(x). To simplify your notation even further, you will also introduce the convention of letting x\u003csub\u003e0\u003c/sub\u003e = 1 (this is the \u003cb\u003eintercept\u003c/b\u003e term), so that\u003c/p\u003e\n\u003cp\u003e\u003cimg src = \"https://image.ibb.co/i94zMp/Capture.jpg\"\u003e\u003c/img\u003e\u003c/p\u003e\n\u003cp\u003ewhere on the right-hand side above you are considering θ and x both as vectors, and here n is the number of input instances (not counting x\u003csub\u003e0\u003c/sub\u003e).\u003c/p\u003e\n\u003cp\u003eBut the main question that gets raised at this point is how do you pick or learn the parameters θ? You cannot change your input instances as to predict the prices. You have only these θ parameters to tune/adjust.\u003c/p\u003e\n\u003cp\u003eOne prominent method seems to be to make h(x) close to y, at least for the training examples you have. To understand this more formally, let\u0026#39;s try defining a function that determines, for each value of the θ’s, how close the h(x\u003csup\u003e(i)\u003c/sup\u003e)’s are to the corresponding y\u003csup\u003e(i)\u003c/sup\u003e ’s. The function should look like the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src = \"https://image.ibb.co/kW8e49/Capture.jpg\"\u003e\u003c/img\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://stackoverflow.com/questions/32274474/machine-learning-linear-regression-using-batch-gradient-descent\"\u003eSource: StackOverflow\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eTo understand the reason behind taking the squared value instead of the absolute value, consider this squared-term as an advantage for the future operations to be performed for training the regression model. But if you want to dig deeper, \u003ca href=\"https://www.quora.com/Why-do-we-use-square-error-instead-of-absolute-value-when-we-calculate-R-2-in-regression-analysis\"\u003ehelp yourself\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eYou just saw one of the most important formulas in the world of Data Science/Machine Learning/Statistics. It is called as \u003cstrong\u003ecost function\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eThis is an essential derivation because not only it gives birth to the next evolution of the linear regression (Ordinary Least Squares) but also formulates the foundations of a whole class of linear modeling algorithms (remember you came across a term called \u003cem\u003eGeneralized Linear Models\u003c/em\u003e).\u003c/p\u003e\n\u003cp\u003eIt is important to note that, linear regression can often be divided into two basic forms:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eSimple Linear Regression (SLR) which deals with just two variables (the one you saw at first)\u003c/li\u003e\n\u003cli\u003eMulti-linear Regression (MLR) which deals with more than two variables (the one you just saw)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese things are very straightforward but can often cause confusion.\u003c/p\u003e\n\u003cp\u003eYou have already laid your foundations of linear regression. Now you will study more about the ways of estimating the parameters you saw in the above section. This estimation of parameters is essentially known as the training of linear regression. Now, there are many methods to train a linear regression model \u003cstrong\u003eOrdinary Least Squares\u003c/strong\u003e (OLS) being the most popular among them. So, it is good to refer a linear regression model trained using OLS as Ordinary Least Squares Linear Regression or just Least Squares Regression.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eNote that the parameters here in this context are also called model coefficients.\u003c/strong\u003e\u003c/p\u003e\n\u003ch2 id=\"optimizing-a-linear-regression-model-various-approaches-\"\u003eOptimizing a linear regression model - various approaches:\u003c/h2\u003e\n\u003cp\u003eLearning/training a linear regression model essentially means estimating the values of the coefficients/parameters used in the representation with the data you have.\u003c/p\u003e\n\u003cp\u003eIn this section, you will take a brief look at some techniques to prepare a linear regression model.\u003c/p\u003e\n\u003ch2 id=\"least-squares-regression-\"\u003eLeast squares\u0026#39; regression:\u003c/h2\u003e\n\u003cp\u003eYou left the previous section with a notion to choose θ so as to minimize J(θ). To do so, let’s use a search algorithm that starts with some \u0026quot;initial guess\u0026quot; for θ, and that iteratively changes θ to make J(θ) smaller, until hopefully, you converge to a value of θ that minimizes J(θ). Specifically, let’s consider the \u003cstrong\u003egradient descent algorithm\u003c/strong\u003e, which starts with some initial θ, and repeatedly performs the update:\u003c/p\u003e\n\u003cp\u003e\u003cimg src= \"https://image.ibb.co/jUXWj9/Capture.jpg\"\u003e\u003c/img\u003e\n\u003cstrong\u003eSource: Andrew Ng\u0026#39;s lecture notes\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e(This update is simultaneously performed for all values of j = 0, . . . , n.) Here, α is called the \u003cstrong\u003elearning rate\u003c/strong\u003e. This is a very natural algorithm that repeatedly takes a step in the direction of steepest decrease of J. This term α effectively controls how steep your algorithm would move to the decrease of J. It can be pictorially expressed as the following:\u003c/p\u003e\n\u003cp\u003e\u003cimg src = \"https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png\"\u003e\n\u003cstrong\u003e Source: \u003ca href=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png\"\u003eml-cheatsheet\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eIntuitively speaking, the above formula denotes the small change that happens in J w.r.t the θ\u003csub\u003ej\u003c/sub\u003e parameter and how it affects the initial value of θ\u003csub\u003ej\u003c/sub\u003e. But look carefully, you have a partial derivative here to deal with. The whole derivation process is out of the scope of this tutorial.\u003c/p\u003e\n\u003cp\u003eJust note that for a single training example, this gives the update rule:\u003c/p\u003e\n\u003cp\u003e\u003cimg src = \"https://image.ibb.co/gpfOxU/Capture.jpg\"\u003e\u003c/img\u003e\n\u003cstrong\u003eSource: \u003ca href=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/gradient_descent_demystified.png\"\u003eml-cheatsheet\u003c/a\u003e\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eThe rule is called the LMS update rule (LMS stands for “least mean squares”) and is also known as the \u003cstrong\u003eWidrow-Hoff learning rule\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s summarize a few things in the context of OLS.\u003c/p\u003e\n\u003cp\u003e\u003cem\u003e\u0026quot;The Ordinary Least Squares procedure seeks to minimize the sum of the squared residuals. This means that given a regression line through the data we calculate the distance from each data point to the regression line, square it, and sum all of the squared errors together. This is the quantity that ordinary least squares seeks to minimize.\u0026quot;\u003c/em\u003e - \u003ca href=\"https://machinelearningmastery.com/linear-regression-for-machine-learning/\"\u003eJason Brownlee\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"optimization-with-gradient-descent-\"\u003eOptimization with gradient descent:\u003c/h2\u003e\n\u003cp\u003eIn the previous training rule, you already got the notion of how gradient descent can be incorporated in this context. Essentially, gradient descent is a process of optimizing the values of the coefficients by iteratively minimizing the error of the model on your training data.\u003c/p\u003e\n\u003cp\u003eMore briefly speaking, it works by starting with random values for each coefficient. The sum of the squared errors is calculated for each pair of input and output values. A learning rate is used as a scale factor, and the coefficients are updated in the direction towards minimizing the error. The process is repeated until a minimum sum squared error is achieved or no further improvement is possible.\u003c/p\u003e\n\u003cp\u003eThe term α (learning rate) is very important here since it determines the size of the improvement step to take on each iteration of the procedure.\u003c/p\u003e\n\u003cp\u003eNow there are commonly two variants of gradient descent:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe method that looks at every example in the entire training set on every step and is called \u003cstrong\u003ebatch gradient descent\u003c/strong\u003e.\u003c/li\u003e\n\u003cli\u003eThe method where you repeatedly run through the training set, and each time you encounter a training example, you update the parameters according to the gradient of the error with respect to that single training example only. This algorithm is called \u003cstrong\u003estochastic gradient descent\u003c/strong\u003e (also incremental gradient descent).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThat is all for gradient descent for this tutorial. Now, you take a look at another way of optimizing a linear regression model, i.e. Regularization.\u003c/p\u003e\n\u003ch2 id=\"regularization-\"\u003eRegularization:\u003c/h2\u003e\n\u003cp\u003eDataCamp already has a good introductory \u003ca href=\"https://www.datacamp.com/community/tutorials/towards-preventing-overfitting-regularization\"\u003earticle on Regularization\u003c/a\u003e. You might want to check that out before proceeding with this one.\u003c/p\u003e\n\u003cp\u003eGenerally, regularization methods work by penalizing the coefficients of features having extremely large values and thereby try to reduce the error. It not only results in an enhanced error rate but also, it reduces the model complexity. This is particularly very useful when you are dealing with a dataset that has a large number of features, and your baseline model is not able to distinguish between the importance of the features (not all features in a dataset are equally important, right?).\u003c/p\u003e\n\u003cp\u003eThere are two variants of regularization procedures for linear regression are:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLasso Regression\u003c/strong\u003e: adds a penalty term which is equivalent to the \u003cstrong\u003eabsolute value of the magnitude of the coefficients\u003c/strong\u003e (also called \u003cem\u003eL1 regularization\u003c/em\u003e). The penalty terms look like:\n\u003cimg src = \"https://image.ibb.co/bzZ2cf/Capture.jpg\"\u003e\u003cimg\u003e\u003c/p\u003e\n\u003cp\u003ewhere,\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\lambda$ is the constant factor that you add in order to control the speed of the improvement in error (\u003cstrong\u003elearning rate\u003c/strong\u003e)\u003c/li\u003e\n\u003cli\u003ethe dataset has (M+1) features, so it runs from 0 to M. w\u003csub\u003e\u003cb\u003ej\u003c/b\u003e\u003c/sub\u003e is the weight/coefficient.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eRidge Regression\u003c/strong\u003e: adds a penalty term which is equivalent to the \u003cstrong\u003esquare of the magnitude of coefficients\u003c/strong\u003e (also called \u003cem\u003eL2 regularization\u003c/em\u003e). The penalty terms look like:\n\u003cimg src = \"https://image.ibb.co/hY4oiL/Capture.jpg\"\u003e\u003cimg\u003e\u003c/p\u003e\n\u003ch2 id=\"is-it-still-worth-the-time-to-learn-linear-regression-\"\u003eIs it still worth the time to learn linear regression?\u003c/h2\u003e\n\u003cp\u003eIs it not? You already saw how gracefully linear regression introduces some of the most critical concepts of Machine Learning such as cost functions, optimization, variable relationships and what not? All these things are vital even if you are constructing a neural network. The applicability may differ in some places, but the overall concepts remain precisely the same. So, without understanding these fundamental things, you will never be able to reason why your neural net is not performing well.\u003c/p\u003e\n\u003cp\u003eMoreover, a simple concept of deriving the relationships among variables gave birth to so many concepts and most importantly it created a whole new family of algorithms - \u003ca href=\"https://onlinecourses.science.psu.edu/stat504/node/216/\"\u003eGeneralized Linear Models\u003c/a\u003e. So for aspiring Data Science/Machine Learning/Artificial Intelligence practitioner, this algorithm is something which cannot be neglected. You already have understood that by now!\u003c/p\u003e\n\u003cp\u003eNo, you will implement a simple linear regression in Python for yourself now. It should be fun!\u003c/p\u003e\n\u003ch2 id=\"a-case-study-in-python-\"\u003eA case study in Python:\u003c/h2\u003e\n\u003cp\u003eFor this case study first, you will use the \u003ca href=\"http://www.statsmodels.org/stable/index.html\"\u003eStatsmodel\u003c/a\u003e library for Python. It is a very popular library which provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests, and statistical data exploration. For the data, you will use the famous \u003ca href=\"https://www.kaggle.com/c/boston-housing\"\u003eBoston House dataset\u003c/a\u003e. The mighty scikit-learn comes with this dataset, so you don\u0026#39;t need to download it separately.\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s start the case study by importing the \u003ccode\u003estatsmodels\u003c/code\u003e library and your dataset:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eimport statsmodels.api as sm\nfrom sklearn import datasets\n\ndata = datasets.load_boston()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eScikit-learn provides a handy description of the dataset, and it can be easily viewed by:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eprint (data.DESCR)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003eBoston House Prices dataset\n===========================\n\nNotes\n------\nData Set Characteristics:  \n\n    :Number of Instances: 506\n\n    :Number of Attributes: 13 numeric/categorical predictive\n\n    :Median Value (attribute 14) is usually the target\n\n    :Attribute Information (in order):\n        - CRIM     per capita crime rate by town\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n        - INDUS    proportion of non-retail business acres per town\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n        - NOX      nitric oxides concentration (parts per 10 million)\n        - RM       average number of rooms per dwelling\n        - AGE      proportion of owner-occupied units built prior to 1940\n        - DIS      weighted distances to five Boston employment centres\n        - RAD      index of accessibility to radial highways\n        - TAX      full-value property-tax rate per $10,000\n        - PTRATIO  pupil-teacher ratio by town\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n        - LSTAT    % lower status of the population\n        - MEDV     Median value of owner-occupied homes in $1000\u0026#39;s\n\n    :Missing Attribute Values: None\n\n    :Creator: Harrison, D. and Rubinfeld, D.L.\n\nThis is a copy of UCI ML housing dataset.\nhttp://archive.ics.uci.edu/ml/datasets/Housing\n\n\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. \u0026#39;Hedonic\nprices and the demand for clean air\u0026#39;, J. Environ. Economics \u0026amp; Management,\nvol.5, 81-102, 1978.   Used in Belsley, Kuh \u0026amp; Welsch, \u0026#39;Regression diagnostics\n...\u0026#39;, Wiley, 1980.   N.B. Various transformations are used in the table on\npages 244-261 of the latter.\n\nThe Boston house-price data has been used in many machine learning papers that address regression\nproblems.   \n\n**References**\n\n   - Belsley, Kuh \u0026amp; Welsch, \u0026#39;Regression diagnostics: Identifying Influential Data and Sources of Collinearity\u0026#39;, Wiley, 1980. 244-261.\n   - Quinlan, R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eNow, before applying linear regression, you will have to prepare the data and segregate the features and the label of the dataset. \u003ccode\u003eMEDV\u003c/code\u003e (median home value) is the label in this case. You can access the features of the dataset using \u003ccode\u003efeature_names\u003c/code\u003e attribute.\u003c/p\u003e\n\u003cp\u003eA bit of \u003ccode\u003epandas\u003c/code\u003e knowledge will come in handy here. \u003ca href=\"https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf\"\u003eThis cheat sheet\u003c/a\u003e is a must-see if you are looking for ways to refresh basic \u003ccode\u003epandas\u003c/code\u003e concepts.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003e# Pandas and NumPy import\nimport numpy as np\nimport pandas as pd\n\n# Set the features  \ndf = pd.DataFrame(data.data, columns=data.feature_names)\n\n# Set the target\ntarget = pd.DataFrame(data.target, columns=[\u0026quot;MEDV\u0026quot;])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eAt this point, you need to consider a few important things about linear regression before applying it to the data. You could have studied this earlier in this tutorial, but studying these factors at this particular point of time will help you get the real feel.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLinear Assumption\u003c/strong\u003e: Linear regression is best employed to capture the relationship between the input variables and the outputs. In order to do so, linear regression assumes this relationship to be linear (which might not be the case all the time). But you can always transform your data so that a linear relationship is maintained. For example, if your data has an exponential relationship, you can apply \u003ca href=\"https://www.dummies.com/programming/big-data/data-science/data-science-using-python-to-modify-data-distributions/\"\u003e\u003cem\u003elog-transform\u003c/em\u003e\u003c/a\u003e to make the relationship linear.\n\u003cbr\u003e\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCollinearity between the features\u003c/strong\u003e: Collinearity is a measure to calculate the importance of a feature of a dataset mathematically. When you have a dataset in which the features are very correlated to each other, linear regression fails to approximate the relationship appropriately and tends to overfit. So, it is efficient to detect the highly correlated features and to drop them before you apply linear regression. If you want to know more about this, feel free to check \u003ca href=\"https://www.kaggle.com/robertoruiz/dealing-with-multicollinearity\"\u003ethis excellent Kaggle kernel\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLet\u0026#39;s do some hands-on now. To keep things simple you will just take \u003ccode\u003eRM — the average number of rooms\u003c/code\u003e feature for now. Note that \u003ccode\u003eStatsmodels\u003c/code\u003e does not add a constant term (recall the factor θ\u003csub\u003e0\u003c/sub\u003e) by default. Let’s see it first without the constant term in your regression model:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eX = df[\u0026quot;RM\u0026quot;]\ny = target[\u0026quot;MEDV\u0026quot;]\n\n# Fit and make the predictions by the model\nmodel = sm.OLS(y, X).fit()\npredictions = model.predict(X)\n\n# Print out the statistics\nmodel.summary()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1540929849/image_es1tjp.png\" /\u003e\u003c/p\u003e\n\u003cp\u003eWhat is this output! It is way too big to understand when you are seeing it for the first time. Let\u0026#39;s go through the most critical points step by step:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe first observation you should make here is you are using \u003ccode\u003eOLS\u003c/code\u003e method to train your linear regression model.\n\u003cbr\u003e\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eThere\u0026#39;s a value corresponding to \u003cstrong\u003eR-Squared\u003c/strong\u003e. R-squared is the “percent of variance explained” by the model.  That is, R-squared is the fraction by which the variance of the errors is less than the variance of the dependent variable. R-squared values range from 0 to 1 and are commonly stated as percentages from 0% to 100%. R-squared will give you an estimate of the relationship between movements of a dependent variable based on an independent variable\u0026#39;s movements. It doesn\u0026#39;t tell you whether your chosen model is good or bad, nor will it tell you whether the data and predictions are biased. A high or low R-square isn\u0026#39;t necessarily good or bad, as it doesn\u0026#39;t convey the reliability of the model, nor whether you\u0026#39;ve chosen the right regression. You can get a low R-squared for a good model, or a high R-square for a poorly fitted model, and vice versa.\n\u003cbr\u003e\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eThe coefficient (\u003cem\u003ecoef\u003c/em\u003e) of 3.634 means that if the \u003ccode\u003eRM\u003c/code\u003e variable increases by 1, the predicted value of \u003ccode\u003eMEDV\u003c/code\u003e increases by 3.634.\n\u003cbr\u003e\u003cbr\u003e\u003c/li\u003e\n\u003cli\u003eThere is a 95% confidence intervals for the RM which means that the model predicts at a 95% percent confidence that the value of RM is between 3.548 to 3.759).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese are the most important points you should take care of for the time being (and you can ignore the warning as well).\u003c/p\u003e\n\u003cp\u003eA constant term can easily be added to the linear regression model. You can do it by \u003ccode\u003eX = sm.add_constant(X)\u003c/code\u003e (X is the name of the dataframe containing the input (independent variables).\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eX = sm.add_constant(X)\n\nmodel = sm.OLS(y, X).fit()\npredictions = model.predict(X)\n\nmodel.summary()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1540930228/Screen_Shot_2018-10-30_at_4.09.02_PM_gb0e9n.png\" /\u003e\u003c/p\u003e\n\u003cp\u003eIt can be clearly seen that the addition of the constant term has a direct effect on the coefficient term. Without the constant term, your model was passing through the origin, but now you have a y-intercept at -34.67. Now the slope of the \u003ccode\u003eRM\u003c/code\u003e predictor is also changed from 3.634 to 9.1021 (\u003cem\u003ecoef\u003c/em\u003e of \u003ccode\u003eRM\u003c/code\u003e).\u003c/p\u003e\n\u003cp\u003eNow you will fit a regression model with more than one variable — you will add \u003ccode\u003eLSTAT\u003c/code\u003e (percentage of lower status of the population) along with the \u003ccode\u003eRM\u003c/code\u003e variable. The model training (fitting) procedure remains the exact same as previous:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eX = df[[\u0026quot;RM\u0026quot;, \u0026quot;LSTAT\u0026quot;]]\ny = target[\u0026quot;MEDV\u0026quot;]\n\nmodel = sm.OLS(y, X).fit()\npredictions = model.predict(X)\n\nmodel.summary()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003ccenter\u003e\u003cimg src=\"https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1540930228/Screen_Shot_2018-10-30_at_4.09.15_PM_oamdc9.png\" /\u003e\u003c/p\u003e\n\u003cp\u003eLet\u0026#39;s interpret this one now:\u003c/p\u003e\n\u003cp\u003eThis model has a much higher R-squared value — 0.948, which essentially means that this model captures 94.8% of the variance in the dependent variable. Now, let\u0026#39;s try to figure out the relationship between the two variables \u003ccode\u003eRM\u003c/code\u003e and \u003ccode\u003eLSTAT\u003c/code\u003e and median house value. As \u003ccode\u003eRM\u003c/code\u003e increases by 1, \u003ccode\u003eMEDV\u003c/code\u003e will increase by 4.9069, and when \u003ccode\u003eLSTAT\u003c/code\u003e increases by 1, \u003ccode\u003eMEDV\u003c/code\u003e will \u003cem\u003edecrease\u003c/em\u003e by 0.6557. This indicates that \u003ccode\u003eRM\u003c/code\u003e and \u003ccode\u003eLSTAT\u003c/code\u003e are statistically significant in predicting (or estimating) the median house value.\u003c/p\u003e\n\u003cp\u003eYou can interpret this relationship in plain English as well:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHouses having a small number of rooms are likely to have low price values.\u003c/li\u003e\n\u003cli\u003eIn the areas where the status of the population, is lower the house prices are likely to be low.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMakes more sense now! Isn\u0026#39;t it?\u003c/p\u003e\n\u003cp\u003eThis was the example of both single and multiple linear regression in \u003ccode\u003eStatsmodels\u003c/code\u003e. Your homework will be to investigate and interpret the results with the further features.\u003c/p\u003e\n\u003cp\u003eNext, let\u0026#39;s see how linear regression can be implemented using your very own \u003ccode\u003escikit-learn\u003c/code\u003e. You already have the dataset imported, but you will have to import the \u003ccode\u003elinear_model\u003c/code\u003e class.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003efrom sklearn import linear_model\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003eX = df\ny = target[\u0026quot;MEDV\u0026quot;]\n\nlm = linear_model.LinearRegression()\nmodel = lm.fit(X,y)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe model training is completed. This \u003ccode\u003esklearn\u003c/code\u003e implementation also uses OLS. Let\u0026#39;s make some predictions of \u003ccode\u003eMEDV\u003c/code\u003e values for the first five samples.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003epredictions = lm.predict(X)\nprint(predictions[0:5])\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e[30.00821269 25.0298606  30.5702317  28.60814055 27.94288232]\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIf you want to know some more details (such as the R-squared, coefficients, etc.) of your model, you can easily do so.\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"lang-python\"\u003elm.score(X,y)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003e0.7406077428649427\n\u003c/code\u003e\u003c/pre\u003e\u003cpre\u003e\u003ccode class=\"lang-python\"\u003elm.coef_\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode\u003earray([-1.07170557e-01,  4.63952195e-02,  2.08602395e-02,  2.68856140e+00,\n       -1.77957587e+01,  3.80475246e+00,  7.51061703e-04, -1.47575880e+00,\n        3.05655038e-01, -1.23293463e-02, -9.53463555e-01,  9.39251272e-03,\n       -5.25466633e-01])\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"wrap-up-\"\u003eWrap up!\u003c/h2\u003e\n\u003cp\u003eBeautiful! You have made it to the end. Covering one of the simplest and the most fundamental algorithms was not that easy, but you did it pretty well. You not only got familiarized with simple linear regression but also studied many fundamental aspects, terms, factors of machine learning. You did an in-depth case study in Python as well.\u003c/p\u003e\n\u003cp\u003eThis tutorial can also be treated as a motivation for you to implement Linear Regression from scratch. Following are the brief steps if anyone wants to do it for real:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eCalculate the mean and variance of the data\u003c/li\u003e\n\u003cli\u003eCalculate covariance\u003c/li\u003e\n\u003cli\u003eEstimate coefficients\u003c/li\u003e\n\u003cli\u003eMake predictions\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFollowing are some references that were used in order to prepare this tutorial:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.coursera.org/learn/machine-learning\"\u003eMachine Learning course by Coursera (taught by the great Andrew Ng)\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://machinelearningmastery.com/implement-simple-linear-regression-scratch-python/\"\u003eImplementing linear regression from scratch\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://web.stanford.edu/~hastie/Papers/ESLII.pdf\"\u003eThe Elements of Statistical Learning\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://towardsdatascience.com/simple-and-multiple-linear-regression-in-python-c928425168f9\"\u003eSimple and Multiple Linear Regression in Python by Adi Bronshtein\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIf you would like to learn more about linear classifiers, take DataCamp\u0026#39;s \u003ca href=\"https://www.datacamp.com/courses/linear-classifiers-in-python\"\u003eLinear Classifiers in Python\u003c/a\u003e course.\u003c/p\u003e","contentUrl":"https://www.datacamp.com/community/tutorials/essentials-linear-regression-python","userContentUrl":null,"illustrationUrl":null,"seoTitle":"Essentials of Linear Regression in Python","seoMetaDescription":"Learn what formulates a regression problem and how a linear regression algorithm works in Python.","seoKeyword":"linear regression, python","mustRead":false,"programmingLanguage":null,"submissionDate":"2018-10-29T20:05:01.790Z","publishDate":"2018-10-31T16:00:00.000Z","episode":null,"isLatest":null,"externalUrl":null,"transcriptUrl":null,"guests":[],"links":null,"isSpam":false,"xp":0,"flaggingUsers":[],"isDisabled":false,"connectedInternalContentId":13546,"createdAt":"2018-10-29T20:05:01.778Z","updatedAt":"2018-11-05T13:47:21.167Z","upvoting":{"voteCount":39,"voted":false},"tags":["statistical modeling","python"],"author":{"id":535025,"slug":"spsayakpaul","avatarUrlSquare":"https://assets.datacamp.com/users/avatars/000/535/025/square/Capture.PNG?1556868932","fullName":"Sayak  Paul","nameFromEmail":"spsayakpaul","isAdmin":false},"recommendedArticles":[]},"isFetched":true,"isFetching":false,"statusMessage":""},"countdownBanner":{"banner":{"showBanner":false,"title":"","text":"","startDate":"","endDate":"","link":"","nonPromo":false},"isBannerOpen":false},"form":{},"list":{"isFetched":false,"isFetching":false,"statusMessage":""},"menu":{"isSidebarMenuOpen":false},"notifications":{"isFetched":false,"isFetching":false,"isReadFetched":false,"isReadFetching":false,"statusMessage":"","readStatusMessage":"","Notifications":[],"NotificationsTotal":0,"unReadCount":0},"preview":{"isFetching":false,"isFetched":false,"statusMessage":"","content":{}},"recommendCS":{"isPosting":false,"isPosted":false,"statusMessage":"","isModalOpen":false,"currentStep":"form"},"spam":{"isFlagging":false,"isSucceeded":false,"statusMessage":"","isSpamModalOpen":false,"isUnSpamModalOpen":false},"tag":{"isRequesting":false,"isSucceeded":false,"statusMessage":"","isDeleteTagModalOpen":false},"tagList":{"isFetched":false,"isFetching":false,"statusMessage":"","list":[],"total":0},"tagSearch":{"isFetching":false,"isFetched":false,"statusMessage":"","content":{}},"user":{"isFetching":false,"isFetched":false,"statusMessage":"","unBan":{"isUnBanning":false,"isSucceeded":false,"statusMessage":"","isUnBanUserModalOpen":false},"ban":{"isBanning":false,"isSucceeded":false,"statusMessage":"","isBanUserModalOpen":false}},"submitArticle":{"isPosting":false,"isPosted":false,"statusMessage":"","timer":0,"articleSlug":"","isModalOpen":false,"currentStep":"form","slug":"","externalUrl":""},"rss":{"isCreating":false,"isSucceeded":false,"statusMessage":""},"rssFeedList":{"isFetched":false,"isFetching":false,"statusMessage":"","list":[],"disconnectModal":{"isFetched":true,"isFetching":false,"isOpen":false,"rssFeedIdToDisconnect":null,"statusMessage":""}},"setAsHomePage":{"isSetAsHomePageModalOpen":false},"analytics":{}},"initialProps":{"asPath":"/community/tutorials/essentials-linear-regression-python"}},"pathname":"/community/tutorial","query":{"slug":"essentials-linear-regression-python"},"buildId":"f80eab32-3c58-4b52-a08c-d754687b7a03","buildStats":{"app.js":{"hash":"b782294ddb8d954b4c94ee4c23476b23"}},"assetPrefix":"/community","nextExport":false,"err":null,"chunks":[]}
          module={}
          __NEXT_LOADED_PAGES__ = []
          __NEXT_LOADED_CHUNKS__ = []

          __NEXT_REGISTER_PAGE = function (route, fn) {
            __NEXT_LOADED_PAGES__.push({ route: route, fn: fn })
          }

          __NEXT_REGISTER_CHUNK = function (chunkName, fn) {
            __NEXT_LOADED_CHUNKS__.push({ chunkName: chunkName, fn: fn })
          }
        </script><script async="" id="__NEXT_PAGE__/community/tutorial" type="text/javascript" src="/community/_next/f80eab32-3c58-4b52-a08c-d754687b7a03/page/community/tutorial.js"></script><script async="" id="__NEXT_PAGE__/_error" type="text/javascript" src="/community/_next/f80eab32-3c58-4b52-a08c-d754687b7a03/page/_error.js"></script><script type="text/javascript" src="/community/_next/b782294ddb8d954b4c94ee4c23476b23/app.js" async=""></script></body></html>